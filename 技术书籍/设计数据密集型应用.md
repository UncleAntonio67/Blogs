作者： Martin Kleppmann                        
阅读日期：20250811——20250830
学习网站：http://ddia.vonng.com/

# 目录

第一部分：数据系统基础——设计数据密集型应用所赖的基本思想
1. 数据系统架构中的权衡 
2. 定义非功能性需求 
3. 数据模型与查询语言 
4. 存储与检索 
5. 编码与演化
第二部分：分布式数据——存储在一台机器上的数据转向讨论分布在多台机器上的数据
6. 复制 
7. 分片 
8. 事务 
9. 分布式系统的麻烦 
10.  一致性与共识 
第三部分：派生数据——从其他数据集派生出一些数据集的系统。
11. 批处理 
12. 流处理 
13. 数据系统的未来 

# 序言

**数据密集型应用（data-intensive applications）** 正在通过使用这些技术进步来推动可能性的边界。一个应用被称为 **数据密集型** 的，如果 **数据是其主要挑战**（数据量，数据复杂度或数据变化速度）—— 与之相对的是 **计算密集型**，即处理器速度是其瓶颈。

# 第一部分：数据系统基础

## 1、数据系统架构中的权衡 

如果数据管理是开发应用程序的主要挑战之一，我们就称应用程序为 **数据密集型（data-intensive）** 的，关心诸如**存储和处理大量数据、管理数据变更、在面对故障和并发时确保一致性，以及确保服务高可用**等问题。一般需要通过**数据库、缓存、索引、流批处理**来构建。

### 分析型与事务型系统

三类人：
- **后端工程师**：构建处理读取和更新数据请求的服务；这些服务通常直接或间接地通过其他服务为外部用户提供服务
- **业务分析师**：生成关于组织活动的报告，以帮助管理层做出更好的决策
- **数据科学家**：在数据中寻找新的见解，或创建由数据分析和机器学习/AI 支持的面向用户的产品功能

业务分析师和数据科学家两者都执行 **分析**，这意味着他们查看用户和后端服务生成的数据，但他们通常不修改这些数据，这导致了两种类型系统之间的分离：
- **事务型系统** 由后端服务和数据基础设施组成，在这里创建数据，例如通过服务外部用户。在这里，应用程序代码基于用户执行的操作读取和修改其数据库中的数据。
- **分析型系统** 服务于业务分析师和数据科学家的需求。它们包含来自事务型系统的只读数据副本，并针对分析所需的数据处理类型进行了优化。

随着这些系统的成熟，出现了两个新的专业角色：**数据工程师** 和 **分析工程师**。
- **数据工程师**是知道如何集成事务型系统和分析型系统的人，并更广泛地负责组织的数据基础设施
- **分析工程师**对数据进行建模和转换，使其对组织中的业务分析师和数据科学家更有用

#### 事务处理与分析的特征

- **OLTP**：事务型系统通常通过某个键查找少量记录（这称为 **点查询**）。基于用户的输入插入、更新或删除记录。因为这些应用程序是交互式的，这种访问模式被称为 **联机事务处理**（OLTP）
- **OLAP**：分析具有非常不同的访问模式。通常，分析查询会扫描大量记录，并计算聚合统计信息（如计数、求和或平均值），而不是将单个记录返回给用户，被称为 **联机分析处理**（OLAP）

|属性|事务型系统（OLTP）|分析型系统（OLAP）|
|---|---|---|
|主要读取模式|点查询（通过键获取单个记录）|对大量记录进行聚合|
|主要写入模式|创建、更新和删除单个记录|批量导入（ETL）或事件流|
|人类用户示例|Web/移动应用程序的最终用户|内部分析师，用于决策支持|
|机器使用示例|检查操作是否被授权|检测欺诈/滥用模式|
|查询类型|固定的查询集，由应用程序预定义|分析师可以进行任意查询|
|数据代表|数据的最新状态（当前时间点）|随时间发生的事件历史|
|数据集大小|GB 到 TB|TB 到 PB|
#### 数据仓库

90 年代初，公司倾向于停止使用其 OLTP 系统进行分析目的，而是在单独的数据库系统上运行分析。这个单独的数据库被称为 **数据仓库**。有以下几个原因：
- 数据孤岛问题
- 查询模式和数据布局不太合适
- 分析查询相当昂贵

数据仓库包含公司中所有各种 OLTP 系统中数据的只读副本。数据从 OLTP 数据库中提取（使用定期数据转储或连续更新流），转换为分析友好的模式，清理，然后加载到数据仓库中。这种将数据导入数据仓库的过程称为 **提取-转换-加载**（ETL）
![[file-20250811204322471.png]]
一些数据库系统提供 **混合事务/分析处理**（HTAP），旨在在单个系统中启用 OLTP 和分析，而无需从一个系统 ETL 到另一个系统。**HTAP 不会取代数据仓库**。相反，它在同一应用程序需要既执行扫描大量行的分析查询，又以低延迟读取和更新单个记录的场景中很有用。

##### 从数据仓库到数据湖

**数据分析师**：使用通过 SQL 查询的 **关系** 数据模型
**数据科学家**：将数据转换为适合训练机器学习模型的形式（特征工程）；使用自然语言处理技术尝试从中提取结构化信息。


数据科学家不喜欢在数据仓库等关系数据库中工作，更喜欢使用 Python 数据分析库（如 pandas 和 scikit-learn）、统计分析语言（如 R）和分布式分析框架。

组织面临着以适合数据科学家使用的形式提供数据的需求。通过数据湖解决：

**数据湖**：一个集中的数据存储库，保存任何可能对分析有用的数据副本，通过 ETL 过程从事务型系统获得。与数据仓库的区别在于，数据湖只是包含文件，而不强制任何特定的文件格式或数据模型。数据湖中的文件可能是数据库记录的集合，使用 Avro 或 Parquet 等文件格式编码，但它们同样可以包含文本、图像、视频、传感器读数、稀疏矩阵、特征向量、基因组序列或任何其他类型的数据 [15](http://ddia.vonng.com/ch1/#fn:15)。除了更灵活之外，这通常也比关系数据存储更便宜，因为数据湖可以使用商品化的文件存储，如对象存储

ETL 过程已经泛化为 **数据管道**

从数据湖加载数据到单独的数据仓库之外，还可以直接在数据湖中的文件上运行典型的数据仓库工作负载（SQL 查询和业务分析），以及数据科学/机器学习工作负载。这种架构被称为 **数据湖仓**，它需要一个查询执行引擎和一个元数据（例如，模式管理）层来扩展数据湖的文件存储。
**数据湖仓**：数据湖上的仓，是湖也是仓

##### 超越数据湖

在某些情况下，分析系统的输出被提供给事务型系统（这个过程有时被称为 **反向 ETL**）

例如，在分析系统中训练的机器学习模型可能会部署到生产环境中，以便为最终用户生成推荐，例如"购买了 X 的人也购买了 Y"。这种分析系统的部署输出也被称为 **数据产品**

#### 权威数据源与派生数据

**权威记录系统**：权威记录系统，也称为 权威数据源，保存某些数据的权威或 规范 版本。当新数据进入时，例如作为用户输入，它首先写入这里。每个事实只表示一次。如果另一个系统与权威记录系统之间存在任何差异，那么权威记录系统中的值（根据定义）是正确的。

**派生数据系统**：派生系统中的数据是从另一个系统获取一些现有数据并以某种方式转换或处理它的结果。如果你丢失了派生数据，你可以从原始源重新创建它。一个经典的例子是缓存：如果存在，可以从缓存提供数据，但如果缓存不包含你需要的内容，你可以回退到底层数据库。反规范化值、索引、物化视图、转换的数据表示和在数据集上训练的模型也属于这一类别。

- 分析系统通常是派生数据系统，因为它们是在其他地方创建的数据的消费者。
- 事务型服务可能包含权威记录系统和派生数据系统的混合。

### 云服务与自托管

公认的管理智慧是，作为组织核心竞争力或竞争优势的事物应该在内部完成，而非核心、例行或常见的事物应该留给供应商。
 **自托管** 的现成软件（开源或商业），即自己部署

#### 云服务的利弊

云服务的最大缺点是你无法控制它：
- 功能定制开发
- 服务宕机等恢复
- 出错难以诊断
- 存在数据安全问题

#### 云原生系统架构

从头开始设计为云原生的系统已被证明具有几个优势：在相同硬件上具有更好的性能、从故障中更快恢复、 能够快速扩展计算资源以匹配负载，以及支持更大的数据集

| 类别       | 自托管系统                     | 云原生系统                                                     |
| -------- | ------------------------- | --------------------------------------------------------- |
| 事务型/OLTP | MySQL、PostgreSQL、MongoDB  | AWS Aurora 、Azure SQL DB Hyperscale 、Google Cloud Spanner |
| 分析型/OLAP | Teradata、ClickHouse、Spark | Snowflake、Google BigQuery、Azure Synapse Analytics         |
###### 云服务的分层

- 自托管软件倾向于使用非常通用的计算资源：CPU、RAM、文件系统和 IP 网络。
- 云实例可以更快地配置，并且有更多种类的大小，但除此之外，它们与传统计算机类似

云原生服务的关键思想是不仅使用由操作系统管理的计算资源，还基于较低级别的云服务构建更高级别的服务。例如：
- **对象存储** 服务存储大文件，隐藏了底层物理机器，不必担心任何一台机器上的磁盘空间用完，也不会丢失数据。
- 许多其他服务反过来建立在对象存储和其他云服务之上：例如，Snowflake 是一个基于云的分析数据库（数据仓库），依赖于 S3 进行数据存储

###### 存储和计算的分离

**传统计算**：磁盘存储被认为是持久的，为了容忍单个硬盘的故障，通常使用 RAID（独立磁盘冗余阵列）在连接到同一台机器的几个磁盘上维护数据副本。它对访问文件系统的应用程序是透明的。
**云计算**：云原生系统通常将这些磁盘更多地视为临时缓存，而不是长期存储。如果实例被替换，本地磁盘会不可访问

云服务还提供可以从一个实例分离并附加到另一个实例的虚拟磁盘存储，种虚拟磁盘实际上不是物理磁盘，而是由一组单独的机器提供的云服务，它模拟磁盘的行为。但是对网络很敏感。

云原生服务通常避免使用虚拟磁盘，而是建立在针对特定工作负载优化的专用存储服务之上。对象存储服务（如 S3）设计用于长期存储相当大的文件，大小从数百千字节到几千兆字节不等。数据库中存储的单个行或值通常比这小得多；
**云数据库通常在单独的服务中管理较小的值，并将较大的数据块（包含许多单个值）存储在对象存储中**

云原生系统中，存储（磁盘）和计算（CPU 和 RAM）在某种程度上分离或 **解耦**。

#### 云时代的运维

**运维**：确保服务可靠地交付给用户（包括配置基础设施和部署应用程序），并确保稳定的生产环境（包括监控和诊断可能影响可靠性的任何问题）

从单个机器到服务的重点转移伴随着运维角色的变化。提供可靠服务的高级目标保持不变，但流程和工具已经发展。DevOps/SRE 理念更加强调：自动化、频繁更新等等。

云正在改变运维的角色，但对运维的需求比以往任何时候都大。

### 分布式与单节点系统

涉及多台机器通过网络通信的系统称为 **分布式系统**。参与分布式系统的每个进程称为 **节点**。做分布式有很多原因，如：高可用、可伸缩、低延迟、弹性、可持续性等等

#### 分布式系统的问题

- 网络传输中断、响应时间慢、分布式事务等等

#### 微服务与 Serverless

分布系统的最常见方式是：分为客户端和服务器，并让客户端向服务器发出请求。最常见的是使用 HTTP 进行此通信。

这种构建应用程序的方式传统上被称为 **面向服务架构**（SOA），目前被细化为**微服务** 架构。

微服务架构的特点：服务有一个明确定义的目的；每个服务公开一个可以由客户端通过网络调用的 API，每个服务有一个负责其维护的团队。

带来的复杂性：每个服务都需要用于部署新版本、调整分配的硬件资源以匹配负载、收集日志、监控服务健康状况以及在出现问题时向值班工程师发出警报的基础设施。

#### 云计算与超级计算

云计算不是构建大规模计算系统的唯一方式；另一种选择是 **高性能计算**（HPC），也称为 **超级计算**。HPC 通常有不同的优先级并使用不同的技术：
- HPC 通常有不同的优先级并使用不同的技术。
- 通常运行大型批处理作业，定期将其计算状态检查点到磁盘。
- 通常通过共享内存和远程直接内存访问（RDMA）进行通信，这支持高带宽和低延迟

### 数据系统、法律与社会

**通用数据保护条例**（GDPR）：多欧洲国家居民对其个人数据更大的控制权和法律权利，类似的隐私法规已在世界各地的各个国家和州采用，包括例如加州消费者隐私法（CCPA）。关于 AI 的法规，例如 **欧盟 AI 法案**，对个人数据的使用方式施加了进一步的限制。

## 2. 定义非功能性需求

应用程序的非功能需求，比如应用程序应该快速、可靠、安全、合规，并且易于维护

从一个案例研究开始本章，研究社交网络服务可能如何工作，这将提供性能和可伸缩性的实际案例。

### 案例研究：社交网络首页时间线

社交网络服务：假设用户每天发布 5 亿条帖子，或平均每秒 5,700 条帖子。偶尔，速率可能飙升至每秒 150,000 条帖子。

![[file-20250812104503252.png]]
帖子应该是及时的，所以假设在某人发布帖子后，我们希望他们的粉丝能够在 5 秒内看到它。一种方法是让用户的客户端每 5 秒重复上述查询（这称为 _轮询_）。如果我们假设有 1000 万用户同时在线登录，这意味着每秒运行 200 万次查询。即使增加轮询间隔，这也是很大的负载。

#### 时间线的物化与更新

每次用户发布帖子时，我们查找他们的所有粉丝，并将该帖子插入到每个粉丝的首页时间线中——就像向邮箱投递消息一样。现在当用户登录时，我们可以简单地给他们这个预先计算的首页时间线。此外，要接收时间线上任何新帖子的通知，用户的客户端只需订阅添加到其首页时间线的帖子流。
![[file-20250812104720007.png]]这种预先计算和更新查询结果的过程称为 **物化**，时间线缓存是 _物化视图_ 的一个例子

### 描述性能

考虑两种主要的度量类型：
- **响应时间**：从用户发出请求到收到所请求答案的经过时间。
- **吞吐量**：系统正在处理的每秒请求数，或每秒数据量。

随着服务的吞吐量接近其容量，由于排队，响应时间急剧增加。
![[file-20250812105314815.png]]

就性能指标而言，响应时间通常是用户最关心的，而吞吐量决定了所需的计算资源，因此决定了服务特定工作负载的成本。
如果系统的最大吞吐量可以通过添加计算资源显著增加，则称系统为 _可伸缩的_。

#### 延迟与响应时间

- _响应时间_ 是客户端看到的；它包括系统中任何地方产生的所有延迟。
- _服务时间_ 是服务主动处理用户请求的持续时间。
- _排队延迟_ 可能发生在流程中的几个点：例如，在收到请求后，它可能需要等待直到 CPU 可用才能被处理；如果同一台机器上的其他任务通过出站网络接口发送大量数据，响应数据包可能需要在发送之前进行缓冲。
- _延迟_ 是一个涵盖请求未被主动处理时间的总称，即在此期间它是 _潜在的_。特别是，_网络延迟_ 或 _网络延迟_ 指的是请求和响应在网络中传输所花费的时间。
![[file-20250812110210600.png]]
#### 平均值、中位数与百分位数

因为响应时间因请求而异，我们需要将其视为值的 _分布_，而不是单个数字。
**平均响应时间**：对于估计吞吐量限制很有用
**中位数**：如果你将响应时间列表从最快到最慢排序，那么 _中位数_ 就在中间中位数成为了解用户通常需要等待多长时间的良好指标。中位数也称为 _第 50 百分位_，有时缩写为 _p50_。
**百分位数**：为了弄清异常值有多糟糕，_第 95_、_99_ 和 _99.9_ 百分位数很常见，它们是 95%、99% 或 99.9% 的请求比该特定阈值快的响应时间阈值。

响应时间的高百分位数，也称为 _尾部延迟_，很重要，因为它们直接影响用户的服务体验。

#### 响应时间指标的应用

高百分位数在被多次调用作为服务单个最终用户请求的一部分的后端服务中尤其重要。

百分位数通常用于 _服务级别目标_（SLO）和 _服务级别协议_（SLA），作为定义服务预期性能和可用性的方式

### 可靠性与容错

可靠性：即使出现问题也能继续正确工作。
为了更准确地说明出现问题，我们将区分 故障 和 失效：
- **故障**：故障是指系统的某个特定 _部分_ 停止正确工作：例如，如果单个硬盘驱动器发生故障，或单台机器崩溃，或外部服务（系统所依赖的）发生中断。
- **失效**：失效是指 _整个_ 系统停止向用户提供所需的服务；换句话说，当它不满足服务级别目标（SLO）时。

故障和失效之间的区别可能会令人困惑，因为它们在不同层面上是同一件事。
#### 容错

**容错**：如果系统在发生某些故障时仍继续向用户提供所需的服务，我们称系统为 _容错的_。
**单点故障**：如果系统不能容忍某个部分变得有故障，我们称该部分为 _单点故障_（SPOF），因为该部分的故障会升级导致整个系统的失效。
**故障注入**：在这种容错系统中，通过故意触发故障来 _增加_ 故障率是有意义。通过故意引发故障，你确保容错机制不断得到锻炼和测试，这可以增加你对故障自然发生时将被正确处理的信心。
**混沌工程**：一门旨在通过故意注入故障等实验来提高对容错机制的信心的学科。

#### 硬件与软件故障

常见的硬件故障：
- 大约 2-5% 的磁性硬盘驱动器每年发生故障
- 大约 0.5-1% 的固态硬盘（SSD）每年发生故障
- 大约千分之一的机器有一个 CPU 核心偶尔计算错误的结果，可能是由于制造缺陷
- RAM 中的数据也可能被损坏，要么是由于宇宙射线等随机事件，要么是由于永久性物理缺陷。即使使用纠错码（ECC）的内存，超过 1% 的机器在给定年份遇到不可纠正的错误
在大规模系统中，硬件故障发生得足够频繁，以至于它们成为正常系统运行的一部分。

##### 通过冗余容忍硬件故障

我们对不可靠硬件的第一反应通常是向各个硬件组件添加冗余，以降低系统的故障率。
- 磁盘可以设置为 RAID 配置（将数据分布在同一台机器的多个磁盘上，以便故障磁盘不会导致数据丢失）
- 服务器可能有双电源和可热插拔的 CPU

硬件冗余增加了单台机器的正常运行时间,使用分布式系统有一些优势，例如能够容忍一个数据中心的完全中断。

**滚动升级**：如果你需要重新启动机器（例如，应用操作系统安全补丁），单服务器系统需要计划停机时间，而多节点容错系统可以一次修补一个节点，而不影响用户的服务。这称为 _滚动升级_

##### 软件故障

硬件故障大多数都是独立的，软件故障通常是高度相关的。这种故障比不相关的硬件故障更难预料，并且它们往往导致比硬件故障更多的系统失效。
- 在特定情况下导致每个节点同时失效的软件错误。
- 使用某些共享、有限资源（如 CPU 时间、内存、磁盘空间、网络带宽或线程）的失控进程
- 系统所依赖的服务变慢、无响应或开始返回损坏的响应。

软件故障的设计方案：仔细考虑系统中的假设和交互；彻底测试；进程隔离；允许进程崩溃和重新启动；避免反馈循环，如重试风暴

#### 人类与可靠性

在日常业务的务实现实中，组织通常优先考虑创收活动而不是增加其抵御错误的韧性的措施。如果在更多功能和更多测试之间有选择，许多组织可以理解地选择功能。鉴于这种选择，当可预防的错误不可避免地发生时，责怪犯错误的人是没有意义的——问题是组织的优先事项。

管理层应该借此机会从每天与之合作的人的角度了解社会技术系统如何工作的细节，并根据这些反馈采取措施改进它

### 可伸缩性

**可伸缩性**：用来描述系统应对负载增加能力的术语

#### 描述负载

包括：吞吐量的度量、某个变量数量的峰值、还有其他影响访问模式并因此影响可伸缩性要求的负载统计特征

目标是在最小化运行系统成本的同时保持系统性能在 SLA 的要求范围内

#### 共享内存、共享磁盘与无共享架构

增加服务硬件资源的最简单方法是将其移动到更强大的机器。
**纵向伸缩**：购买一台机器（或租用云实例）具有更多 CPU 核心、更多 RAM 和更多磁盘空间。
**共享内存架构**：通过使用多个进程或线程在单台机器上获得并行性；成本增长速度快于线性：具有两倍硬件资源的高端机器通常成本远远超过两倍
**共享磁盘架构**：使用几台具有独立 CPU 和 RAM 的机器，但将数据存储在机器之间共享的磁盘阵列上，这些机器通过快速网络连接：_网络附加存储_（NAS）或 _存储区域网络_（SAN）
**无共享架构**（ _横向伸缩_ 或 _向外扩展_）：具有多个节点的分布式系统，每个节点都有自己的 CPU、RAM 和磁盘。节点之间的任何协调都在软件级别通过传统网络完成。
- 优点：有线性伸缩的潜力，它可以使用提供最佳性价比的任何硬件
- 缺点：显式分片，它会产生分布式系统的所有复杂性

#### 可伸缩性原则

可伸缩性的一个良好通用原则是将系统分解为可以在很大程度上相互独立运行的较小组件
另一个好原则是不要让事情变得比必要的更复杂。如果单机数据库可以完成工作，它可能比复杂的分布式设置更可取。

### 可运维性

软件的大部分成本不在其初始开发中，而在其持续维护中——修复错误、保持其系统运行、调查故障、将其适应新平台、为新用例修改它、偿还技术债务和添加新功能

几个广泛适用的原则：
- **可运维性（Operability）**：使组织容易保持系统平稳运行
- **简单性（Simplicity）**：通过使用易于理解、一致的模式和结构来实施它，并避免不必要的复杂性，使新工程师容易理解系统。
- **可演化性（Evolvability）**：使工程师将来容易对系统进行更改，随着需求变化而适应和扩展它以用于未预料的用例。

#### 可运维性：让运维更轻松

运维很重要：良好的运维通常可以解决糟糕（或不完整）软件的局限性，但再好的软件碰上糟糕的运维也难以可靠地运行

良好的可操作性意味着使常规任务变得容易，使运维团队能够将精力集中在高价值活动上。 数据系统可以做各种事情来使常规任务变得容易，包括：
- 允许监控工具检查系统的关键指标，并支持可观测性工具
- 避免对单个机器的依赖（允许在系统整体继续不间断运行的同时关闭机器进行维护）
- 提供良好的文档和易于理解的操作模型

#### 简单性：管理复杂度

推理复杂性的一种尝试是将其分为两类，**本质复杂性** 和 **偶然复杂性**：
- 本质复杂性是应用程序问题域中固有的
- 偶然复杂性仅由于我们工具的限制而产生
管理复杂性的最佳工具之一是 **抽象**。良好的抽象可以在干净、易于理解的外观后面隐藏大量实现细节。良好的抽象也可以用于各种不同的应用程序。

#### 可演化性：让变化更容易

在组织流程方面，_敏捷_ 工作模式为适应变化提供了框架。敏捷社区还开发了在频繁变化的环境中开发软件时有用的技术工具和流程， 例如测试驱动开发（TDD）和重构。

## 3. 数据模型与查询语言

**数据模型**不仅影响软件的编写方式，还影响我们 **思考问题** 的方式。

如何用更低层次的数据模型来 **表示**应用程序：
- 观察现实世界并用对象或数据结构，以及操作这些数据结构的 API 来建模。这些结构通常是特定于应用程序的。
- 用通用的数据模型来表达它们，例如 JSON 或 XML 文档、关系数据库中的表，或者图中的顶点和边
- 用内存、磁盘或网络上的字节来表示文档/关系/图数据

**基本思想**：每一层通过提供一个简洁的数据模型来隐藏下层的复杂性。

在本章中，通过比较关系模型、文档模型、基于图的数据模型、事件溯源和数据框来探讨这些权衡。

### 关系模型与文档模型

 1970 年Edgar Codd提出最早的的关系模型：SQL，数据被组织成 **关系**（在 SQL 中称为 **表**），其中每个关系是 **元组**（在 SQL 中称为 **行**）的无序集合。
 20 世纪 80 年代中期，关系数据库管理系统（RDBMS）和 SQL 已成为大多数需要存储和查询具有某种规则结构的数据的人的首选工具。 
 20 世纪 70 年代和 80 年代初，**网状模型** 和 **层次模型** 是主要的替代方案，但关系模型最终战胜了它们。
  2010 年代，**NoSQL** 是试图推翻关系数据库主导地位的最新流行词。一些数据库将自己标榜为 _NewSQL_，因为它们旨在提供 NoSQL 系统的可伸缩性以及传统关系数据库的数据模型和事务保证。 NoSQL 和 NewSQL 的想法在数据系统设计中产生了很大的影响，但随着这些原则被广泛采用，这些术语的使用已经减少。
  NoSQL 运动的一个持久影响是 **文档模型** 的流行，它通常将数据表示为 JSON。 这个模型最初由专门的文档数据库（如 MongoDB 和 Couchbase）推广，尽管大多数关系数据库现在也增加了 JSON 支持。
#### 对象关系不匹配

大部分应用程序开发都是使用面向对象的编程语言完成的。如果数据存储在关系表中，则需要在应用程序代码中的对象和数据库的表、行、列模型之间建立一个笨拙的转换层。这种模型之间的脱节有时被称为 **阻抗不匹配**。

##### 对象关系映射（ORM）

对象关系映射（ORM）框架（如 ActiveRecord 和 Hibernate）减少了这个转换层所需的样板代码量，但它们经常受到批评，有如下问题：
- ORM 很复杂，无法完全隐藏两种模型之间的差异
- ORM 通常仅用于 OLTP 应用程序开发，为分析目的提供数据的数据工程师仍然需要使用底层的关系表示
- 许多 ORM 仅适用于关系型 OLTP 数据库
- 一些 ORM 会自动生成关系模式，但这些模式对于直接访问关系数据的用户来说可能很尴尬
- ORM 使得意外编写低效查询变得容易，例如 _N+1 查询问题_
ORM的优势：
- 对于非常适合关系模型的数据，ORM 减少了这种转换所需的样板代码量
- ORM 有助于缓存数据库查询的结果，这可以帮助减少数据库的负载
- ORM 还可以帮助管理模式迁移和其他管理活动

##### 用于一对多关系的文档数据模型

**一对多关系**的关系化表达
![[file-20250812214919026.png]]
JSON 模型能减少应用程序代码和存储层之间的阻抗不匹配，但是JSON 作为数据编码格式也存在问题。JSON 表示具有更好的 _局部性_，所有相关信息都在一个地方，使查询既更快又更简单。
![[file-20250812215109300.png]]

#### 规范化、反规范化与连接

使用 ID，数据更加规范化：对人类有意义的信息只存储在一个地方，所有引用它的地方都使用 ID。当你直接存储文本时，你在使用它的每条记录中都复制了对人类有意义的信息；这种表示是 _反规范化_ 的。

规范化表示的缺点是，每次要显示包含 ID 的记录时，都必须进行额外的查找以将 ID 解析为人类可读的内容。

文档数据库可以存储规范化和反规范化的数据，但它们通常与反规范化相关联 —— 部分是因为 JSON 数据模型使得存储额外的反规范化字段变得容易，部分是因为许多文档数据库中对连接的弱支持使得规范化不方便。

##### 规范化的权衡
作为一般原则，规范化数据通常写入更快（因为只有一个副本），但查询更慢（因为它需要连接）；反规范化数据通常读取更快（连接更少），但写入更昂贵（更多副本要更新，使用更多磁盘空间）

规范化往往更适合 OLTP 系统，其中读取和更新都需要快速；分析系统通常使用反规范化数据表现更好，因为它们批量执行更新，只读查询的性能是主要关注点

在中小规模的系统中，规范化数据模型通常是最好的，因为你不必担心保持数据的多个副本相互一致，执行连接的成本是可以接受的。然而，在非常大规模的系统中，连接的成本可能会成为问题。
##### 社交网络案例研究中的反规范化

通过 ID 查找人类可读信息的过程称为 _hydrating_ ID，它本质上是在应用程序代码中执行的连接

在读取数据时必须执行连接并不像有时声称的那样，是创建高性能、可扩展服务的障碍。
**规范化和反规范化本质上并不好或坏** —— **它们只是在读写性能以及实施工作量方面的权衡**。

#### 多对一与多对多关系

![[Pasted image 20250813140710.png]]
多对一和多对多关系不容易适应一个自包含的 JSON 文档；它们更适合规范化表示。
![[Pasted image 20250813140803.png]]
规范化表示仅在一个地方存储关系，并依赖 _二级索引_来允许有效地双向查询关系。

#### 星型与雪花型：分析模式

数据仓库通常是关系型的，并且数据仓库中表结构有一些广泛使用的约定：_星型模式_、_雪花模式_、_维度建模_ ，以及 _一张大表_（OBT）。

一个可能在杂货零售商的数据仓库中找到的星型模式示例。
![[Pasted image 20250813141442.png]]
一个大型企业可能在其数据仓库中有许多 PB 的交易历史，主要表示为事实表。

事实表中的一些列是属性，例如产品售出的价格和从供应商那里购买它的成本（允许计算利润率）。事实表中的其他列是对其他表的外键引用，称为 **_维度表_**。由于事实表中的每一行代表一个事件，**维度**代表事件的 _谁_、_什么_、_哪里_、_何时_、_如何_ 和 _为什么_。

- **星形模式**：当表关系被可视化时，事实表位于中间，被其维度表包围；到这些表的连接就像星星的光芒。
- **雪花模式**：星型模式的变体，维度被进一步分解为子维度。雪花模式比星型模式更规范化，但星型模式通常更受欢迎，因为它们对分析师来说更简单

星型或雪花模式主要由多对一关系组成，表示为事实表对维度表的外键，或维度对子维度的外键。
原则上，其他类型的关系可能存在，但它们通常被反规范化以简化查询。

一些数据仓库模式进一步进行反规范化，完全省略维度表，将维度中的信息折叠到事实表上的反规范化列中，这种方法被称为 **_一张大表_**（OBT），虽然它需要更多的存储空间，但有时可以实现更快的查询。

#### 何时使用哪种模型

- 文档数据模型的主要论点是模式灵活性、由于局部性而获得更好的性能，以及对于某些应用程序来说，它更接近应用程序使用的对象模型。
- 关系模型通过为连接、多对一和多对多关系提供更好的支持来反击。

应用程序中的数据具有类似文档的结构，可以使用文档模型，将类似文档的结构 _切碎_（shredding）为多个表的关系技术可能导致繁琐的模式和不必要复杂的应用程序代码。
一些应用程序允许用户选择项目的顺序,文档模型很好地支持此类应用程序，因为项目（或它们的 ID）可以简单地存储在 JSON 数组中以确定它们的顺序。

文档模型有局限性：不能直接引用文档中的嵌套项

##### 文档模型中的模式灵活性

**大多数文档数据库以及关系数据库中的 JSON 支持不会对文档中的数据强制执行任何模式。**

- **_读时模式_**：数据的结构是隐式的，只有在读取数据时才解释，文档数据库
- **_写时模式_**：关系数据库的传统方法，其中模式是显式的，数据库确保所有数据在写入时都符合它

读时模式类似于编程语言中的动态（运行时）类型检查，而写时模式类似于静态（编译时）类型检查。

如果集合中的项目由于某种原因并非都具有相同的结构则读时模式方法是有利的 —— 例如，因为：
- 有许多不同类型的对象，将每种类型的对象放在自己的表中是不切实际的。
- 数据的结构由你无法控制且可能随时更改的外部系统决定。
##### 读写的数据局部性

文档通常存储为单个连续字符串，编码为 JSON、XML 或二进制变体（如 MongoDB 的 BSON）。如果你的应用程序经常需要访问整个文档（例如，在网页上渲染它），则这种 **_存储局部性_ 具有性能优势。**

将相关数据存储在一起以获得局部性的想法并不限于文档模型。Google 的 Spanner 数据库在关系数据模型中提供相同的局部性属性，允许模式声明表的行应该交错（嵌套）在父表中；由 Google 的 Bigtable 推广并在 HBase 和 Accumulo 等中使用的 _宽列_ 数据模型具有 _列族_ 的概念，其目的类似于管理局部性。

##### 文档的查询语言

关系数据库和文档数据库之间的另一个区别是你用来查询它的语言或 API。大多数关系数据库使用 SQL 查询，但文档数据库更加多样化。
- XML 数据库通常使用 XQuery 和 XPath 查询，它们旨在允许复杂的查询，包括跨多个文档的连接，并将其结果格式化为 XML
- JSON Pointer 和 JSONPath 为 JSON 提供了等效于 XPath 的功能。
- MongoDB 的聚合管道，其用于连接的 `$lookup` 运算符，是 JSON 文档集合查询语言的一个例子。

##### 文档和关系数据库的融合

关系数据库增加了对 JSON 类型和查询运算符的支持，以及索引文档内属性的能力。一些文档数据库（如 MongoDB、Couchbase 和 RethinkDB）增加了对连接、二级索引和声明式查询语言的支持。

关系-文档混合是一个强大的组合。

### 图数据模型

图由两种对象组成：_顶点_（也称为 _节点_ 或 _实体_）和 _边_（也称为 _关系_ 或 _弧_）。许多类型的数据可以建模为图。典型的例子包括：
- **社交图**：顶点是人，边表示哪些人相互认识。
- **网页图**：顶点是网页，边表示指向其他页面的 HTML 链接。
- **道路或铁路网络**：顶点是交叉点，边表示它们之间的道路或铁路线。

图的表示方式：
- **_邻接表_**：每个顶点存储其相距一条边的邻居顶点的 ID。以使用 _邻接矩阵_，这是一个二维数组，其中每一行和每一列对应一个顶点，当行顶点和列顶点之间没有边时值为零，如果有边则值为一。邻接表适合图遍历，矩阵适合机器学习

同质数据图/非同质数据图

图的构建和查询数据的方式：
- _属性图_ 模型（由 Neo4j、Memgraph、KùzuDB  和其他 实现）
- _三元组存储_ 模型（由 Datomic、AllegroGraph、Blazegraph 和其他实现）
- 图的四种查询语言（Cypher、SPARQL、Datalog 和 GraphQL）

#### 属性图

在 _属性图_（也称为 _标记属性图_）模型中，将图存储视为由两个关系表组成，一个用于顶点，一个用于边。每个顶点包含：
- 唯一标识符
- 标签（字符串），描述此顶点表示的对象类型
- 一组出边
- 一组入边
- 属性集合（键值对）
每条边包含：
- 唯一标识符
- 边开始的顶点（_尾顶点_）
- 边结束的顶点（_头顶点_）
- 描述两个顶点之间关系类型的标签
- 属性集合（键值对）

此模型的一些重要方面是：
1. 任何顶点都可以有一条边将其与任何其他顶点连接。
2. 给定任何顶点，你可以有效地找到其入边和出边，从而 _遍历_ 图
3. 通过对不同类型的顶点和关系使用不同的标签，你可以在单个图中存储几种不同类型的信息，同时仍保持简洁的数据模型。

#### Cypher 查询语言

_Cypher_ 是用于属性图的查询语言，最初为 Neo4j 图数据库创建，后来作为 _openCypher_ 发展为开放标准
除了 Neo4j，Cypher 还得到 Memgraph、KùzuDB、Amazon Neptune、Apache AGE（在 PostgreSQL 中存储）等的支持。它以电影《黑客帝国》中的角色命名，与密码学中的密码无关

#### SQL 中的图查询

在关系数据库中，你通常事先知道查询中需要哪些连接。另一方面，在图查询中，你可能需要遍历可变数量的边才能找到你要查找的顶点 —— 也就是说，连接的数量不是预先固定的。
- 在 Cypher 中，`:WITHIN*0..` 非常简洁地表达了这个事实：它意味着"跟随 `WITHIN` 边，零次或多次"。它就像正则表达式中的 `*` 运算符。
- SQL:1999 以来，查询中可变长度遍历路径的想法可以使用称为 _递归公用表表达式_（`WITH RECURSIVE` 语法）的东西来表达。

4 行 Cypher 查询需要 31 行 SQL 的事实表明，正确选择数据模型和查询语言可以产生多大的差异。

有计划向 SQL 标准添加一种名为 GQL 的图查询语言，它将提供受 Cypher、GSQL  和 PGQL 启发的语法。

#### 三元组存储与 SPARQL

元组存储模型大多等同于属性图模型

在三元组存储中，所有信息都以非常简单的三部分语句的形式存储：（_主语_、_谓语_、_宾语_）。例如，在三元组（_Jim_、_likes_、_bananas_）中，_Jim_ 是主语，_likes_ 是谓语（动词），_bananas_ 是宾语。

三元组的主语等同于图中的顶点。宾语是两种东西之一：
1. 原始数据类型的值，如字符串或数字。在这种情况下，三元组的谓语和宾语等同于主语顶点上属性的键和值。
2. 图中的另一个顶点。在这种情况下，谓语是图中的边，主语是尾顶点，宾语是头顶点。

三元组存储是另一种在其原始用例之外找到用途的语义网技术：即使你对语义网没有兴趣，三元组也可以成为应用程序的良好内部数据模型。

##### RDF 数据模型

Turtle 语言实际上是在 _资源描述框架_（RDF）[55](http://ddia.vonng.com/ch3/#fn:55) 中编码数据的一种方式，这是为语义网设计的数据模型。RDF 数据也可以用其他方式编码，例如（更冗长地）用 XML

##### SPARQL 查询语言

_SPARQL_ 是使用 RDF 数据模型的三元组存储的查询语言（它是 _SPARQL Protocol and RDF Query Language_ 的首字母缩略词，发音为 “sparkle”。）
它早于 Cypher，由于 Cypher 的模式匹配是从 SPARQL 借用的，它们看起来非常相似。

SPARQL 得到 Amazon Neptune、AllegroGraph、Blazegraph、OpenLink Virtuoso、Apache Jena 和各种其他三元组存储的支持

#### Datalog：递归关系查询

Datalog 是一种比 SPARQL 或 Cypher 更古老的语言：它源于 20 世纪 80 年代的学术研究

它是一种非常有表现力的语言，对于复杂查询特别强大。几个小众数据库，包括 Datomic、LogicBlox、CozoDB 和 LinkedIn 的 LIquid使用 Datalog 作为它们的查询语言。

Datalog 实际上基于关系数据模型，递归查询是 Datalog 的特殊优势。

Datalog 数据库的内容由 _事实_ 组成，每个事实对应于关系表中的一行。
Datalog 是 Prolog 的子集，这是一种编程语言，如果你学过计算机科学，你可能见过它。

#### GraphQL

GraphQL 是一种查询语言，从设计上讲，它比我们在本章中看到的其他查询语言限制性更强。
目的是允许在用户设备上运行的客户端软件（如移动应用程序或 JavaScript Web 应用程序前端）请求具有特定结构的 JSON 文档，其中包含渲染其用户界面所需的字段。GraphQL 接口允许开发人员快速更改客户端代码中的查询，而无需更改服务器端 API。

采用 GraphQL 的组织通常需要工具将 GraphQL 查询转换为对内部服务的请求，这些服务通常使用 REST 或 gRPC

GraphQL 不允许递归查询（与 Cypher、SPARQL、SQL 或 Datalog 不同），并且不允许任意搜索条件

### 事件溯源与 CQRS

迄今为止讨论的所有数据模型中，数据以与写入相同的形式被查询 —— 无论是 JSON 文档、表中的行，还是图中的顶点和边。

使用事件作为真相源，并将每个状态变化表达为事件的想法被称为 **_事件溯源_**。
维护单独的读优化表示并从写优化表示派生它们的原则称为 **_命令查询责任分离**（CQRS）_

事件溯源与星型模式事实表之间的相似之处：事件溯源与星型模式事实表之间的相似之处
- 事实表中的行都具有相同的列集
- 事件溯源中可能有许多不同的事件类型，每种都有不同的属性。事实表是无序集合
事件溯源和 CQRS 有几个优点：
- 事件更好地传达了 _为什么_ 发生某事的意图
- 事件溯源的关键原则是物化视图以可重现的方式从事件日志派生
- 可以有多个物化视图，针对应用程序所需的特定查询进行优化。
- 以新方式呈现现有信息，很容易从现有事件日志构建新的物化视图。
事件溯源和 CQRS 也有缺点：
- 如果涉及外部信息，可能会得到不同的结果
- 事件不可变的要求会在事件包含用户的个人数据时产生问题，因为用户可能行使他们的权利（例如，根据 GDPR）请求删除他们的数据
- 如果存在外部可见的副作用，重新处理事件需要小心

有一些专业系统：EventStoreDB、MartenDB（基于 PostgreSQL）和 Axon Framework，还可以使用消息代理（如 Apache Kafka）来存储事件日志

唯一重要的要求是事件存储系统必须保证所有物化视图以与它们在日志中出现的完全相同的顺序处理事件

### 数据框、矩阵与数组

数据框：R 语言、Python 的 Pandas 库、Apache Spark、ArcticDB、Dask 和其他系统支持的数据模型。它们是数据科学家为训练机器学习模型准备数据的流行工具，但它们也广泛用于数据探索、统计数据分析、数据可视化和类似目的。

数据框通常不是通过声明式查询（如 SQL）而是通过一系列修改其结构和内容的命令来操作的。
![[Pasted image 20250813161537.png]]

矩阵只能包含数字，各种技术用于将非数字数据转换为矩阵中的数字。例如：日期缩放、独热编码
数据以数字矩阵的形式存在，它就适合线性代数运算。
数据框足够灵活，允许数据从关系形式逐渐演变为矩阵表示，同时让数据科学家控制最适合实现数据分析或模型训练过程目标的表示。

### 总结

_关系模型_ 尽管已有半个多世纪的历史，但对许多应用来说仍然是一个重要的数据模型——特别是在数据仓库和商业分析中，关系星型或雪花模式和 SQL 查询无处不在。然而，关系数据的几种替代方案也在其他领域变得流行：
- _文档模型_ 针对数据以独立的 JSON 文档形式出现的用例，以及一个文档与另一个文档之间的关系很少的情况。
- _图数据模型_ 走向相反的方向，针对任何东西都可能与一切相关的用例，以及查询可能需要遍历多个跳跃才能找到感兴趣的数据（可以使用 Cypher、SPARQL 或 Datalog 中的递归查询来表达）。
- _数据框_ 将关系数据推广到大量列，从而在数据库和构成大量机器学习、统计数据分析和科学计算基础的多维数组之间提供桥梁。

## 4. 存储与检索

数据库如何存储你提供的数据，以及当你请求时如何再次找到这些数据。
事务型工作负载（OLTP）优化的存储引擎和针对分析型工作负载优化的存储引擎之间存在巨大差异

### OLTP 系统的存储与索引

世界上最简单的数据库

```bash
#!/bin/bash
db_set () {
  echo "$1,$2" >> database
}

db_get () {
  grep "^$1," database | sed -e "s/^$1,//" | tail -n 1
}```

这两个函数实现了一个键值存储。你可以调用 `db_set key value`，它将在数据库中存储 `key` 和 `value`。

为了高效地找到数据库中特定键的值，我们需要一个不同的数据结构：_索引_。

索引是从主数据派生出的 _额外_ 结构。许多数据库允许你添加和删除索引，这不会影响数据库的内容；它只影响查询的性能。
维护额外的结构会产生开销，特别是在写入时。对于写入，很难超越简单地追加到文件的性能，因为这是最简单的写入操作。**任何类型的索引通常都会减慢写入速度**，因为每次写入数据时也需要更新索引。

数据库通常不会默认为所有内容建立索引，而是要求你 —— 编写应用程序或管理数据库的人 —— 使用你对应用程序典型查询模式的了解来手动选择索引。

#### 日志结构存储

继续将数据存储在 `db_set` 写入的仅追加文件中，你只是想加快读取速度。一种方法是在内存中保留一个哈希映射，其中每个键都映射到文件中可以找到该键最新值的字节偏移量
![[Pasted image 20250815105925.png]]
向文件追加新的键值对时，你也会更新哈希映射以反映刚刚写入数据的偏移量。
存在几个问题：
- 永远不会释放被覆盖的旧日志条目占用的磁盘空间；如果你不断写入数据库，可能会耗尽磁盘空间。
- 哈希映射不是持久化的，所以当你重启数据库时必须重建它
- 哈希表必须适合内存。原则上，你可以在磁盘上维护哈希表，但不幸的是，很难让磁盘上的哈希映射表现良好。
- 范围查询效率不高。

##### SSTable 文件格式

实际上，哈希表很少用于数据库索引，相反，保持数据 _按键排序_ 的结构更为常见
这种结构的一个例子是 _排序字符串表_（_Sorted String Table_），简称 _SSTable_，这种文件格式也存储键值对，但它确保它们按键排序，每个键在文件中只出现一次。
![[Pasted image 20250815143608.png]]

将 SSTable 中的键值对分组为几千字节的 _块_，然后在索引中存储每个块的第一个键。这种只存储部分键的索引称为 _稀疏_ 索引。这个索引存储在 SSTable 的单独部分，例如使用不可变 B 树、字典树或其他允许查询快速查找特定键的数据结构。

一个块的第一个键是 `handbag`，下一个块的第一个键是 `handsome`。现在假设你要查找键 `handiwork`，它没有出现在稀疏索引中。由于排序，你知道 `handiwork` 必须出现在 `handbag` 和 `handsome` 之间。这意味着你可以寻找到 `handbag` 的偏移量，然后从那里扫描文件，直到找到 `handiwork`

每个记录块都可以压缩，除了节省磁盘空间外，压缩还减少了 I/O 带宽使用，代价是使用更多一点的 CPU 时间。

##### 构建和合并 SSTable

SSTable 文件格式在读取方面比仅追加日志更好，但它使写入更加困难。
每次在中间某处插入键时都必须重写整个 SSTable，写入将变得太昂贵。

可以用 _日志结构_ 方法解决这个问题，这是仅追加日志和排序文件之间的混合：
1. 当写入操作到来时，将其添加到内存中的有序映射数据结构中，例如红黑树、跳表 或字典树。使用这些数据结构，你可以按任意顺序插入键，高效地查找它们，并按排序顺序读回它们。这个内存数据结构称为 _内存表_（_memtable_）。
2. 当内存表变得大于某个阈值（通常是几兆字节）时，将其按排序顺序作为 SSTable 文件写入磁盘。我们将这个新的 SSTable 文件称为数据库的最新 _段_，它与旧段一起作为单独的文件存储。每个段都有自己内容的单独索引。当新段被写入磁盘时，数据库可以继续写入新的内存表实例，当 SSTable 写入完成时，旧内存表的内存被释放。
3. 为了读取某个键的值，首先尝试在内存表和最新的磁盘段中找到该键。如果没有找到，就在下一个较旧的段中查找，依此类推，直到找到键或到达最旧的段。如果键没有出现在任何段中，则它不存在于数据库中。
4. 不地在后台运行合并和压实过程，以合并段文件并丢弃被覆盖或删除的值。

合并段的工作方式类似于 _归并排序_ 算法：
并排开始读取输入文件，查看每个文件中的第一个键，将最低的键（根据排序顺序）复制到输出文件，然后重复。如果同一个键出现在多个输入文件中，只保留较新的值。这会产生一个新的合并段文件，也按键排序，每个键只有一个值，并且它使用最少的内存，因为我们可以一次遍历一个键的 SSTable。
![[Pasted image 20250815150221.png]]

为了确保数据库崩溃时内存表中的数据不会丢失，存储引擎在磁盘上保留一个单独的日志，每次写入都会立即追加到该日志中。此日志不按键排序，但这无关紧要，因为它的唯一目的是在崩溃后恢复内存表。每次内存表被写出到 SSTable 后，日志的相应部分就可以丢弃。

如果你想删除一个键及其关联的值，你必须向数据文件追加一个称为 _墓碑_（_tombstone_）的特殊删除记录。当日志段合并时，墓碑告诉合并过程丢弃已删除键的任何先前值。一旦墓碑合并到最旧的段中，它就可以被丢弃。

这里描述的算法本质上就是 **RocksDB、Cassandra、Scylla 和 HBase** 中使用的算法，它们都受到 Google 的 Bigtable 论文 9 的启发

1996 年以 _日志结构合并树_（_Log-Structured Merge-Tree_）或 _LSM 树_（_LSM-Tree_）的名称发布，建立在早期日志结构文件系统工作的基础上 。因此，基于合并和压实排序文件原理的存储引擎通常被称为 _LSM 存储引擎_。

在 LSM 存储引擎中，**段文件是一次性写入**的（通过写出内存表或合并一些现有段），此后它是不可变的。段的合并和压实可以在后台线程中完成，当它进行时，我们**仍然可以使用旧的段文件继续提供读取服务**。当合并过程完成时，我们将读取请求切换到使用新的合并段而不是旧段，然后可以删除旧的段文件。

段文件也非常适合写入对象存储。

如果在写出内存表或合并段时发生崩溃，数据库可以删除未完成的 SSTable 并重新开始。

##### 布隆过滤器

使用 LSM 存储，**读取很久以前更新的键或不存在的键可能会很慢**，因为存储引擎需要检查多个段文件。为了加快此类读取，LSM 存储引擎通常在每个段中包含一个 _布隆过滤器_（_Bloom filter_），它提供了一种快速但近似的方法来检查特定键是否出现在特定 SSTable 中。

一个包含两个键和 16 位的布隆过滤器示例：
![[Pasted image 20250815152322.png]]对于 SSTable 中的每个键，我们计算一个哈希函数，产生一组数字，然后将其解释为位数组的索引
将对应于这些索引的位设置为 1，其余保持为 0。例如，键 `handbag` 哈希为数字 (2, 9, 4)，所以我们将第 2、9 和 4 位设置为 1。然后将**位图与键的稀疏索引**一起存储为 SSTable 的一部分。这需要一点额外的空间，但与 SSTable 的其余部分相比，布隆过滤器通常很小。

- **查询不在**：如果至少有一个位是 0，我们知道该键肯定不在 SSTable 中。
- **查询可能在**：如果查询中的位都是 1，那么该键很可能在 SSTable 中，但也有可能是巧合，所有这些位都被其他键设置为 1。这种看起来键存在但实际上不存在的情况称为 **_假阳性_**（_false positive_）。
假阳性的概率取决于键的数量、每个键设置的位数和布隆过滤器中的总位数。

经验法则：为 SSTable 中的每个键分配 10 位布隆过滤器空间以获得 1% 的假阳性概率，每为每个键分配额外的 5 位，概率就会降低十倍。

##### 压实策略

SM 存储如何选择何时执行压实，以及在压实中包括哪些 SSTable。许多基于 LSM 的存储系统允许你配置使用哪种压实策略，常见选择如下：
- **分层压实**：较新和较小的 SSTable 依次合并到较旧和较大的 SSTable 中，可以处理非常高的写入吞吐量
- **分级压实**：键范围被分成较小的 SSTable，较旧的数据被移动到单独的"级别"中，这允许压实更增量地进行，并且比分层策略使用更少的磁盘空间。这种策略对于读取比分层压实更有效，因为存储引擎需要读取更少的 SSTable 来检查它们是否包含该键。

**经验法则**：如果你主要有写入而读取很少，分层压实表现更好，而如果你的工作负载以读取为主，分级压实表现更好。

**嵌入式存储引擎**的例子包括 RocksDB、SQLite、LMDB、DuckDB 和 KùzuDB。在与应用程序代码相同的进程中运行的库，通常读取和写入本地磁盘上的文件，你通过正常的函数调用与它们交互。

#### B 树

按键读取和写入数据库记录最广泛使用的结构是 _B 树_。
1970 年引入B树，几乎所有关系数据库中的标准索引实现，许多非关系数据库也使用它们。
像 SSTable 一样，B 树按键保持键值对排序，这允许高效的键值查找和范围查询

**B 树有着非常不同的设计理念。**

- LSM：将数据库分解为可变大小的 _段_，通常为几兆字节或更大，写入一次后就不可变。
- B树：将数据库分解为固定大小的 _块_ 或 _页_，并可能就地覆盖页。页传统上大小为 4 KiB，但 PostgreSQL 现在默认使用 8 KiB，MySQL 默认使用 16 KiB。

每个页都可以使用页号来标识，这允许一个页引用另一个页 —— 类似于指针，但在磁盘上而不是在内存中。如果所有页都存储在同一个文件中，将页号乘以页大小就给我们文件中页所在位置的字节偏移量。我们可以使用这些页引用来构建页树
![[Pasted image 20250815154238.png]]一个页被指定为 B 树的 _根_；每当你想在索引中查找一个键时，你就从这里开始。该页包含几个键和对子页的引用。每个子负责一个连续的键范围，引用之间的键指示这些范围之间的边界在哪里。

B 树的一个页中对子页的引用数称为 _分支因子_。分支因子取决于存储页引用和范围边界所需的空间量，但通常为几百。

更新 B 树中现有键的值，你搜索包含该键的叶页，并用包含新值的版本覆盖磁盘上的该页。如果你想添加一个新键，你需要找到其范围包含新键的页并将其添加到该页。如果页中没有足够的空闲空间来容纳新键，则页被分成两个半满的页，并更新父页以说明键范围的新细分。
![[Pasted image 20250815154759.png]]想插入键 334，但范围 333–345 的页已经满了。因此，我们将其分成范围 333–337（包括新键）的页和 337–344 的页。我们还必须更新父页以引用两个子页，它们之间的边界值为 337。如果父页没有足够的空间容纳新引用，它也可能需要被分割，分割可以一直持续到树的根。当根被分割时，我们在它上面创建一个新根。

这个算法确保树保持 _平衡_：具有 _n_ 个键的 B 树始终具有 _O_(log _n_) 的深度。大多数数据库可以适合三或四层深的 B 树，所以你不需要跟随许多页引用来找到你要查找的页。

##### 使 B 树可靠

B 树的基本底层写操作是用新数据覆盖磁盘上的页。
一次覆盖多个页，如在页分割中，是一个危险的操作：如果数据库在只写入了部分页后崩溃，你最终会得到一个损坏的树（例如，可能有一个 _孤立_ 页，它不是任何父页的子页）。如果硬件不能原子地写入整个页，你也可能最终得到部分写入的页（这称为 _撕裂页_（_torn page_））。

为了使数据库对崩溃具有弹性，B 树实现通常包括磁盘上的额外数据结构：**_预写日志_**（_write-ahead log_，WAL）。这是一个仅追加文件，每个 B 树修改必须在应用于树本身的页之前写入其中。当数据库在崩溃后恢复时，此日志用于将 B 树恢复到一致状态

为了提高性能，B 树实现通常不会立即将每个修改的页写入磁盘，而是**首先将 B 树页缓冲在内存**中一段时间。预写日志还确保在崩溃的情况下数据不会丢失：**只要数据已写入 WAL，并使用 `fsync()` 系统调用刷新到磁盘，数据就是持久的**，因为数据库将能够在崩溃后恢复它

##### B 树变体

- 一些数据库（如 LMDB）使用写时复制方案，而不是覆盖页并维护 WAL 以进行崩溃恢复。
- 通过不存储整个键而是缩写它来节省页中的空间。特别是在树内部的页中，键只需要提供足够的信息来充当键范围之间的边界。
- 为了加快按排序顺序扫描键范围，一些 B 树实现尝试布局树，使叶页按顺序出现在磁盘上，减少磁盘寻道次数。

#### 比较 B 树与 LSM 树

**经验法则**：LSM 树更适合写入密集型应用，而 B 树对读取更快。
存储引擎有时会混合两种方法的特征，例如具有多个 B 树并以 LSM 风格合并它们。
##### 读取性能

- B树：在 B 树中，查找键涉及在 B 树的每个级别读取一个页。由于级别数通常很小，这意味着从 B 树读取通常很快并且具有可预测的性能。
- LSM：在 LSM 存储引擎中，读取通常必须检查处于不同压实阶段的几个不同 SSTable，但布隆过滤器有助于减少所需的实际磁盘 I/O 操作数。

在 LSM 存储引擎中，读取通常必须检查处于不同压实阶段的几个不同 SSTable，但布隆过滤器有助于减少所需的实际磁盘 I/O 操作数。使得**范围查询在 LSM 方法中比点查询更昂贵**

现代 SSD（特别是 NVMe）可以并行执行许多独立的读请求。LSM 树和 B 树都能够提供高读取吞吐量，但存储引擎需要仔细设计以利用这种并行性

##### 顺序与随机写入

- B树：如果应用程序写入的键分散在整个键空间中，生成的磁盘操作也会随机分散，因为存储引擎需要覆盖的页可能位于磁盘的任何位置。
- LSM：日志结构存储引擎一次写入整个段文件（无论是写出内存表还是压实现有段），这比 B 树中的页大得多。

许多小的、分散的写入模式（如 B 树中的）称为 **_随机写入_**，而较少的大写入模式（如 LSM 树中的）称为 **_顺序写入_**。磁盘通常具有比随机写入更高的顺序写入吞吐量，这意味着**日志结构存储引擎通常可以在相同硬件上处理比 B 树更高的写入吞吐量**。这种差异在旋转磁盘硬盘（HDD）上特别大；在今天大多数数据库使用的固态硬盘（SSD）上，差异较小，但仍然明显

- 旋转磁盘硬盘（HDD）上，**顺序写入比随机写入快得多**：随机写入必须机械地将磁头移动到新位置，并等待盘片的正确部分经过磁头下方，这需要几毫秒 —— 在计算时间尺度上是永恒的。
- SSD（固态硬盘）包括 NVMe（非易失性内存快速，即连接到 PCI Express 总线的闪存）现在已经在许多用例中超越了 HDD，它们不受这种机械限制。SSD 对**顺序写入的吞吐量也高于随机写入**。原因是闪存可以一次读取或写入一页（通常为 4 KiB），但**只能一次擦除一个块**。块中的某些页可能包含有效数据，而其他页可能包含不再需要的数据。在擦除块之前，控制器必须首先将包含有效数据的页移动到其他块中；这个过程称为 **_垃圾回收_**（GC）

##### 写放大

对于任何类型的存储引擎，来自应用程序的一次写请求都会转换为底层磁盘上的多个 I/O 操作。

- 对于 LSM 树，一个值首先被写入日志以保证持久性，然后在内存表写入磁盘时再次写入，并且每次键值对参与压实时再次写入。
- B 树索引必须至少写入每条数据两次：一次写入预写日志，一次写入树页本身。此外，它们有时需要写出整个页，即使该页中只有几个字节发生了变化，以确保 B 树在崩溃或断电后可以正确恢复

如果你获取在某个工作负载中写入磁盘的总字节数，然后除以如果你只是写入没有索引的仅追加日志需要写入的字节数，你就得到了 **_写放大_**。

写放大是 LSM 树和 B 树中的问题。哪个更好取决于各种因素，例如键和值的长度，以及你覆盖现有键与插入新键的频率

LSM 树往往具有较低的写放大，因为它们不必写入整个页，并且可以压缩 SSTable 的块

##### 磁盘空间使用

**_主键_ 索引**：主键唯一标识关系表中的一行，或文档数据库中的一个文档，或图数据库中的一个顶点。数据库中的其他记录可以通过其主键（或 ID）引用该行/文档/顶点，索引用于解析此类引用。

**_二级索引_**：可以使用 `CREATE INDEX` 命令在同一个表上创建多个二级索引，允许你按主键以外的列进行搜索。在二级索引中，索引值不一定是唯一的；也就是说，同一索引条目下可能有许多行（文档、顶点）。这可以通过两种方式解决：要么使索引中的每个值成为匹配行标识符的列表（如全文索引中的倒排列表），要么通过向其追加行标识符使每个条目唯一。具有就地更新的存储引擎（如 B 树）和日志结构存储都可用于实现索引。


##### 在索引中存储值

索引中的键是查询搜索的内容，但值可以是几种东西之一：
- 如果实际数据（行、文档、顶点）直接存储在索引结构中，则称为 _聚簇索引_。例如，在 MySQL 的 InnoDB 存储引擎中，表的主键始终是聚簇索引
- 值可以是对实际数据的引用：要么是相关行的主键（InnoDB 对二级索引这样做），要么是对磁盘上位置的直接引用。在后一种情况下，存储行的地方称为 _堆文件_，它以无特定顺序存储数据（它可能是仅追加的，或者它可能跟踪已删除的行以便稍后用新数据覆盖它们）。例如，Postgres 使用堆文件方法
- 两者之间的折中是 _覆盖索引_ 或 _包含列的索引_，它在索引中存储表的 _某些_ 列，除了在堆上或主键聚簇索引中存储完整行

#### 全内存存储

随着 RAM 变得更便宜，每千兆字节成本的论点被侵蚀。许多数据集根本不是那么大，因此将它们完全保留在内存中是完全可行的，可能分布在几台机器上。这导致了 _内存数据库_ 的发展。

一些内存键值存储，例如 Memcached，仅用于缓存，如果机器重新启动，数据丢失是可以接受的。但其他内存数据库旨在实现持久性，这可以通过特殊硬件（例如电池供电的 RAM）、将更改日志写入磁盘、将定期快照写入磁盘或将内存状态复制到其他机器来实现。

VoltDB、SingleStore 和 Oracle TimesTen 等产品是具有关系模型的内存数据库

Redis 和 Couchbase 通过异步写入磁盘提供弱持久性。

内存数据库的另一个有趣领域是提供难以使用基于磁盘的索引实现的数据模型。例如，Redis 为各种数据结构（例如优先队列和集合）提供类似数据库的接口。因为它将所有数据保留在内存中，其实现相对简单。

### 分析型数据存储

表面上，数据仓库和关系型 OLTP 数据库看起来很相似，因为它们都有 SQL 查询接口。然而，系统的内部可能看起来完全不同，因为它们针对非常不同的查询模式进行了优化。许多数据库供应商现在专注于支持事务处理或分析工作负载，但不是两者兼而有之。

一些数据库，如 Microsoft SQL Server、SAP HANA 和 SingleStore，在同一产品中支持事务处理和数据仓库。

#### 云数据仓库

与传统数据仓库不同，云数据仓库利用可扩展的云基础设施，如对象存储和无服务器计算平台。

Apache Hive、Trino 和 Apache Spark 等开源数据仓库也随着云的发展而发展。随着分析数据存储转移到对象存储上的数据湖，开源仓库已经开始分解：
- **查询引擎**：Trino、Apache DataFusion 和 Presto 等查询引擎解析 SQL 查询，将其优化为执行计划，并针对数据执行它们。执行通常需要并行、分布式数据处理任务。一些查询引擎提供内置任务执行，而其他选择使用第三方执行框架，如 Apache Spark 或 Apache Flink。
- **存储格式**：存储格式确定表的行如何编码为文件中的字节，然后通常存储在对象存储或分布式文件系统中。此类存储格式的示例包括 Parquet、ORC、Lance 或 Nimble
- **表格式**：以 Apache Parquet 和类似存储格式编写的文件一旦编写通常是不可变的。为了支持行插入和删除，使用 Apache Iceberg 或 Databricks 的 Delta 格式等表格式。表格式指定定义哪些文件构成表以及表模式的文件格式。此类格式还提供高级功能，例如时间旅行（查询表在以前时间点的能力）、垃圾回收，甚至事务。
- **数据目录**：就像表格式定义哪些文件构成表一样，数据目录定义哪些表组成数据库。目录用于创建、重命名和删除表。与存储和表格式不同，Snowflake 的 Polaris 和 Databricks 的 Unity Catalog 等数据目录通常作为可以使用 REST 接口查询的独立服务运行。Apache Iceberg 也提供目录，可以在客户端内运行或作为单独的进程运行。

#### 列式存储

数据仓库按照惯例通常使用带有大型事实表的关系模式，该表包含对维度表的外键引用。
在本节中我们将重点关注事实的存储。

典型的数据仓库查询一次只访问其中的 4 或 5 列。
大多数 OLTP 数据库中，存储是以 _面向行_ 的方式布局的：表中一行的所有值彼此相邻存储。文档数据库类似：整个文档通常作为一个连续的字节序列存储。

_面向列_（或 _列式_）存储背后的想法很简单：不要将一行中的所有值存储在一起，而是将每 _列_ 中的所有值存储在一起。如果每列单独存储，查询只需要读取和解析该查询中使用的那些列，这可以节省大量工作。
![[Pasted image 20250815191357.png]]

列式存储引擎并不真的一次存储整个列（可能包含数万亿行）。相反，它们将表分解为数千或数百万行的块，并且在每个块内，它们分别存储每列的值

由于许多查询都限制在特定的日期范围内，因此通常使每个块包含特定时间戳范围的行。然后查询只需要在与所需日期范围重叠的那些块中加载它需要的列。

列式存储如今几乎用于所有分析数据库，从大规模云数据仓库（如 Snowflake ）到单节点嵌入式数据库（如 DuckDB ），以及产品分析系统（如 Pinot 和 Druid ）。它用于存储格式，如 Parquet、ORC 、Lance 和 Nimble，以及内存分析格式，如 Apache Arrow   和 Pandas/NumPy。一些时间序列数据库，如 InfluxDB IOx 和 TimescaleDB，也基于面向列的存储。

##### 列压缩

在数据仓库中特别有效的一种技术是 _位图编码_，可以将具有 _n_ 个不同值的列转换为 _n_ 个单独的位图：每个不同值一个位图，每行一位。如果该行具有该值，则该位为 1，否则为 0。
![[file-20250816215228398.png]]


位图索引非常适合数据仓库中常见的查询类型，位图也可用于回答图查询

不要将**面向列的数据库**与 **_宽列_（也称为 _列族_）**数据模型**混淆，在该模型中，一行可以有数千列，并且不需要所有行都有相同的列。
尽管名称相似，宽列数据库是面向行的，因为它们将一行中的所有值存储在一起。Google 的 Bigtable、Apache Accumulo 和 HBase 是宽列模型的例子。

##### 列存储中的排序顺序

在列存储中，行的存储顺序并不一定重要。最简单的是按插入顺序存储它们，因为这样插入新行只需追加到每列。独立排序每列是没有意义的

数据库管理员可以使用他们对常见查询的了解来选择表应按哪些列排序。

##### 写入列式存储

使用列式存储，在排序表的中间某处写入单个行将非常低效，因为你必须从插入位置开始重写所有压缩列。但是，一次批量写入许多行会分摊重写这些列的成本，使其高效。

通常使用日志结构方法以批次执行写入。所有写入首先进入面向行的、排序的内存存储。当积累了足够的写入时，它们将与磁盘上的列编码文件合并，并批量写入新文件。由于旧文件保持不可变，新文件一次写入，对象存储非常适合存储这些文件。

#### 查询执行：编译与向量化

用于分析的复杂 SQL 查询被分解为由多个阶段组成的 _查询计划_，称为 _算子_，这些算子可能分布在多台机器上以并行执行。
查询规划器可以通过选择使用哪些算子、以何种顺序执行它们以及在哪里运行每个算子来执行大量优化。

高效查询执行的两种替代方法已经出现：
- **查询编译**：查询引擎获取 SQL 查询并生成用于执行它的代码。代码逐行迭代，查看感兴趣列中的值，执行所需的任何比较或计算，如果满足所需条件，则将必要的值复制到输出缓冲区。查询引擎将生成的代码编译为机器代码（通常使用现有编译器，如 LLVM），然后在已加载到内存中的列编码数据上运行它。
- **向量化处理**：查询被解释，而不是编译，但通过批量处理列中的许多值而不是逐行迭代来提高速度。一组固定的预定义算子内置在数据库中；我们可以向它们传递参数并获得一批结果

两个位图之间的按位 AND 适合向量化。可以将 `product_sk` 列和"香蕉"的 ID 传递给相等算子，并获得一个位图（输入列中每个值一位，如果是香蕉则为 1）；然后我们可以将 `store_sk` 列和感兴趣商店的 ID 传递给相同的相等算子，并获得另一个位图；然后我们可以将两个位图传递给"按位 AND"算子，结果将是一个位图，包含特定商店中所有香蕉销售的 1。
![[file-20250816220235402.png]]

两者都可以通过利用现代 CPU 的特性来实现非常好的性能：
- 优先选择顺序内存访问而不是随机访问以减少缓存未命中
- 在紧密的内部循环中完成大部分工作（即，具有少量指令且没有函数调用）以保持 CPU 指令处理管道繁忙并避免分支预测错误
- 利用并行性，例如多线程和单指令多数据（SIMD）指令
- 直接对压缩数据进行操作，而无需将其解码为单独的内存表示

#### 物化视图与多维数据集

 **_物化视图_**：在关系数据模型中，它们是表状对象，其内容是某些查询的结果。**物化视图是查询结果的实际副本**，写入磁盘，而虚拟视图只是编写查询的快捷方式。当你从虚拟视图读取时，SQL 引擎会即时将其扩展为视图的基础查询，然后处理扩展的查询。

物化视图可以改善在重复需要执行相同查询的工作负载中的读取性能。

**_物化聚合_** 是一种可以在数据仓库中有用的物化视图类型。

_多维数据集_ 或 _OLAP 立方体_ 通过创建按不同维度分组的聚合网格来缓存查询最常使用的一些计数或总和。
![[file-20250816220810602.png]]
现在假设每个事实只有两个维度表的外键 —— 在图 中，这些是 `date_key` 和 `product_sk`。你现在可以绘制一个二维表，日期沿着一个轴，产品沿着另一个轴。每个单元格包含具有该日期-产品组合的所有事实的属性（例如 `net_price`）的聚合（例如 `SUM`）。然后，你可以沿着每行或列应用相同的聚合，并获得已减少一个维度的摘要（不管日期的产品销售，或不管产品的日期销售）。

物化多维数据集的优点是某些查询变得非常快，因为它们已经有效地预先计算了。例如，如果你想知道昨天每个商店的总销售额，你只需要查看适当维度的总计 —— 不需要扫描数百万行。

缺点是多维数据集没有与查询原始数据相同的灵活性。

大多数数据仓库尽可能多地保留原始数据，并仅将聚合（如多维数据集）用作某些查询的性能提升。

### 多维索引与全文索引

最常见的多列索引类型称为 **_联合索引_**，它通过将一列追加到另一列来将几个字段组合成一个键（索引定义指定字段以何种顺序连接）。**_多维索引_** 允许你一次查询多个列。在地理空间数据中这尤其重要。

#### 全文检索

**全文检索**：允许你通过可能出现在文本中任何位置的关键字搜索文本文档集合（网页、产品描述等）

可以将全文检索视为另一种多维查询：在这种情况下，可能出现在文本中的每个单词（_词项_）是一个维度。包含词项 _x_ 的文档在维度 _x_ 中的值为 1，不包含 _x_ 的文档的值为 0。

许多搜索引擎用来回答此类查询的数据结构称为 _倒排索引_。这是一个键值结构，其中键是词项，值是包含该词项的所有文档的 ID 列表（_倒排列表_）。如果文档 ID 是顺序数字，倒排列表也可以表示为稀疏位图。

Elasticsearch 和 Solr 使用的全文索引引擎 Lucene 就是这样工作的。它将词项到倒排列表的映射存储在类似 SSTable 的排序文件中，这些文件使用我们在本章前面看到的相同日志结构方法在后台合并

除了将文本分解为单词，另一种选择是查找长度为 _n_ 的所有子字符串，称为 _n_ 元语法。例如，字符串 `"hello"` 的三元语法（_n_ = 3）是 `"hel"`、`"ell"` 和 `"llo"`

为了处理文档或查询中的拼写错误，Lucene 能够在一定编辑距离内搜索文本中的单词（编辑距离为 1 意味着已添加、删除或替换了一个字母）.它通过将词项集存储为字符上的有限状态自动机（类似于 _字典树_ )并将其转换为 _莱文斯坦自动机_ 来实现，该自动机支持在给定编辑距离内高效搜索单词。

#### 向量嵌入

为了理解文档的语义 —— 它的含义 —— 语义搜索索引使用嵌入模型将文档转换为浮点值向量，称为 **_向量嵌入_**。向量表示多维空间中的一个点，每个浮点值表示文档沿着一个维度轴的位置。嵌入模型生成的向量嵌入在（这个多维空间中）彼此接近，当嵌入的输入文档在语义上相似时。

索引擎使用距离函数（如余弦相似度或欧几里得距离）来测量向量之间的距离。余弦相似度测量两个向量角度的余弦以确定它们的接近程度，而欧几里得距离测量空间中两点之间的直线距离。

许多早期的嵌入模型，如 Word2Vec、BERT 和 GPT 都处理文本数据。这些模型通常实现为神经网络。研究人员继续为视频、音频和图像创建嵌入模型。最近，模型架构已经变成 _多模态_ 的：单个模型可以为多种模态（如文本和图像）生成向量嵌入。

语义搜索引擎在用户输入查询时使用嵌入模型生成向量嵌入。用户的查询和相关上下文（例如用户的位置）被输入到嵌入模型中。嵌入模型生成查询的向量嵌入后，搜索引擎必须使用向量索引找到具有相似向量嵌入的文档。

向量索引存储文档集合的向量嵌入。要查询索引，你传入查询的向量嵌入，索引返回其向量最接近查询向量的文档。使用专门的向量索引，例如：
- **平面索引**：向量按原样存储在索引中。查询必须读取每个向量并测量其与查询向量的距离。平面索引是准确的，但测量查询与每个向量之间的距离很慢。
- **倒排文件（IVF）索引**：向量空间被聚类为向量的分区（称为 _质心_），以减少必须比较的向量数量。IVF 索引比平面索引更快，但只能给出近似结果：即使查询和文档彼此接近，它们也可能落入不同的分区。
- **分层可导航小世界**：HNSW 索引维护向量空间的多个层，每一层都表示为一个图，其中节点表示向量，边表示与附近向量的接近度。与 IVF 索引一样，HNSW 索引是近似的。
![[file-20250816223315450.png]]
## 5. 编码与演化

新旧版本的代码，以及新旧数据格式，可能会同时在系统中共存。为了使系统继续平稳运行，我们需要在两个方向上保持兼容性：
- **向后兼容性**：较新的代码可以读取由较旧代码写入的数据。
- **向前兼容性**：较旧的代码可以读取由较新代码写入的数据。
向前兼容性可能更棘手，因为它需要旧代码忽略新版本代码添加的部分。
![[file-20250817161737123.png]]
当旧版本的应用程序更新之前由新版本应用程序写入的数据时，如果不小心，数据可能会丢失。

- 将研究几种编码数据的格式，包括 JSON、XML、Protocol Buffers 和 Avro。特别是，我们将研究它们如何处理模式变化，以及它们如何支持新旧数据和代码需要共存的系统。
- 将讨论这些格式如何用于数据存储和通信：在数据库、Web 服务、REST API、远程过程调用（RPC）、工作流引擎以及事件驱动系统（如 actor 和消息队列）中。

### 编码数据的格式

程序通常以（至少）两种不同的表示形式处理数据：
1. 在内存中，数据保存在对象、结构体、列表、数组、哈希表、树等中。这些数据结构针对 CPU 的高效访问和操作进行了优化（通常使用指针）。
2. 当你想要将数据写入文件或通过网络发送时，必须将其编码为某种自包含的字节序列（例如，JSON 文档）。由于指针对任何其他进程都没有意义，因此这种字节序列表示通常与内存中常用的数据结构看起来截然不同。

需要在两种表示之间进行某种转换。从**内存**表示到**字节序列**的转换称为 **_编码_**（也称为 _序列化_ 或 _编组_），反向过程称为 **_解码_**（_解析_、_反序列化_、_反编组_）。

多数系统需要在内存对象和平面字节序列之间进行转换

#### 特定语言的格式

许多编程语言都内置了将内存对象编码为字节序列的支持。例如，Java 有 `java.io.Serializable`，Python 有 `pickle`，Ruby 有 `Marshal`，等等。许多第三方库也存在，例如 Java 的 Kryo。

有许多深层次的问题：
- 编码通常与特定的编程语言绑定，用另一种语言读取数据非常困难
- 为了以相同的对象类型恢复数据，解码过程需要能够实例化任意类。安全问题的来源
- 在这些库中，数据版本控制通常是事后考虑的：由于它们旨在快速轻松地编码数据，因此它们经常忽略向前和向后兼容性的不便问题
- Java 的内置序列化因其糟糕的性能和臃肿的编码而臭名昭著
除了非常临时的目的外，使用语言的内置编码通常是个坏主意

#### JSON、XML 及其二进制变体

当转向可以由许多编程语言编写和读取的标准化编码时，JSON 和 XML 是显而易见的竞争者。
- XML 经常因过于冗长和不必要的复杂而受到批评
- JSON 的流行主要是由于它在 Web 浏览器中的内置支持以及相对于 XML 的简单性。
- CSV 是另一种流行的与语言无关的格式，但它只支持表格数据而不支持嵌套。

除了表面的语法问题之外，它们还有一些微妙的问题：
- 数字的编码有很多歧义，无法区分数字和恰好由数字组成的字符串（除非引用外部模式）
- JSON 和 XML 对 Unicode 字符串（即人类可读文本）有很好的支持，但它们不支持二进制字符串（没有字符编码的字节序列）。通过使用 Base64 将二进制数据编码为文本来绕过这个限制，有点取巧，并且会将数据大小增加 33%。
- XML 模式和 JSON 模式功能强大，因此学习和实现起来相当复杂。
- CSV 没有任何模式，因此应用程序需要定义每行和每列的含义。
JSON、XML 和 CSV 对许多目的来说已经足够好了

##### JSON 模式

JSON 模式已被广泛采用，作为系统间交换或写入存储时对数据建模的一种方式。

JSON 模式规范提供了许多功能。模式包括标准原始类型，包括字符串、数字、整数、对象、数组、布尔值或空值。

JSON 模式还提供了一个单独的验证规范，允许开发人员在字段上叠加约束。例如，`port` 字段可能具有最小值 1 和最大值 65535。

JSON 模式可以具有开放或封闭的内容模型。开放内容模型允许模式中未定义的任何字段以任何数据类型存在，而封闭内容模型只允许显式定义的字段。JSON 模式中的开放内容模型在 `additionalProperties` 设置为 `true` 时启用，这是默认值。

除了开放和封闭内容模型以及验证器之外，JSON 模式还支持条件 if/else 模式逻辑、命名类型、对远程模式的引用等等。所有这些都构成了一种非常强大的模式语言。

##### 二进制编码

JSON 比 XML 更简洁，但与二进制格式相比，两者仍然使用大量空间。这一观察导致了大量 JSON 二进制编码（MessagePack、CBOR、BSON、BJSON、UBJSON、BISON、Hessian 和 Smile 等等）和 XML 二进制编码（例如 WBXML 和 Fast Infoset）的发展。

二进制编码更紧凑，有时解析速度更快。


JSON 的二进制编码示例：
二进制编码长度为 66 字节，仅比文本 JSON 编码（去除空格后）占用的 81 字节少一点。
![[file-20250817163825592.png]]

#### Protocol Buffers

**Protocol Buffers** (protobuf) 是 Google 开发的二进制编码库，类似于 Apache Thrift。

Protocol Buffers 需要为任何编码的数据提供模式。

它接受显示的模式定义，并生成以各种编程语言实现该模式的类。你的应用程序代码可以调用此生成的代码来编码或解码模式的记录。使用 Protocol Buffers 编码器编码需要 33 字节
![[file-20250817164102467.png]]

最大区别是没有字段名

Protocol Buffers 通过将字段类型和标签号打包到单个字节中来节省更多空间

Protocol Buffers 没有显式的列表或数组数据类型。

##### 字段标签与模式演化

Protocol Buffers 如何在保持向后和向前兼容性的同时处理模式更改？

字段标签对编码数据的含义至关重要。
编码记录只是其编码字段的串联。每个字段由其标签号（示例模式中的数字 `1`、`2`、`3`）标识，并带有数据类型注释（例如字符串或整数）。更改模式中字段的名称，因为编码数据从不引用字段名，但你不能更改字段的标签，因为这会使所有现有的编码数据无效。

**向前兼容性**：数据类型注释允许解析器确定需要跳过多少字节，并保留未知字段,保持了向前兼容性：旧代码可以读取由新代码编写的记录。
**向后兼容性**：只要每个字段都有唯一的标签号，新代码总是可以读取旧数据，因为标签号仍然具有相同的含义。如果在新模式中添加了字段，而你读取尚未包含该字段的旧数据，则它将填充默认值

#### Avro

**Apache Avro**： 是另一种二进制编码格式，与 Protocol Buffers 有着有趣的不同。它于 2009 年作为 Hadoop 的子项目启动，因为 **Protocol Buffers 不太适合 Hadoop 的用例**

Avro 也使用模式来指定正在编码的数据的结构。它有两种模式语言：一种（Avro IDL）用于人工编辑，另一种（基于 JSON）更容易被机器读取。

Avro 二进制编码只有 32 字节长——是我们看到的所有编码中最紧凑的。

编码只是由串联在一起的值组成。字符串只是一个长度前缀，后跟 UTF-8 字节，但编码数据中没有任何内容告诉你它是字符串。它也可能是整数，或完全是其他东西。整数使用可变长度编码进行编码。
![[file-20250817170306643.png]]

Avro 如何支持模式演化？

##### 写入者模式与读取者模式

**_写入者模式_**：当应用程序想要编码一些数据（将其写入文件或数据库，通过网络发送等）时，它使用它知道的任何版本的模式对数据进行编码——例如，该模式可能被编译到应用程序中。这被称为 _写入者模式_。

当应用程序想要解码一些数据（从文件或数据库读取，从网络接收等）时，它使用两个模式：与用于编码相同的写入者模式，以及 **_读取者模式_**，后者可能不同。读取者模式定义了应用程序代码期望的每条记录的字段及其类型。

![[file-20250817170503388.png]]
在 Protocol Buffers 中，编码和解码可以使用不同版本的模式。在 Avro 中，解码使用两个模式：写入者模式必须与用于编码的模式相同，但读取者模式可以是较旧或较新的版本。

如果读取者模式和写入者模式相同，解码很容易。如果它们不同，Avro 通过并排查看写入者模式和读取者模式并将数据从写入者模式转换为读取者模式来解决差异。

如果写入者模式和读取者模式的字段顺序不同，这没有问题，因为模式解析通过字段名匹配字段。如果读取数据的代码遇到出现在写入者模式中但不在读取者模式中的字段，它将被忽略。如果读取数据的代码期望某个字段，但写入者模式不包含该名称的字段，则使用读取者模式中声明的默认值填充它。

![[file-20250817170900613.png]]

##### 模式演化规则

使用 Avro，向前兼容性意味着你可以将新版本的模式作为写入者，将旧版本的模式作为读取者。相反，向后兼容性意味着你可以将新版本的模式作为读取者，将旧版本作为写入者。
为了保持兼容性，你只能添加或删除具有默认值的字段。

更改字段的数据类型是可能的，前提是 Avro 可以转换该类型。更改字段的名称是可能的，但有点棘手：读取者模式可以包含字段名的别名，因此它可以将旧写入者的模式字段名与别名匹配。这意味着更改字段名是向后兼容的，但不是向前兼容的。同样，向联合类型添加分支是向后兼容的，但不是向前兼容的。

#### 但什么是写入者模式？

读取者如何知道特定数据是用哪个写入者模式编码的？我们不能只在每条记录中包含整个模式，因为模式可能比编码数据大得多，使二进制编码节省的所有空间都白费了。

答案取决于 Avro 的使用环境。举几个例子：
- **包含大量记录的大文件**：Avro 的一个常见用途是存储包含数百万条记录的大文件，所有记录都使用相同的模式编码。在这种情况下，该文件的写入者可以在文件开头只包含一次写入者模式。
- **具有单独写入记录的数据库**：在数据库中，不同的记录可能在不同的时间点使用不同的写入者模式编写——你不能假定所有记录都具有相同的模式。最简单的解决方案是在每个编码记录的开头包含一个版本号，并在数据库中保留模式版本列表。读取者可以获取记录，提取版本号，然后从数据库中获取该版本号的写入者模式。使用该写入者模式，它可以解码记录的其余部分。
- **通过网络连接发送记录**：当两个进程通过双向网络连接进行通信时，它们可以在连接设置时协商模式版本，然后在连接的生命周期内使用该模式。

##### 动态生成的模式

与 Protocol Buffers 相比，Avro 方法的一个优点是模式不包含任何标签号。

区别在于 Avro 对 _动态生成_ 的模式更友好。

如果你使用 Avro，你可以相当容易地从关系模式生成 Avro 模式（我们之前看到的 JSON 表示），并使用该模式对数据库内容进行编码，将其全部转储到 Avro 对象容器文件中。可以为每个数据库表生成记录模式，每列成为该记录中的一个字段。数据库中的列名映射到 Avro 中的字段名。

如果数据库模式发生变化，你可以从更新的数据库模式生成新的 Avro 模式，并以新的 Avro 模式导出数据。数据导出过程不需要关注模式更改——它可以在每次运行时简单地进行模式转换。

相比之下，如果你为此目的使用 Protocol Buffers，字段标签可能必须手动分配：每次数据库模式更改时，管理员都必须手动更新从数据库列名到字段标签的映射。这种动态生成的模式根本不是 Protocol Buffers 的设计目标，而 Avro 则是。

#### 模式的优点

Protocol Buffers 和 Avro 都使用模式来描述二进制编码格式。由于 Protocol Buffers 和 Avro 更简单实现和使用，它们已经发展到支持相当广泛的编程语言。

许多数据系统也为其数据实现某种专有二进制编码。例如，大多数关系数据库都有一个网络协议，你可以通过它向数据库发送查询并获取响应。这些协议通常特定于特定数据库，数据库供应商提供驱动程序（例如，使用 ODBC 或 JDBC API），将数据库网络协议的响应解码为内存数据结构。

文本数据格式（如 JSON、XML 和 CSV）广泛存在，但基于模式的二进制编码也是一个可行的选择。它们具有许多良好的属性：
- 它们可以比各种"二进制 JSON"变体紧凑得多，因为它们可以从编码数据中省略字段名。
- 模式是一种有价值的文档形式，并且由于解码需要模式，因此你可以确保它是最新的
- 保留模式数据库允许你在部署任何内容之前检查模式更改的向前和向后兼容性。

模式演化允许与无模式/读时模式 JSON 数据库相同的灵活性，同时还提供更好的数据保证和更好的工具。

### 数据流的模式

数据可以通过许多方式从一个进程流向另一个进程。谁编码数据，谁解码数据？数据在进程之间流动的一些最常见方式：

- 通过数据库
- 通过服务调用
- 通过工作流引擎
- 通过异步消息 

#### 流经数据库的数据流

在数据库中，写入数据库的进程对数据进行编码，从数据库读取的进程对其进行解码。可能只有一个进程访问数据库，在这种情况下，读取者只是同一进程的后续版本——在这种情况下，你可以将在数据库中存储某些内容视为 _向未来的自己发送消息_。

向后兼容性在这里显然是必要的；否则你未来的自己将无法解码你之前写的内容。
数据库中的值可能由 _较新_ 版本的代码写入，随后由仍在运行的 _较旧_ 版本的代码读取。因此，数据库通常也需要向前兼容性。

##### 不同时间写入的不同值

数据库通常允许在任何时间更新任何值。这意味着在单个数据库中，你可能有一些五毫秒前写入的值，以及一些五年前写入的值。

大多数关系数据库允许简单的模式更改，例如添加具有 `null` 默认值的新列，而无需重写现有数据。从磁盘上的编码数据中缺少的任何列读取旧行时，数据库会为其填充 `null`。因此，模式演化允许整个数据库看起来好像是用单个模式编码的，即使底层存储可能包含用各种历史版本的模式编码的记录。

更复杂的模式更改——例如，将单值属性更改为多值，或将某些数据移动到单独的表中——仍然需要重写数据，通常在应用程序级别。在此类迁移中保持向前和向后兼容性仍然是一个研究问题

#### 归档存储

由于数据转储是一次性写入的，此后是不可变的，因此像 Avro 对象容器文件这样的格式非常适合。这也是将数据编码为分析友好的列式格式（如 Parquet）的好机会

#### 流经服务的数据流：REST 与 RPC

当你有需要通过网络进行通信的进程时，有几种不同的方式来安排这种通信。最常见的安排是有两个角色：_客户端_ 和 _服务器_。服务器通过网络公开 API，客户端可以连接到服务器以向该 API 发出请求。服务器公开的 API 称为 _服务_。

在某些方面，服务类似于数据库：它们通常允许客户端提交和查询数据。
面向服务/微服务架构的一个关键设计目标是通过使服务可独立部署和演化来使应用程序更容易更改和维护。一个常见的原则是每个服务应该由一个团队拥有，该团队应该能够频繁发布服务的新版本，而无需与其他团队协调。


##### Web 服务

当 HTTP 用作与服务通信的底层协议时，它被称为 _Web 服务_。
 Web 服务不仅用于 Web，还用于几种不同的上下文：
 1. 在用户设备上运行的客户端应用程序（例如，移动设备上的原生应用程序，或浏览器中的 JavaScript Web 应用程序）向服务发出 HTTP 请求。这些请求通常通过公共互联网进行。
2. 一个服务向同一组织拥有的另一个服务发出请求，通常位于同一数据中心内，作为面向服务/微服务架构的一部分。
3. 一个服务向不同组织拥有的服务发出请求，通常通过互联网。这用于不同组织后端系统之间的数据交换。此类别包括在线服务提供的公共 API，例如信用卡处理系统或用于共享访问用户数据的 OAuth。

 最流行的服务设计理念是 REST，它建立在 HTTP 的原则之上。它强调简单的数据格式，使用 URL 来标识资源，并使用 HTTP 功能进行缓存控制、身份验证和内容类型协商。根据 REST 原则设计的 API 称为 _RESTful_。

服务采用 RESTful 设计原则，客户端也需要以某种方式找出这些详细信息。服务开发人员通常使用接口定义语言（IDL）来定义和记录其服务的 API 端点和数据模型，并随着时间的推移演化它们。其他开发人员可以使用服务定义来确定如何查询服务。两种最流行的服务 IDL 是 OpenAPI（也称为 Swagger）和 gRPC。
- OpenAPI 用于发送和接收 JSON 数据的 Web 服务
- 而 gRPC 服务发送和接收 Protocol Buffers。

即使采用了设计理念和 IDL，开发人员仍必须编写实现其服务 API 调用的代码。通常采用服务框架来简化这项工作。Spring Boot、FastAPI 和 gRPC 等服务框架允许开发人员为每个 API 端点编写业务逻辑，而框架代码处理路由、指标、缓存、身份验证等。

许多框架将服务定义和服务器代码耦合在一起。在某些情况下，例如流行的 Python FastAPI 框架，服务器是用代码编写的，IDL 会自动生成。在其他情况下，例如 gRPC，首先编写服务定义，然后生成服务器代码脚手架。两种方法都允许开发人员从服务定义生成各种语言的客户端库和 SDK。除了代码生成之外，Swagger 等 IDL 工具还可以生成文档、验证模式更改兼容性，并为开发人员提供查询和测试服务的图形用户界面。
#### 远程过程调用（RPC）的问题

Web 服务只是通过网络进行 API 请求的一长串技术的最新化身。
所有这些都基于 _远程过程调用_ (RPC) 的想法，这个想法自 1970 年代以来就存在了。
RPC 模型试图使向远程网络服务的请求看起来与在编程语言中调用函数或方法相同，在同一进程内（这种抽象称为 _位置透明性_）。尽管 RPC 起初似乎很方便，但这种方法从根本上是有缺陷的。

网络请求与本地函数调用非常不同：
- 本地函数调用是可预测的，要么成功要么失败，仅取决于你控制的参数。网络请求是不可预测的：由于网络问题，请求或响应可能会丢失，或者远程机器可能速度慢或不可用，而这些问题完全超出了你的控制。
- 本地函数调用要么返回结果，要么抛出异常，要么永不返回（因为它进入无限循环或进程崩溃）。网络请求有另一种可能的结果：它可能由于 _超时_ 而没有返回结果。在这种情况下，你根本不知道发生了什么。
- 如果你重试失败的网络请求，可能会发生前一个请求实际上已经通过，只是响应丢失了。在这种情况下，重试将导致操作执行多次，除非你在协议中构建去重机制（_幂等性_）
- 每次调用本地函数时，通常需要大约相同的时间来执行。网络请求比函数调用慢得多，其延迟也变化很大：在良好的时候，它可能在不到一毫秒内完成，但当网络拥塞或远程服务过载时，执行完全相同的操作可能需要许多秒。
- 当你调用本地函数时，你可以有效地将引用（指针）传递给本地内存中的对象。当你发出网络请求时，所有这些参数都需要编码为可以通过网络发送的字节序列。
- 客户端和服务可能以不同的编程语言实现，因此 RPC 框架必须将数据类型从一种语言转换为另一种语言。这可能会变得很丑陋

REST 的部分吸引力在于它将网络上的状态传输视为与函数调用不同的过程。

##### 负载均衡器、服务发现和服务网格


**_服务发现_**：所有服务都通过网络进行通信。因此，客户端必须知道它正在连接的服务的地址——这个问题称为 _服务发现_。

**_负载均衡_**：为了提供更高的可用性和可伸缩性，通常在不同的机器上运行服务的多个实例，其中任何一个都可以处理传入的请求。将请求分散到这些实例上称为 _负载均衡_。

有许多负载均衡和服务发现解决方案可用：
- _硬件负载均衡器_ 是安装在数据中心的专用设备。它们允许客户端连接到单个主机和端口，传入连接被路由到运行服务的服务器之一。
- 软件负载均衡器 的行为方式与硬件负载均衡器大致相同。但是，软件负载均衡器（如 Nginx 和 HAProxy）不需要特殊设备，而是可以安装在标准机器上的应用程序。
- _域名服务 (DNS)_ 是当你打开网页时在互联网上解析域名的方式。它通过允许多个 IP 地址与单个域名关联来支持负载均衡
- _服务发现系统_ 使用集中式注册表而不是 DNS 来跟踪哪些服务端点可用。当新服务实例启动时，它通过声明它正在侦听的主机和端口以及相关元数据向服务发现系统注册自己。

当客户端希望连接到服务时，它首先查询发现系统以获取可用端点列表，然后直接连接到端点。

- _服务网格_ 是一种复杂的负载均衡形式，它结合了软件负载均衡器和服务发现。与在单独机器上运行的传统软件负载均衡器不同，服务网格负载均衡器通常作为进程内客户端库或作为客户端和服务器上的进程或"边车"容器部署。客户端应用程序连接到它们自己的本地服务负载均衡器，该负载均衡器连接到服务器的负载均衡器。从那里，连接被路由到本地服务器进程。

##### RPC 的数据编码与演化

对于可演化性，RPC 客户端和服务器可以独立更改和部署非常重要。

RPC 方案的向后和向前兼容性属性继承自它使用的任何编码：

- gRPC（Protocol Buffers）和 Avro RPC 可以根据各自编码格式的兼容性规则进行演化。
- RESTful API 最常使用 JSON 作为响应，以及 JSON 或 URI 编码/表单编码的请求参数作为请求。添加可选请求参数和向响应对象添加新字段通常被认为是保持兼容性的更改。

服务兼容性变得更加困难，因为 RPC 通常用于跨组织边界的通信，因此服务提供者通常无法控制其客户端，也无法强制它们升级。因此，兼容性需要保持很长时间，也许是无限期的。如果需要破坏兼容性的更改，服务提供者通常最终会并行维护服务 API 的多个版本。

- 对于 RESTful API，常见的方法是在 URL 中使用版本号或在 HTTP `Accept` 标头中使用。
- 对于使用 API 密钥识别特定客户端的服务，另一个选项是在服务器上存储客户端请求的 API 版本，并允许通过单独的管理界面更新此版本选择

#### 持久化执行与工作流

处理单个付款需要许多服务调用。支付处理器服务可能会调用欺诈检测服务以检查欺诈，调用信用卡服务以扣除信用卡费用，并调用银行服务以存入扣除的资金

![[file-20250817222919732.png]]

这一系列步骤称为 _工作流_，每个步骤称为 _任务_。工作流通常定义为任务图。工作流定义可以用通用编程语言、领域特定语言 (DSL) 或标记语言编写

工作流由 _工作流引擎_ 运行或执行。工作流引擎确定何时运行每个任务、任务必须在哪台机器上运行、如果任务失败该怎么办（例如，如果机器在任务运行时崩溃）、允许并行执行多少任务等。

##### 持久化执行

持久化执行框架已成为构建需要事务性的基于服务的架构的流行方式。

持久化执行框架是为工作流提供 _精确一次语义_ 的一种方式。如果任务失败，框架将重新执行该任务，但会跳过任务在失败之前成功完成的任何 RPC 调用或状态更改。相反，框架将假装进行调用，但实际上将返回先前调用的结果。

由于持久化执行框架期望以确定性方式重放所有代码（相同的输入产生相同的输出），因此随机数生成器或系统时钟等非确定性代码会产生问题

#### 事件驱动的架构

**_事件驱动架构_**：这是编码数据从一个进程流向另一个进程的另一种方式。请求称为 _事件_ 或 _消息_；

与 RPC 不同，发送者通常不会等待接收者处理事件。此外，事件通常不是通过直接网络连接发送给接收者，而是通过称为 _消息代理_（也称为 _事件代理_、_消息队列_ 或 _面向消息的中间件_）的中介，它临时存储消息

使用消息代理与直接 RPC 相比有几个优点：：
- 如果接收者不可用或过载，它可以充当缓冲区，从而提高系统可靠性。
- 它可以自动将消息重新传递给已崩溃的进程，从而防止消息丢失。
- 它避免了服务发现的需要，因为发送者不需要直接连接到接收者的 IP 地址。
- 它允许将相同的消息发送给多个接收者。
- 它在逻辑上将发送者与接收者解耦（发送者只是发布消息，不关心谁使用它们）。

通过消息代理的通信是 _异步的_：发送者不会等待消息被传递，而是简单地发送它然后忘记它。

##### 消息代理

过去，消息代理的格局由 TIBCO、IBM WebSphere 和 webMethods 等公司的商业企业软件主导，然后开源实现（如 RabbitMQ、ActiveMQ、HornetQ、NATS 和 Apache Kafka）变得流行。最近，云服务（如 Amazon Kinesis、Azure Service Bus 和 Google Cloud Pub/Sub）也获得了采用。

最常使用两种消息分发模式：
- 一个进程将消息添加到命名 **_队列_**，代理将该消息传递给该队列的 _消费者_。如果有多个消费者，其中一个会收到消息。
- 一个进程将消息发布到命名 **_主题_**，代理将该消息传递给该主题的所有 _订阅者_。如果有多个订阅者，他们都会收到消息。

消息代理通常不强制执行任何特定的数据模型——消息只是带有一些元数据的字节序列，因此你可以使用任何编码格式。常见的方法是使用 Protocol Buffers、Avro 或 JSON，并在消息代理旁边部署模式注册表来存储所有有效的模式版本并检查其兼容性

消息代理在消息的持久性方面有所不同。许多将消息写入磁盘，以便在消息代理崩溃或需要重新启动时不会丢失。与数据库不同，许多消息代理在消息被消费后会自动再次删除消息。
##### 分布式 actor 框架

_Actor 模型_ 是单个进程中并发的编程模型。与其直接处理线程（以及相关的竞态条件、锁定和死锁问题），逻辑被封装在 _actor_ 中。每个 actor 通常代表一个客户端或实体，它可能有一些本地状态（不与任何其他 actor 共享），并通过发送和接收异步消息与其他 actor 通信。消息传递不能保证：在某些错误场景中，消息将丢失。由于每个 actor 一次只处理一条消息，因此它不需要担心线程，并且每个 actor 可以由框架独立调度。

分布式 actor 框架本质上将消息代理和 actor 编程模型集成到单个框架中。

# 第二部分：分布式数据

如果 **多台机器** 参与数据的存储和检索，会发生什么？希望将数据库分布到多台机器上：
- **可伸缩性**：如果你的数据量、读取负载、写入负载超出单台机器的处理能力，可以将负载分散到多台计算机上。
- **容错 / 高可用性**：如果你的应用需要在单台机器（或多台机器，网络或整个数据中心）出现故障的情况下仍然能继续工作，则可使用多台机器，以提供冗余。一台故障时，另一台可以接管。
- **延迟**：如果在世界各地都有用户，你也许会考虑在全球范围部署多个服务器，从而每个用户可以从地理上最近的数据中心获取服务，避免了等待网络数据包穿越半个世界。

伸缩至更高的负载：
- **垂直伸缩**：许多处理器，内存和磁盘可以在同一个操作系统下相互连接，快速的相互连接允许任意处理器访问内存或磁盘的任意部分。在这种 **共享内存架构（shared-memory architecture）** 中，所有的组件都可以看作一台单独的机器。**共享磁盘架构（shared-disk architecture）**，它使用多台具有独立处理器和内存的机器，但将数据存储在机器之间共享的磁盘阵列上，这些磁盘通过快速网络连接。
- **水平伸缩**：运行数据库软件的每台机器 / 虚拟机都称为 **节点（node）**。每个节点只使用各自的处理器，内存和磁盘。节点之间的任何协调，都是在软件层面使用传统网络实现的。这是本章重点。

复制 vs 分区：
- **复制（Replication）**：在几个不同的节点上保存数据的相同副本，可能放在不同的位置。复制提供了冗余：如果一些节点不可用，剩余的节点仍然可以提供数据服务。复制也有助于改善性能
- **分区 (Partitioning)**：将一个大型数据库拆分成较小的子集（称为 **分区**，即 partitions），从而不同的分区可以指派给不同的 **节点**（nodes，亦称 **分片**，即 sharding）

![[file-20250818094413923.png]]
## 6. 复制

**复制**：通过网络连接的多台机器上保存相同数据的副本。
可能出于以下几个原因想要复制数据：
- 使数据在地理上更接近用户（从而减少访问延迟）
- 即使系统的部分组件出现故障，也能让系统继续工作（从而提高可用性）
- 扩展能够处理读查询的机器数量（从而提高读吞吐量）

处理复制的所有困难都在于处理复制数据的 **变更**，这也是本章的主题。讨论三种复制节点间变更的算法族：**单主**、**多主** 和 **无主** 复制。几乎所有分布式数据库都使用这三种方法之一。它们各有利弊，我们将详细研究。

数据库复制是一个古老的话题——自 20 世纪 70 年代研究以来，原理并没有太大变化。

**复制**：副本会快速将一个节点的写入反映到其他节点上
**备份**：备份存储数据的旧快照，以便你可以回到过去的时间点。如果你不小心删除了一些数据，复制并不能帮助你，因为删除操作也会传播到副本，所以如果你想恢复被删除的数据，就需要备份。

### 单主复制

存储数据库副本的每个节点称为 **副本**。有了多个副本，不可避免地会出现一个问题：我们如何确保所有数据最终都出现在所有副本上？

最常见的解决方案称为 **基于主节点的复制**、**主备复制** 或 **主动/被动复制**。
1. 其中一个副本被指定为 **主节点**（也称为 **主库** 或 **源**）。当客户端想要写入数据库时，他们必须将请求发送给主节点。
2. 其他副本称为 **从节点**（**只读副本**、**从库** 或 **热备**）。每当主节点将新数据写入其本地存储时，它也会将数据变更作为 **复制日志** 或 **变更流** 的一部分发送给所有从节点。
3. 当客户端想要从数据库读取时，它可以查询主节点或任何从节点。然而，只有主节点接受写入.
![[file-20250818095753050.png]]

单主复制被广泛使用。它是许多关系数据库的内置功能，如 PostgreSQL、MySQL、Oracle Data Guard  和 SQL Server 的 Always On 可用性组 。它也用于一些文档数据库，如 MongoDB 和 DynamoDB，消息代理如 Kafka，复制块设备如 DRBD，以及一些网络文件系统。

#### 同步复制与异步复制

复制系统的一个重要细节是复制是 **同步** 发生还是 **异步** 发生。（在关系数据库中，这通常是一个可配置选项；其他系统通常硬编码为其中之一。）

一个网站用户更新他们的个人资料图片。在某个时间点，客户端向主节点发送更新请求；不久之后，主节点收到了它。在某个时间点，主节点将数据变更转发给从节点。最终，主节点通知客户端更新成功。
![[file-20250818100246210.png]]

- 对从节点 1 的复制是 **同步的**：主节点等待从节点 1 确认它已收到写入，然后才向用户报告成功，并使写入对其他客户端可见。
- 对从节点 2 的复制是 **异步的**：主节点发送消息，但不等待从节点的响应。

同步复制的优缺点：
- 优点：是从节点保证拥有与主节点一致的最新数据副本
- 缺点：如果同步从节点没有响应（因为它已崩溃，或存在网络故障，或任何其他原因），写入就无法处理。主节点必须阻塞所有写入并等待同步副本再次可用。

将所有从节点都设为同步是不切实际的：任何一个节点的中断都会导致整个系统停止。

如果数据库提供同步复制，通常意味着 **一个** 从节点是同步的，其他的是异步的。如果同步从节点变得不可用或缓慢，异步从节点之一将变为同步。这保证了你**至少在两个节点上拥有**最新的数据副本：主节点和一个同步从节点。这种配置有时也称为 **半同步**。


在某些系统中，**多数**（例如，包括主节点在内的 5 个副本中的 3 个）副本被同步更新，其余少数是异步的。这是 **仲裁** 的一个例子。多数仲裁通常用于使用共识协议进行自动主节点选举的系统中。

多数仲裁通常用于使用共识协议进行自动主节点选举的系统中，这意味着即使已向客户端确认，写入也不能保证持久。

#### 设置新的副本

不时地，你需要设置新的从节点——也许是为了增加副本的数量，或者替换失败的节点。如何确保新的从节点拥有主节点数据的准确副本？

设置从节点通常可以在不停机的情况下完成。从概念上讲，过程如下所示：
1. 在某个时间点获取主节点数据库的一致快照——如果可能，不锁定整个数据库。
2. 将快照复制到新的从节点。
3. 从节点连接到主节点并请求自快照拍摄以来发生的所有数据变更。这要求快照与主节点复制日志中的确切位置相关联。该位置有各种名称：例如，PostgreSQL 称之为 **日志序列号**；MySQL 有两种机制，**binlog 位点** 和 **全局事务标识符**（GTID）。
4. 当从节点处理了自快照以来的数据变更积压后，我们说它已经 **追上进度**。它现在可以继续处理主节点发生的数据变更。

也可以将复制日志归档到对象存储；连同对象存储中整个数据库的定期快照，这是实现数据库备份和灾难恢复的好方法。

在对象存储中存储数据库数据有许多好处：
- 与其他云存储选项相比，对象存储价格便宜
- 对象存储还提供具有非常高持久性保证的多区域、双区域或多区域复制。
- 数据库可以使用对象存储的 **条件写入** 功能——本质上是 **比较并设置**（CAS）操作——来实现事务和领导者选举
- 将来自多个数据库的数据存储在同一对象存储中可以简化数据集成，特别是在使用 Apache Parquet 和 Apache Iceberg 等开放格式时。

对象存储的读写延迟比本地磁盘或 EBS 等虚拟块设备要高得多。

#### 处理节点故障

能够在不停机的情况下重新启动单个节点对于操作和维护来说是一个很大的优势。如何通过基于主节点的复制实现高可用性？

##### 从节点故障：追赶恢复

如果从节点崩溃并重新启动，或者如果主节点和从节点之间的网络暂时中断，从节点可以很容易地恢复：从其日志中，它知道在故障发生之前处理的最后一个事务。

从节点恢复在概念上很简单，但在性能方面可能具有挑战性：如果数据库具有高写入吞吐量，或者如果从节点已离线很长时间，可能有很多写入需要赶上。在进行这种追赶时，恢复的从节点和主节点（需要将写入积压发送到从节点）都会有高负载。

##### 领导者故障：故障转移

处理主节点故障更加棘手：其中一个从节点需要被提升为新的主节点，客户端需要重新配置以将其写入发送到新的主节点，其他从节点需要开始从新的主节点消费数据变更。这个过程称为 **故障转移**。

自动故障转移过程通常包括以下步骤：
1. **确定主节点已失败。** 可能会出现许多问题：崩溃、停电、网络问题等。没有万无一失的方法来检测出了什么问题，所以大多数系统只是使用超时
2. **选择新的主节点。** 这可以通过选举过程完成（其中主节点由剩余副本的多数选择），或者新的主节点可以由先前建立的 **控制器节点** 任命。让所有节点就新主节点达成一致是一个共识问题
3. **重新配置系统以使用新的主节点。** 客户端现在需要将其写入请求发送到新的主节点
故障转移充满了可能出错的事情：
- 如果使用异步复制，新的主节点可能在失败之前没有收到来自旧主节点的所有写入。这意味着你认为已提交的写入实际上并不持久。
- 如果数据库之外的其他存储系统需要与数据库内容协调，丢弃写入尤其危险。
- 可能会发生两个节点都认为自己是主节点的情况。这种情况称为 **脑裂**，这是危险的
- 在宣布主节点死亡之前，正确的超时是什么？更长的超时意味着在主节点失败的情况下恢复时间更长。

**栅栏机制**：通过限制或关闭旧主节点来防止脑裂被称为 **栅栏机制**，或者更强调地说，**向头部开枪**（STONITH）

故障转移最重要的是选择一个最新的从节点作为新的主节点——如果使用同步或半同步复制，这将是旧主节点在确认写入之前等待的从节点。使用异步复制，你可**以选择具有最大日志序列号的从节点**。这最小化了故障转移期间丢失的数据量：丢失几分之一秒的写入可能是可以容忍的，但选择落后几天的从节点可能是灾难性的。

#### 复制日志的实现

基于主节点的复制在底层是如何工作的？

##### 基于语句的复制

在最简单的情况下，主节点记录它执行的每个写入请求（**语句**）并将该语句日志发送给其从节点。容易出现的问题：
- 任何调用非确定性函数的语句，例如 `NOW()` 获取当前日期和时间或 `RAND()` 获取随机数，可能会在每个副本上生成不同的值。
- 如果语句使用自增列，或者如果它们依赖于数据库中的现有数据（例如，`UPDATE … WHERE <某条件>`），它们必须在每个副本上以完全相同的顺序执行，否则它们可能会产生不同的效果。
- 具有副作用的语句（例如，触发器、存储过程、用户定义的函数）可能会导致每个副本上发生不同的副作用，除非副作用是绝对确定的。

基于语句的复制在 MySQL 5.1 版本之前使用。它今天有时仍在使用，因为它相当紧凑，但默认情况下，如果语句中有任何非确定性，MySQL 现在会切换到基于行的复制（稍后讨论）。VoltDB 使用基于语句的复制，并通过要求事务是确定性的来使其安全。
许多数据库更喜欢其他复制方法。

##### 预写日志（WAL）传输

WAL 包含将索引和堆恢复到一致状态所需的所有信息，我们可以使用完全相同的日志在另一个节点上构建副本：除了将日志写入磁盘外，主节点还通过网络将其发送给其从节点。当从节点处理此日志时，它构建了与主节点上找到的完全相同的文件副本。

此复制方法在 PostgreSQL 和 Oracle 等中使用。主要缺点是日志在非常低的级别描述数据：WAL 包含哪些字节在哪些磁盘块中被更改的详细信息。这使得复制与存储引擎紧密耦合。如果数据库从一个版本更改其存储格式到另一个版本，通常不可能在主节点和从节点上运行不同版本的数据库软件。

##### 逻辑（基于行）日志复制

为复制和存储引擎使用不同的日志格式，这允许复制日志与存储引擎内部解耦。这种复制日志称为 **逻辑日志**，以区别于存储引擎的（**物理**）数据表示。

关系数据库的逻辑日志通常是描述以行粒度对数据库表的写入的记录序列：
- 对于插入的行，日志包含所有列的新值。
- 对于删除的行，日志包含足够的信息来唯一标识被删除的行。通常这将是主键，但如果表上没有主键，则需要记录所有列的旧值。
- 对于更新的行，日志包含足够的信息来唯一标识更新的行，以及所有列的新值（或至少所有已更改的列的新值）。

MySQL 除了 WAL 之外还保留一个单独的逻辑复制日志，称为 **binlog**（当配置为使用基于行的复制时）。PostgreSQL 通过将物理 WAL 解码为行插入/更新/删除事件来实现逻辑复制。

由于逻辑日志与存储引擎内部解耦，因此可以更容易地保持向后兼容，允许主节点和从节点运行不同版本的数据库软件。

### 复制延迟的问题

基于主节点的复制要求所有写入都通过单个节点，但只读查询可以转到任何副本。对于主要由读取和只有少量写入组成的工作负载，有一个有吸引力的选择：创建许多从节点，并将读取请求分布在这些从节点上。这减轻了主节点的负载，并允许附近的副本提供读取请求。

**读扩展** 架构：通过添加更多从节点来简单地增加服务只读请求的容量。然而，这种方法只有在使用异步复制时才现实可行。

如果应用程序从 **异步** 从节点读取，如果从节点已落后，它可能会看到过时的信息。这导致数据库中出现明显的不一致。
这种不一致只是一种临时状态——如果你停止向数据库写入并等待一段时间，从节点最终将赶上并与主节点保持一致。因此，这种效果被称为 **最终一致性**。

重点介绍复制延迟时可能发生的三个问题示例。

#### 读己之写

提交新数据时，必须将其发送到主节点，但当用户查看数据时，可以从从节点读取。如果数据经常被查看但只是偶尔被写入，这尤其合适。

如果用户在写入后不久查看数据，新数据可能尚未到达副本。对用户来说，看起来他们提交的数据丢失了，所以他们会不高兴。
![[file-20250818111950877.png]]

在这种情况下，我们需要 **写后读一致性**，也称为 **读己之写一致性**。
这是一种保证，如果用户重新加载页面，他们将始终看到他们自己提交的任何更新。它不对其他用户做出承诺：其他用户的更新可能直到稍后才可见。然而，它向用户保证他们自己的输入已正确保存。

如何在基于主节点的复制系统中实现写后读一致性？有各种可能的技术。提及其中几个：
- 当读取用户可能已修改的内容时，从主节点或同步更新的从节点读取；否则，从异步更新的从节点读取。
- 如果应用程序中的大多数东西都可能被用户编辑，那种方法将不会有效，因为大多数东西都必须从主节点读取（否定了读扩展的好处）。在这种情况下，可以使用其他标准来决定是否从主节点读取。
- 客户端可以记住其最近写入的时间戳——然后系统可以确保为该用户提供任何读取的副本至少反映该时间戳之前的更新。
- 客户端可以记住其最近写入的时间戳——然后系统可以确保为该用户提供任何读取的副本至少反映该时间戳之前的更新。

当同一用户从多个设备访问你的服务时，会出现另一个复杂情况，例如桌面网络浏览器和移动应用程序。在这种情况下，你可能希望提供 **跨设备** 写后读一致性：如果用户在一个设备上输入一些信息，然后在另一个设备上查看它，他们应该看到他们刚刚输入的信息。需要考虑一些额外的问题：
- 需要记住用户上次更新的时间戳的方法变得更加困难，因为在一个设备上运行的代码不知道在另一个设备上发生了什么更新。此元数据将需要集中化。
- 如果你的副本分布在不同的地区，则无法保证来自不同设备的连接将路由到同一地区。

**地区**：指代单个地理位置中的一个或多个数据中心。
**可用区** 或简称 **区域**：云提供商在同一地理区域中定位多个数据中心。

#### 单调读
从异步从节点读取时可能发生的第二个异常示例是，用户可能会看到事物 **在时间上倒退**。

![[file-20250818145217824.png]]

**单调读**：一种保证这种异常不会发生的保证。它是比强一致性更弱的保证，但比最终一致性更强的保证。当你读取数据时，你可能会看到一个旧值；单调读只意味着如果一个用户按顺序进行多次读取，他们不会看到时间倒退——即，在之前读取较新数据后，他们不会读取较旧的数据。

**实现方式**：实现单调读的一种方法是确保每个用户始终从同一副本进行读取（不同的用户可以从不同的副本读取）

#### 一致前缀读

第三个复制延迟异常示例涉及违反因果关系。

如果某些分片的复制比其他分片慢，观察者可能会在看到问题之前看到答案。
![[file-20250818145729919.png]]

防止这种异常需要另一种类型的保证：**一致前缀读**。如果一系列写入以某个顺序发生，那么任何读取这些写入的人都会看到它们以相同的顺序出现。

如果数据库始终以相同的顺序应用写入，读取始终会看到一致的前缀，因此这种异常不会发生。（许多分布式数据库中，不同的分片独立运行，因此没有全局的写入顺序）

**实现方式**：确保任何因果相关的写入都写入同一分片——但在某些应用程序中，这无法有效完成。

#### 复制延迟的解决方案

对于应用程序开发人员来说，最简单的编程模型是选择一个为副本提供强一致性保证的数据库，例如线性一致性和 ACID 事务。在 2010 年代初期，**NoSQL** 运动推广了这样的观点，即这些功能限制了可伸缩性，大规模系统必须接受最终一致性。

许多数据库开始提供强一致性和事务，同时还提供分布式数据库的容错、高可用性和可伸缩性优势。这种趋势被称为 **NewSQL**，以与 NoSQL 形成对比

### 多主复制

**单主复制的主要缺点**：所有写入都必须通过一个主节点。如果由于任何原因无法连接到主节点，例如你和主节点之间的网络中断，你就无法写入数据库。

**多主 配置**：允许多个节点接受写入。复制仍然以相同的方式进行：每个处理写入的节点必须将该数据变更转发给所有其他节点。每个主节点同时充当其他主节点的从节点。

与单主复制一样，可以选择使其同步或异步。同步多主复制因此给你一个非常类似于单主复制的模型。本节的其余部分专注于异步多主复制，其中任何主节点都可以处理写入，即使其与其他主节点的连接中断。

#### 跨地域运行

**地理分布式**、**地域分布式** 或 **地域复制** 设置：有一个数据库，在几个不同的地区有副本
- 使用单主复制，主节点必须在 **一个** 地区，所有写入都必须通过该地区。
- 在多主配置中，你可以在 **每个** 地区都有一个主节点。在每个地区内，使用常规的主从复制（从节点可能在与主节点不同的可用区中）；在地区之间，每个地区的主节点将其变更复制到其他地区的主节点。
![[file-20250818151509934.png]]

单主和多主配置在多地区部署中的表现：
- **性能**：在单主配置中，每次写入都必须通过互联网到拥有主节点的地区。在多主配置中，每次写入都可以在本地地区处理，并异步复制到其他地区。
- **地区故障容忍**：在单主配置中，如果拥有主节点的地区变得不可用，故障转移可以将另一个地区的从节点提升为主节点。在多主配置中，每个地区可以独立于其他地区继续运行，并在离线地区恢复上线时赶上复制。
- **网络问题容忍**：单主配置对这种跨地区链路中的问题非常敏感，因为当一个地区的客户端想要写入另一个地区的主节点时，它必须通过该链路发送其请求并等待响应才能完成。具有异步复制的多主配置可以更好地容忍网络问题
- **一致性**：单主系统可以提供强一致性保证，例如可串行化事务。多主系统的最大缺点是它们能够实现的一致性要弱得多

多主复制不如单主复制常见，但许多数据库仍然支持它，包括 MySQL、Oracle、SQL Server 和 YugabyteDB。

由于多主复制在许多数据库中是一个有点改装的功能，因此通常存在微妙的配置陷阱和与其他数据库功能的令人惊讶的交互。例如，自增键、触发器和完整性约束可能会有问题。因此，多主复制通常被认为是应该尽可能避免的危险领域。

##### 多主复制拓扑

**复制拓扑**：写入从一个节点传播到另一个节点的通信路径。
![[file-20250818152046035.png]]

 **全对全**，最通用的拓扑是 **全对全**，其中每个主节点将其写入发送到每个其他主节点。
 **环形拓扑**，其中每个节点从一个节点接收写入并将这些写入（加上其自己的任何写入）转发到另一个节点。
 **星形** 形状，一个指定的根节点将写入转发到所有其他节点。星形拓扑可以推广到树形。
在环形和星形拓扑中，写入可能需要通过几个节点才能到达所有副本。

##### 不同拓扑的问题

环形和星形拓扑的一个问题是，如果只有一个节点发生故障，它可能会中断其他节点之间的复制消息流，使它们无法通信，直到节点被修复。

全对全拓扑也可能有问题。特别是，一些网络链路可能比其他链路更快（例如，由于网络拥塞），结果是一些复制消息可能会"超越"其他消息。

![[file-20250818152453806.png]]
客户端 A 在主节点 1 上向表中插入一行，客户端 B 在主节点 3 上更新该行。然而，主节点 2 可能以不同的顺序接收写入：它可能首先接收更新（从其角度来看，这是对数据库中不存在的行的更新），然后才接收相应的插入（应该在更新之前）。

为了正确排序这些事件，可以使用一种称为 **版本向量** 的技术

#### 同步引擎与本地优先软件

另一种适合多主复制的情况是，如果你有一个需要在与互联网断开连接时继续工作的应用程序。
在这种情况下，每个设备都有一个充当主节点的本地数据库副本（它接受写入请求），并且在你所有设备上的日历副本之间有一个异步多主复制过程（同步）

##### 实时协作、离线优先和本地优先应用

许多现代 Web 应用程序提供 **实时协作** 功能，例如用于文本文档和电子表格的 Google Docs 和 Sheets，用于图形的 Figma，以及用于项目管理的 Linear。

每个打开共享文件的 Web 浏览器选项卡都是一个副本，你对文件进行的任何更新都会异步复制到打开同一文件的其他用户的设备。

离线编辑和实时协作都需要类似的复制基础设施：应用程序需要捕获用户对文件所做的任何更改，并立即将它们发送给协作者（如果在线），或本地存储它们以供稍后发送（如果离线）。此外，应用程序需要接收来自协作者的更改，将它们合并到用户的文件本地副本中，并更新用户界面以反映最新版本。如果多个用户同时更改了文件，可能需要冲突解决逻辑来合并这些更改

支持此过程的软件库称为 **同步引擎**；允许用户在离线时继续编辑文件的应用程序（可能使用同步引擎实现）称为 **离线优先**。

##### 同步引擎的利弊

今天构建 Web 应用程序的主导方式是在客户端保留很少的持久状态，并在需要显示新数据或需要更新某些数据时依赖向服务器发出请求。
同步引擎方法有许多优点：
- 在本地拥有数据意味着用户界面的响应速度可以比必须等待服务调用获取某些数据时快得多。
- 允许用户在离线时继续工作是有价值的，特别是在具有间歇性连接的移动设备上。
- 与在应用程序代码中执行显式服务调用相比，同步引擎简化了前端应用程序的编程模型。
- 为了实时显示其他用户的编辑，你需要接收这些编辑的通知并相应地有效更新用户界面。
同步引擎由 Lotus Notes 在 20 世纪 80 年代开创。

#### 处理写入冲突

多主复制的最大问题：是不同主节点上的并发写入可能导致需要解决的冲突。

用户 1 将页面标题从 A 更改为 B，用户 2 独立地将标题从 A 更改为 C。每个用户的更改成功应用于其本地主节点。然而，当更改异步复制时，检测到冲突。这个问题在单主数据库中不会发生。
![[file-20250818185248189.png]]

现在我们假设我们可以检测冲突，并且我们想找出解决它们的最佳方法。

##### 冲突避免

冲突的一种策略是首先避免它们发生。例如，如果应用程序可以确保特定记录的所有写入都通过同一主节点，那么即使整个数据库是多主的，也不会发生冲突。

冲突避免的另一个例子：想象你想要插入新记录并基于自增计数器为它们生成唯一 ID。如果你有两个主节点，你可以设置它们，使得一个主节点只生成奇数，另一个只生成偶数。这样你可以确保两个主节点不会同时为不同的记录分配相同的 ID。

##### 最后写入者胜（丢弃并发写入）

如果无法避免冲突，解决它们的最简单方法是为每个写入附加时间戳，并始终使用具有最大时间戳的值。

这种方法称为 **最后写入者胜**（LWW），因为具有最大时间戳的写入可以被认为是"最后"的。然而，这个术语是误导性的，并发写入的时间戳顺序本质上是随机的。

**LWW 的真正含义**是：当同一记录在不同的主节点上并发写入时，其中一个写入被随机选择为获胜者，其他写入被静默丢弃，即使它们在各自的主节点上成功处理。这实现了最终所有副本都处于一致状态的目标，但代价是数据丢失。

LWW 的另一个问题是，如果使用实时时钟（例如 Unix 时间戳）作为写入的时间戳，系统对时钟同步变得非常敏感。如果一个节点的时钟领先于其他节点，并且你尝试覆盖该节点写入的值，你的写入可能会被忽略，因为它可能具有较低的时间戳，即使它明显发生得更晚。这个问题可以通过使用 **逻辑时钟** 来解决

##### 手动冲突解决

在数据库中，冲突停止整个复制过程直到人类解决它是不切实际的。相反，数据库通常存储给定记录的所有并发写入值
下次查询该记录时，数据库返回 **所有** 这些值，而不仅仅是最新的值。然后，你可以以任何你想要的方式解决这些值，无论是在应用程序代码中自动（例如，你可以将 B 和 C 连接成"B/C"），还是通过询问用户。然后，你将新值写回数据库以解决冲突。

这种冲突解决方法在某些系统中使用，例如 CouchDB。然而，它也存在许多问题：
- 数据库的 API 发生变化，可能使应用程序代码中的数据难以处理。
- 要求用户手动合并兄弟节点是很多工作，在许多情况下，自动合并比打扰用户更好。
- 如果不仔细进行，自动合并兄弟节点可能会导致令人惊讶的行为。
- 如果多个节点观察到冲突并并发解决它，冲突解决过程本身可能会引入新的冲突。

![[file-20250818191232841.png]]
#### 自动冲突解决

对于许多应用程序，处理冲突的最佳方法是使用自动将并发写入合并为一致状态的算法。自动冲突解决确保所有副本 **收敛** 到相同的状态

LWW 是冲突解决算法的一个简单示例。已经为不同类型的数据开发了更复杂的合并算法，目标是尽可能保留所有更新的预期效果：
- 如果数据是文本，我们可以检测从一个版本到下一个版本插入或删除了哪些字符
- 如果数据是项目集合，我们可以通过跟踪插入和删除类似于文本来合并它。
- 如果数据是表示可以递增或递减的计数器的整数（例如，社交媒体帖子上的点赞数），合并算法可以告诉每个兄弟节点上发生了多少次递增和递减，并正确地将它们相加，以便结果不会重复计数也不会丢弃更新。
- 如果数据是键值映射，我们可以通过将其他冲突解决算法之一应用于该键下的值来合并对同一键的更新。

冲突解决的可能性是有限的。自动冲突解决足以构建许多有用的应用程序。

#### CRDT 与操作变换

两个算法族通常用于实现自动冲突解决：**无冲突复制数据类型**（CRDT）[46](http://ddia.vonng.com/ch6/#fn:46) 和 **操作变换**（OT）：
- **操作变换**：需要转换每个操作的索引以考虑已经应用的并发操作
- **无冲突复制数据类型**：大多数 CRDT 为每个字符提供唯一的、不可变的 ID，并使用这些 ID 来确定插入/删除的位置，而不是索引。

![[file-20250818192040984.png]]

OT 最常用于文本的实时协作编辑，例如在 Google Docs 中，而 CRDT 可以在分布式数据库中找到，例如 Redis Enterprise、Riak 和 Azure Cosmos DB。

##### 什么是冲突？

其他类型的冲突可能更难以检测。例如，考虑一个会议室预订系统：它跟踪哪个房间由哪组人在什么时间预订。此应用程序需要确保每个房间在任何时间只由一组人预订（即，同一房间不得有任何重叠的预订）。在这种情况下，如果为同一房间同时创建两个不同的预订，可能会出现冲突。即使应用程序在允许用户进行预订之前检查可用性，如果两个预订是在两个不同的主节点上进行的，也可能会发生冲突。

### 无主复制

单主和多主复制基于这样的想法：客户端向一个节点（主节点）发送写入请求，数据库系统负责将该写入复制到其他副本。主节点确定写入应该处理的顺序，从节点以相同的顺序应用主节点的写入。

一些数据存储系统采用不同的方法，放弃主节点的概念，并允许任何副本直接接受来自客户端的写入。
在亚马逊于 2007 年将其用于其内部 **Dynamo** 系统后，无主复制再次成为数据库的时尚架构。Riak、Cassandra 和 ScyllaDB 是受 Dynamo 启发的具有无主复制模型的开源数据存储，因此这种数据库也被称为 **Dynamo 风格**。

在某些无主实现中，客户端直接将其写入发送到多个副本，而在其他实现中，协调器节点代表客户端执行此操作。然而，与主节点数据库不同，**该协调器不强制执行特定的写入顺序**。正如我们将看到的，这种设计差异对数据库的使用方式产生了深远的影响。

#### 当节点故障时写入数据库

在无主配置中，故障转移不存在。

客户端（用户 1234）将写入并行发送到所有三个副本，两个可用副本接受写入，但不可用副本错过了它。假设三个副本中有两个确认写入就足够了：在用户 1234 收到两个 **ok** 响应后，我们认为写入成功。客户端只是忽略了其中一个副本错过写入的事实。可能会得到 **陈旧**（过时）值作为响应。
![[file-20250818194352760.png]]
当客户端从数据库读取时，它不只是将其请求发送到一个副本：**读取请求也并行发送到多个节点**。客户端可能会从不同的节点获得不同的响应；例如，从一个节点获得最新值，从另一个节点获得陈旧值。

为了区分哪些响应是最新的，哪些是过时的，写入的每个值都需要用版本号或时间戳标记，它使用具有最大时间戳的值。

##### 追赶错过的写入

复制系统应确保最终所有数据都复制到每个副本。在不可用节点恢复上线后，它如何赶上它错过的写入？在 Dynamo 风格的数据存储中使用了几种机制：
- **读修复**：当客户端并行从多个节点进行读取时，它可以检测任何陈旧响应。
- **提示移交**：如果一个副本不可用，另一个副本可能会以 **提示** 的形式代表其存储写入。
- **反熵**：一个后台进程定期查找副本之间数据的差异，并将任何缺失的数据从一个副本复制到另一个。

##### 读写仲裁

一般地说，如果有 _n_ 个副本，每次写入必须由 _w_ 个节点确认才能被认为成功，并且我们必须为每次读取查询至少 _r_ 个节点。只要 _w_ + _r_ > _n_，我们在读取时期望获得最新值，因为我们读取的 _r_ 个节点中至少有一个必须是最新的。遵守这些 _r_ 和 _w_ 值的读取和写入称为 **仲裁** 读取和写入。可以将 _r_ 和 _w_ 视为读取或写入有效所需的最小投票数。

仲裁条件 _w_ + _r_ > _n_ 允许系统容忍不可用节点，如下所示：
- 如果 _w_ < _n_，如果节点不可用，我们仍然可以处理写入。
- 如果 _r_ < _n_，如果节点不可用，我们仍然可以处理读取。
- 使用 _n_ = 3，_w_ = 2，_r_ = 2，我们可以容忍一个不可用节点
- 使用 _n_ = 5，_w_ = 3，_r_ = 3，我们可以容忍两个不可用节点。
![[file-20250818195125779.png]]
如果少于所需的 _w_ 或 _r_ 个节点可用，写入或读取将返回错误。

#### 仲裁一致性的局限

如果你有 _n_ 个副本，并且你选择 _w_ 和 _r_ 使得 _w_ + _r_ > _n_，你通常可以期望每次读取都返回为键写入的最新值。
通常，_r_ 和 _w_ 被选择为多数（超过 _n_/2）节点，因为这确保了 _w_ + _r_ > _n_，同时仍然容忍最多 _n_/2（向下舍入）个节点故障。

你也可以将 _w_ 和 _r_ 设置为较小的数字，使得 _w_ + _r_ ≤ _n_（即，不满足仲裁条件）。在这种情况下，读取和写入仍将发送到 _n_ 个节点，但需要较少的成功响应数才能使操作成功。

使用较小的 _w_ 和 _r_，你更有可能读取陈旧值，因为你的读取更可能没有包含具有最新值的节点。

然而，即使使用 _w_ + _r_ > _n_，在某些边缘情况下，一致性属性可能会令人困惑。一些场景包括：
- 如果携带新值的节点失败，并且其数据从携带旧值的副本恢复，存储新值的副本数量可能低于 _w_，破坏仲裁条件。
- 如果读取与写入操作并发，读取可能会或可能不会看到并发写入的值。特别是，一次读取可能看到新值，而后续读取看到旧值
- 如果两个写入并发发生，其中一个可能首先在一个副本上处理，另一个可能首先在另一个副本上处理。

尽管仲裁似乎保证读取返回最新写入的值，但实际上并不那么简单。Dynamo 风格的数据库通常针对可以容忍最终一致性的用例进行了优化。参数 _w_ 和 _r_ 允许你调整读取陈旧值的概率，但明智的做法是**不要将它们视为绝对保证。**

##### 监控陈旧性

对于基于主节点的复制，数据库通常公开复制延迟的指标，你可以将其输入到监控系统。

在具有无主复制的系统中，没有固定的写入应用顺序，这使得监控更加困难。副本为移交存储的提示数量可以是系统健康的一个度量，但很难有用地解释。最终一致性是一个故意模糊的保证，但为了可操作性，能够量化"最终"很重要。

#### 单主与无主复制的性能

基于单个主节点的复制系统可以提供在无主系统中难以或不可能实现的强一致性保证。

从主节点读取确保最新响应，但它存在性能问题：
- 读取吞吐量受主节点处理请求能力的限制
- 如果主节点失败，你必须等待检测到故障，并在继续处理请求之前完成故障转移。
- 系统对主节点上的性能问题非常敏感：如果主节点响应缓慢，例如由于过载或某些资源争用，增加的响应时间也会立即影响用户。

无主架构的一大优势是它对此类问题更有弹性。因为没有故障转移，并且请求无论如何都并行发送到多个副本，一个副本变慢或不可用对响应时间的影响很小：客户端只是使用响应更快的其他副本的响应。（**请求对冲**）

无主系统也可能有性能问题：
- 即使系统不需要执行故障转移，一个副本确实需要检测另一个副本何时不可用，以便它可以存储有关不可用副本错过的写入的提示。
- 你拥有的副本越多，你的仲裁就越大，在请求完成之前你必须等待的响应就越多。即使你只等待最快的 _r_ 或 _w_ 个副本响应，即使你并行发出请求，更大的 _r_ 或 _w_ 增加了你遇到慢副本的机会，增加了总体响应时间
- 大规模网络中断使客户端与大量副本断开连接，可能使形成仲裁变得不可能。

多主复制可以提供比无主复制更大的网络中断弹性，因为读取和写入只需要与一个主节点通信，该主节点可以与客户端位于同一位置。然而，由于一个主节点上的写入异步传播到其他主节点，读取可能任意过时。仲裁读取和写入提供了一种折衷：良好的容错性，同时也有很高的可能性读取最新数据。

##### 多地区操作

Cassandra 和 ScyllaDB 在正常的无主模型中实现了它们的多地区支持：客户端直接将其写入发送到所有地区的副本，你可以从各种一致性级别中进行选择，这些级别确定请求成功所需的响应数。例如，你可以请求所有地区中副本的仲裁、每个地区中的单独仲裁，或仅客户端本地地区的仲裁。本地仲裁避免了必须等待到其他地区的缓慢请求，但它也更可能返回陈旧结果。

Riak 将客户端和数据库节点之间的所有通信保持在一个地区本地，因此 _n_ 描述了一个地区内的副本数。数据库集群之间的跨地区复制在后台异步发生，其风格类似于多主复制。

#### 检测并发写入

由于可变的网络延迟和部分故障，事件可能以不同的顺序到达不同的节点。

- 节点 1 接收来自 A 的写入，但由于瞬时中断从未接收来自 B 的写入。
- 节点 2 首先接收来自 A 的写入，然后接收来自 B 的写入。
- 节点 3 首先接收来自 B 的写入，然后接收来自 A 的写入。
![[file-20250818202759005.png]]

为了最终保持一致，副本应该收敛到相同的值。后写入者胜（由 Cassandra 和 ScyllaDB 使用）、手动解决或 CRDT。

最后写入者胜很容易实现：每个写入都标有时间戳，具有更高时间戳的值总是覆盖具有较低时间戳的值。然而，时间戳不会告诉你两个值是否实际上冲突（即，它们是并发写入的）或不冲突（它们是一个接一个写入的）。如果你想显式解决冲突，系统需要更加小心地检测并发写入。

##### “先发生"关系与并发

- **先发生于**：如果操作 B 知道 A，或依赖于 A，或以某种方式建立在 A 之上，则操作 A **先发生于** 另一个操作 B。
- **并发的**：如果两个操作都不先发生于另一个（即，两者都不知道另一个）

每当你有两个操作 A 和 B 时，有三种可能性：要么 A 先发生于 B，要么 B 先发生于 A，要么 A 和 B 是并发的。我们需要的是一个算法来告诉我们两个操作是否并发。如果一个操作先发生于另一个，后面的操作应该覆盖前面的操作，但如果操作是并发的，我们有一个需要解决的冲突。

似乎两个操作如果"同时"发生，应该称为并发——但实际上，它们是否**真的在时间上重叠并不重要**。由于分布式系统中的时钟问题，实际上很难判断两件事是否恰好在同一时间发生。

人们有时将这一原则与物理学中的狭义相对论联系起来，如果两个事件之间的时间短于光在它们之间传播的时间，那么相隔一定距离发生的两个事件不可能相互影响。即使光速原则上允许一个操作影响另一个，两个操作也可能是并发的。

##### 捕获先发生关系


![[file-20250818204707509.png]]
操作之间的数据流以图形方式说明。
![[file-20250818204957531.png]]

服务器可以通过查看版本号来确定两个操作是否并发。
当写入包含来自先前读取的版本号时，这告诉我们写入基于哪个先前状态。如果你在不包含版本号的情况下进行写入，它与所有其他写入并发，因此它不会覆盖任何内容——它只会作为后续读取的值之一返回。
##### 版本向量

来自所有副本的版本号集合称为 **版本向量**。
版本向量在读取值时从数据库副本发送到客户端，并且在随后写入值时需要发送回数据库。
版本向量允许数据库区分覆盖和并发写入。

## 7. 分片

如果我们不想让每个节点都存储所有数据，我们可以将大量数据分割成更小的 _分片（shards）_ 或 _分区（partitions）_，并将不同的分片存储在不同的节点上。我们将在本章讨论分片。

分片通常与复制结合使用，以便每个分片的副本存储在多个节点上。这意味着，即使每条记录属于恰好一个分片，它仍然可以存储在多个不同的节点上以提供容错能力。
每个分片的主节点被分配给一个节点，其从节点被分配给其他节点。每个节点可能是某些分片的主节点，同时是其他分片的从节点。

分片方案的选择大部分独立于复制方案的选择

![[file-20250818210221706.png]]

 **_分片_**：根据你使用的软件不同有许多不同的名称：在 Kafka 中称为 _分区（partition）_，在 CockroachDB 中称为 _范围（range）_，在 HBase 和 TiDB 中称为 _区域（region）_，在 Bigtable 和 YugabyteDB 中称为 _表块（tablet）_，在 Cassandra、ScyllaDB 和 Riak 中称为 _虚节点（vnode）_，在 Couchbase 中称为 _虚桶（vBucket）_

一些数据库将分区和分片视为两个不同的概念。例如，在 PostgreSQL 中，分区是将大表拆分为存储在同一台机器上的多个文件的方法（这有几个优点，例如可以非常快速地删除整个分区），而分片则是将数据集拆分到多台机器上。在许多其他系统中，分区只是分片的另一个词。

### 分片的利与弊

对数据库进行分片的主要原因是 _可伸缩性_：如果数据量或写吞吐量已经超出单个节点的处理能力，这是一个解决方案，它允许你将数据和写入分散到多个节点上。
分片是我们实现 _水平扩展_（_横向扩展_ 架构）的主要工具之一。

分片是一个**重量级解决方案**，主要在大规模场景下才有意义。如果你的数据量和写吞吐量可以在单台机器上处理（而单台机器现在可以做很多事情！），通常最好避免分片并坚持使用单分片数据库。

分片通常适用于键值数据，你可以轻松地按键进行分片，但对于关系数据则较难，因为你可能想要通过二级索引搜索，或连接可能分布在不同分片中的记录。
分片的另一个问题是写入可能需要更新多个不同分片中的相关记录。虽然单节点上的事务相当常见，但确保跨多个分片的一致性需要 _分布式事务_。

#### 面向多租户的分片

软件即服务（SaaS）产品和云服务通常是 _多租户_ 的，其中每个租户是一个客户。多个用户可能在同一租户上拥有登录帐户，但每个租户都有一个独立的数据集，与其他租户分开。
使用分片实现多租户有几个优点：
- **资源隔离**：如果一个租户执行计算密集型操作，如果它们在不同的分片上运行，其他租户的性能受影响的可能性较小。
- **权限隔离**：如果你的访问控制逻辑中存在错误，如果这些租户的数据集彼此物理分离存储，你意外地给一个租户访问另一个租户数据的可能性较小
- **基于单元的架构**：在 基于单元的架构 中，特定租户集的服务和存储被分组到一个自包含的 单元 中，不同的单元被设置为可以在很大程度上彼此独立运行。这种方法提供了 故障隔离。
- **按租户备份和恢复**：单独备份每个租户的分片使得可以从备份中恢复租户的状态而不影响其他租户，这在租户意外删除或覆盖重要数据的情况下很有用
- **法规合规性**：数据隐私法规（如 GDPR）赋予个人访问和删除存储的所有关于他们的数据的权利。
- **数据驻留**：如果特定租户的数据需要存储在特定司法管辖区以符合数据驻留法律，具有区域感知的数据库可以允许你将该租户的分片分配给特定区域。

使用分片实现多租户的主要挑战是：
- 它假设每个单独的租户都足够小，可以适应单个节点。如果情况并非如此，并且你有一个对于一台机器来说太大的租户，你将需要在单个租户内额外执行分片，这将我们带回到为可伸缩性进行分片的主题
- 如果你有许多小租户，那么为每个租户创建单独的分片可能会产生太多开销。
- 如果你需要支持跨多个租户连接数据的功能，如果你需要跨多个分片连接数据，这些功能将变得更难实现。

### 键值数据的分片

分片的目标是将数据和查询负载均匀地分布在各节点上。如果每个节点承担公平的份额，那么理论上——10 个节点应该能够处理 10 倍的数据量和 10 倍单个节点的读写吞吐量（忽略复制）

如果分片不公平，使得某些分片比其他分片有更多的数据或查询，我们称之为 **_倾斜_**。如果有一个键具有特别高的负载（例如，社交网络中的名人），我们称之为 **_热键_**。

需要一种算法，它以记录的分区键作为输入，并告诉我们该记录在哪个分片中。
#### 按键的范围分片

一种分片方法是为每个分片分配一个连续的分区键范围

![[file-20250819181734712.png]]
键的范围不一定是均匀分布的，因为你的数据可能不是均匀分布的。键的范围不一定是均匀分布的，因为你的数据可能不是均匀分布的。

键范围分片的一个**缺点**是，如果有大量对相邻键的写入，你很容易得到一个热分片。

##### 重新平衡键范围分片数据

一些数据库，如 HBase 和 MongoDB，允许你在空数据库上配置一组初始分片，这称为 _预分割_。
对于自动管理分片边界的数据库，分片分割通常由以下触发：
- 分片达到配置的大小（例如，在 HBase 上，默认值为 10 GB）
- 在某些系统中，写吞吐量持续高于某个阈值。

键范围分片的一个优点是分片数量适应数据量。如果只有少量数据，少量分片就足够了，因此开销很小；如果有大量数据，每个单独分片的大小被限制在可配置的最大值

缺点是分割分片是一项昂贵的操作，因为它需要将其所有数据重写到新文件中，类似于日志结构存储引擎中的压实。

#### 按键的哈希分片

如果你不关心分区键是否彼此接近（例如，如果它们是多租户应用程序中的租户 ID），一种常见方法是先对分区键进行哈希，然后将其映射到分片。

一个好的哈希函数接受倾斜的数据并使其均匀分布。假设你有一个 32 位哈希函数，它接受一个字符串。每当你给它一个新字符串时，它返回一个介于 0 和 2³² − 1 之间的看似随机的数字。即使输入字符串非常相似，它们的哈希值也会均匀分布在该数字范围内

出于分片目的，哈希函数不需要是密码学强度的：例如，MongoDB 使用 MD5，而 Cassandra 和 ScyllaDB 使用 Murmur3。许多编程语言都内置了简单的哈希函数（因为它们用于哈希表），但**它们可能不适合分片**：例如，在 Java 的 `Object.hashCode()` 和 Ruby 的 `Object#hash` 中，**相同的键在不同的进程中可能有不同的哈希值**，使它们不适合分片

##### 哈希取模节点数

取哈希值 _模_ 系统中的节点数，似乎是将每个键分配给节点的简单方法。_mod N_ 方法的问题是，如果节点数 _N_ 发生变化，大多数键必须从一个节点移动到另一个节点。
![[file-20250819183257697.png]]
_mod N_ 函数易于计算，但它导致非常低效的再平衡，因为存在大量不必要的记录从一个节点移动到另一个节点。我们需要一种不会移动超过必要数据的方法。

##### 固定数量的分片

一个简单但广泛使用的解决方案是创建比节点多得多的分片，并为每个节点分配多个分片。
现在，如果向集群添加一个节点，系统可以从现有节点重新分配一些分片到新节点，直到它们再次公平分布。

![[file-20250819183356908.png]]
在这个模型中，只有整个分片在节点之间移动，这比分割分片更便宜。分片的数量不会改变，也不会改变键到分片的分配。唯一改变的是分片到节点的分配。

这种分片方法被 Citus（PostgreSQL 的分片层）、Riak、Elasticsearch 和 Couchbase 等使用。只要你对首次创建数据库时需要多少分片有很好的估计，它就很有效。然后你可以轻松添加或删除节点，但受限于你不能拥有比分片更多的节点。

如果你发现最初配置的分片数量是错误的，需要进行昂贵的重新分片操作。一些系统不允许在并发写入数据库时进行重新分片，这使得在没有停机时间的情况下更改分片数量变得困难。

当分片大小"恰到好处"时可以实现最佳性能，既不太大也不太小，如果分片数量固定但数据集大小变化，这可能很难实现。

##### 按哈希范围分片

将键范围分片与哈希函数结合，使每个分片包含 _哈希值_ 的范围而不是 _键_ 的范围。

使用 16 位哈希函数的示例，该函数返回 0 到 65,535 = 2¹⁶ − 1 之间的数字（实际上，哈希通常是 32 位或更多）。即使输入键非常相似（例如，连续的时间戳），它们的哈希值也会在该范围内均匀分布。然后我们可以为每个分片分配一个哈希值范围：例如，值 0 到 16,383 分配给分片 0，值 16,384 到 32,767 分配给分片 1，依此类推。

![[file-20250819184151857.png]]
与键范围分片一样，哈希范围分片中的分片在变得太大或负载太重时可以被分割。这仍然是一个昂贵的操作。

与键范围分片相比的缺点是，对分区键的范围查询效率不高，因为**范围内的键现在分散在所有分片**中。

数据仓库如 BigQuery、Snowflake 和 Delta Lake 支持类似的索引方法，尽管术语不同。例如，在 BigQuery 中，分区键决定记录驻留在哪个分区中，而"集群列"决定记录在分区内如何排序。Snowflake 自动将记录分配给"微分区"，但允许用户为表定义集群键。Delta Lake 支持手动和自动分区分配，并支持集群键。聚集数据不仅可以提高范围扫描性能，还可以提高压缩和过滤性能。

哈希范围分片被 YugabyteDB 和 DynamoDB 使用，并且是 MongoDB 中的一个选项。Cassandra 和 ScyllaDB 使用这种方法的一个变体。

![[file-20250819184523866.png]]

##### 一致性哈希

_一致性哈希_ 算法是一种哈希函数，它以满足两个属性的方式将键映射到指定数量的分片：
1. 映射到每个分片的键数大致相等，并且
2. 当分片数量变化时，尽可能少的键从一个分片移动到另一个分片。
这里的一致性描述了键尽可能保持在同一个分片中的倾向。

Cassandra 和 ScyllaDB 使用的分片算法类似于一致性哈希的原始定义。

#### 倾斜的工作负载与缓解热点

一致性哈希确保键在节点间均匀分布，但这并不意味着实际负载是均匀分布的。

如果工作负载高度倾斜，仍然可能最终导致某些服务器过载，而其他服务器几乎处于空闲状态。

基于键范围（或哈希范围）定义分片的系统使得可以将单个热键放在自己的分片中，甚至可能为其分配专用机器，或者在应用程序级别补偿倾斜。

#### 运维：自动/手动再均衡

分片的分割和再平衡是自动发生还是手动发生？
- 一些系统自动决定何时分割分片以及何时将它们从一个节点移动到另一个节点，无需任何人工交互
- 其他系统则让分片由管理员明确配置。还有一个中间地带：例如，Couchbase 和 Riak 自动生成建议的分片分配，但需要管理员提交才能生效。

自动化与自动故障检测结合可能很危险。
在再平衡过程中有人参与可能是件好事。它比完全自动的过程慢，但它可以帮助防止操作意外。

### 请求路由

这个问题为 _请求路由_，它与 _服务发现_ 非常相似，两者之间最大的区别是：
- 对于运行应用程序代码的服务，每个实例通常是**无状态的**，负载均衡器可以将请求发送到任何实例。
- 对于分片数据库，对键的请求只能由包含该键的分片的副本节点处理。

有几种不同的方法：
1. 允许客户端连接任何节点（例如，通过循环负载均衡器）。如果该节点恰好拥有请求适用的分片，它可以直接处理请求；否则，它将请求转发到适当的节点，接收回复，并将回复传递给客户端。
2. 首先将客户端的所有请求发送到路由层，该层确定应该处理每个请求的节点并相应地转发它。这个路由层本身不处理任何请求；它只充当分片感知的负载均衡器。
3. 要求客户端知道分片和分片到节点的分配。在这种情况下，客户端可以直接连接到适当的节点，而无需任何中介。
![[file-20250819185620340.png]]

有一些关键问题：
- 谁决定哪个分片应该存在于哪个节点上？
- 执行路由的组件如何了解分片到节点分配的变化？
- 当分片从一个节点移动到另一个节点时，有一个切换期，在此期间新节点已接管，但对旧节点的请求可能仍在传输中。如何处理这些？

许多分布式数据系统依赖于单独的协调服务（如 ZooKeeper 或 etcd）来跟踪分片分配，使用共识算法来提供容错和防止脑裂。每个节点在 ZooKeeper 中注册自己，ZooKeeper 维护分片到节点的权威映射。其他参与者，如路由层或分片感知客户端，可以在 ZooKeeper 中订阅此信息。每当分片所有权发生变化，或者添加或删除节点时，ZooKeeper 都会通知路由层，以便它可以保持其路由信息最新。

![[file-20250819190804313.png]]

HBase 和 SolrCloud 使用 ZooKeeper 管理分片分配，Kubernetes 使用 etcd 跟踪哪个服务实例在哪里运行。MongoDB 有类似的架构，但它依赖于自己的 _配置服务器_ 实现和 _mongos_ 守护进程作为路由层。Kafka、YugabyteDB 和 TiDB 使用内置的 Raft 共识协议实现来执行此协调功能。

Cassandra、ScyllaDB 和 Riak 采用不同的方法：它们在节点之间使用 _流言协议_ 来传播集群状态的任何变化。这提供了比共识协议弱得多的一致性；可能会出现脑裂，其中集群的不同部分对同一分片有不同的节点分配。无主数据库可以容忍这一点，因为它们通常提供弱一致性保证.

关于请求路由的讨论集中在查找单个键的分片，这对于分片 OLTP 数据库最相关。
分析数据库通常也使用分片，但它们通常有非常不同类型的查询执行：查询通常需要并行聚合和连接来自许多不同分片的数据，而不是在单个分片中执行。

### 分片与二级索引

如果涉及二级索引，情况会变得更加复杂。二级索引通常不唯一地标识记录，而是一种搜索特定值出现的方法：查找用户 `123` 的所有操作、查找包含单词 `hogwash` 的所有文章、查找颜色为 `red` 的所有汽车等。

键值存储通常没有二级索引，但它们是关系数据库的基础，在文档数据库中也很常见，它们是 Solr 和 Elasticsearch 等搜索引擎的 _存在理由_。二级索引的问题是它们不能整齐地映射到分片。有两种主要方法来使用二级索引对数据库进行分片：本地索引和全局索引。

#### 本地二级索引

本地二级索引：每个分片只索引其自己分片内的记录。
如果你想让用户搜索汽车，允许他们按颜色和制造商过滤，你需要在 `color` 和 `make` 上建立二级索引。如果你已声明索引，数据库可以自动执行索引。

![[file-20250819192153292.png]]
在这种索引方法中，每个分片是完全独立的：每个分片维护自己的二级索引，仅覆盖该分片中的文档。它不关心存储在其他分片中的数据。每当你需要写入数据库——添加、删除或更新记录——你只需要处理包含你正在写入的文档 ID 的分片。出于这个原因，这种类型的二级索引被称为 **_本地索引_**。在信息检索上下文中，它也被称为 **_文档分区索引_**

本地二级索引被广泛使用：例如，MongoDB、Riak、Cassandra、Elasticsearch、SolrCloud 和 VoltDB  都使用本地二级索引。

#### 全局二级索引

可以构建一个覆盖所有分片数据的 **_全局索引_**，而不是每个分片有自己的本地二级索引。
全局索引也必须进行分片，但它可以以不同于主键索引的方式进行分片。

全局二级索引反映来自所有分片的数据，并且本身按索引值进行分片。
来自所有分片的红色汽车的 ID 出现在索引的 `color:red` 下，但索引是分片的
![[file-20250819192456076.png]]

全局索引的**优点**是具有单个条件的查询（如 _color = red_）只需要从单个分片读取以获取发布列表。但是，如果你想获取记录而不仅仅是 ID，你仍然必须从负责这些 ID 的所有分片中读取。

如果你有多个搜索条件或词项（例如，搜索某种颜色和某种制造商的汽车，或搜索同一文本中出现的多个单词），很可能这些词项将被分配给不同的分片。要计算两个条件的逻辑 AND，系统需要找到两个发布列表中都出现的所有 ID。如果发布列表很短，这没问题，但如果它们很长，通过网络发送它们来计算它们的交集可能会很慢。

全局二级索引的另一个挑战是写入比本地索引更复杂，因为写入单个记录可能会影响索引的多个分片。

全局二级索引被 CockroachDB、TiDB 和 YugabyteDB 使用；DynamoDB 支持本地和全局二级索引。

## 8. 事务

在数据系统的残酷现实中，很多事情都可能出错，为了实现可靠性，系统必须处理这些故障，确保它们不会导致整个系统的灾难性故障。

数十年来，事务一直是简化这些问题的首选机制。事务是应用程序将多个读写操作组合成一个逻辑单元的一种方式。

事务中的所有读写操作被视作单个操作来执行：整个事务要么成功（_提交_），要么失败（_中止_、_回滚_）。

事务不是**自然法则**；它们是**有目的地创建的**，即为了**简化应用程序的编程模型**。通过使用事务，应用程序可以自由地忽略某些潜在的错误场景和并发问题，因为数据库会替应用处理好这些。

### 事务到底是什么？

几乎所有的关系型数据库和一些非关系数据库都支持事务。大多遵循 1975 年由 IBM System R（第一个 SQL 数据库）引入的风格。MySQL、PostgreSQL、Oracle、SQL Server 等的事务支持与 System R 惊人地相似。

在 2000 年代后期，非关系（NoSQL）数据库开始流行起来。事务是这一运动的主要牺牲品：许多这一代数据库完全放弃了事务，或者重新定义了这个词，用来描述比以前理解的更弱的保证集。

围绕 NoSQL 分布式数据库的炒作导致了一种流行的信念，任何大规模系统都必须放弃事务以保持良好的性能和高可用性。最近，这种信念被证明是错误的。

所谓的"NewSQL"数据库， 已经证明，事务系统可以扩展到大数据量和高吞吐量。这些系统将分片与共识协议相结合，以大规模提供强 ACID 保证。

#### ACID 的含义

事务提供的安全保证通常由众所周知的首字母缩略词 _ACID_ 来描述，它代表_原子性_（Atomicity）、_一致性_（Consistency）、_隔离性_（Isolation）和_持久性_（Durability）。它由 Theo Härder 和 Andreas Reuter 于 1983 年提出，旨在为数据库中的容错机制建立精确的术语。

不符合 ACID 标准的系统有时被称为 _BASE_，它代表基本可用（Basically Available）、_软状态_（Soft state）和最终一致性（Eventual consistency）似乎 BASE 唯一合理的定义是"非 ACID"；即，它几乎可以代表任何你想要的东西。

##### 原子性

ACID 原子性描述了当客户端想要进行多次写入，但在某些写入被处理后发生故障时会发生什么——例如，进程崩溃、网络连接中断、磁盘变满或违反了某些完整性约束。如果这些写入被分组到一个原子事务中，并且由于故障无法完成（_提交_）事务，则事务被_中止_，数据库必须丢弃或撤消该事务中迄今为止所做的任何写入。

##### 一致性

_一致性_这个词被严重滥用：
- 副本一致性和异步复制系统中出现的最终一致性问题
- 数据库的一致快照（例如，用于备份）是整个数据库在某一时刻存在的快照。
- 一致性哈希是某些系统用于再平衡的分片方法
- 在 CAP 定理中，一致性一词用于表示线性一致性
- 在 ACID 的上下文中，一致性是指应用程序特定的数据库处于"良好状态"的概念。

📌ACID 一致性的思想是，你对**数据有某些陈述（_不变式_）必须始终为真**。例如，在会计系统中，所有账户的贷方和借方必须始终平衡。
_ACID_ 一致性是指事务的执行不会破坏数据库的完整性约束_，所谓的完整性约束包括数据关系的完整性和业务逻辑的完整性。
ACID 中的 C 通常取决于应用程序如何使用数据库，而不仅仅是数据库的属性。
##### 隔离性

ACID 意义上的_隔离性_意味着同时执行的事务彼此隔离：它们不能相互干扰。经典的数据库教科书将隔离性形式化为 _可串行化_，这意味着每个事务可以假装它是唯一在整个数据库上运行的事务。

![[file-20250819195852641.png]]

**可串行化有性能成本**。在实践中，许多数据库使用比**可串行化更弱的隔离形式**：也就是说，它们允许并发事务以有限的方式相互干扰。一些流行的数据库，如 Oracle，甚至没有实现它（Oracle 有一个称为"可串行化"的隔离级别，但它实际上实现了_快照隔离_，这是比可串行化更弱的保证）

##### 持久性

_持久性_ 是一个承诺，即一旦事务成功提交，它写入的任何数据都不会被遗忘，即使发生硬件故障或数据库崩溃。

完美的持久性不存在：如果所有硬盘和所有备份同时被销毁，显然你的数据库无法挽救你。

在实践中，没有一种技术可以提供绝对保证。只有各种降低风险的技术，包括写入磁盘、复制到远程机器和备份——它们可以而且应该一起使用。

#### 单对象与多对象操作

在 ACID 中，原子性和隔离性描述了如果客户端在同一事务中进行多次写入，数据库应该做什么：
- **原子性**：如果在写入序列的中途发生错误，事务应该被中止，并且到该点为止所做的写入应该被丢弃。
- **隔离性**：并发运行的事务不应该相互干扰。

![[file-20250819201128041.png]]
并发运行的事务不应该相互干扰。邮箱列表显示有未读消息，但计数器显示零未读消息，因为计数器增量尚未发生。
如果在事务过程中某处发生错误，邮箱的内容和未读计数器可能会失去同步。在原子事务中，如果对计数器的更新失败，事务将被中止，插入的电子邮件将被回滚。

![[file-20250819201215890.png]]

多对象事务需要某种方式来确定哪些读写操作属于同一事务。在关系数据库中，这通常基于客户端与数据库服务器的 TCP 连接：在任何特定连接上，`BEGIN TRANSACTION` 和 `COMMIT` 语句之间的所有内容都被认为是同一事务的一部分。如果 TCP 连接中断，事务必须被中止。

##### 单对象写入

当单个对象被更改时，原子性和隔离性也适用。原子性可以使用日志实现崩溃恢复，隔离性可以使用每个对象上的锁来实现（一次只允许一个线程访问对象）。

##### 多对象事务的需求

在许多其他情况下，需要协调对多个不同对象的写入：
- 在关系数据模型中，一个表中的行通常具有对另一个表中行的外键引用。插入引用彼此的多个记录时，外键必须正确且最新，否则数据变得毫无意义。
- 在具有二级索引的数据库中，每次更改值时都需要更新索引。从事务的角度来看，这些索引是不同的数据库对象。

##### 处理错误和中止

事务的一个关键特性是，如果发生错误，它可以被中止并安全地重试。ACID 数据库基于这样的哲学：如果数据库有违反其原子性、隔离性或持久性保证的危险，它宁愿完全放弃事务，也不允许它保持半完成状态。

并非所有系统都遵循这种哲学。特别是，具有无领导者复制的数据存储更多地基于"尽力而为"的基础工作，数据库将尽其所能，如果遇到错误，它不会撤消已经完成的操作。

尽管重试中止的事务是一种简单有效的错误处理机制，但它并不完美。
### 弱隔离级别

如果两个事务不访问相同的数据，或者都是只读的，它们可以安全地并行运行，因为它们互不依赖。仅当一个事务读取另一个事务并发修改的数据时，或者当两个事务尝试同时修改相同的数据时，才会出现并发问题（竞态条件）。

并发错误很难通过测试发现，因为这些错误只有在时机不巧时才会触发。
出于这个原因，数据库长期以来一直试图通过提供 _事务隔离_ 来向应用程序开发人员隐藏并发问题。

在实践中，隔离不幸并不那么简单。可串行化隔离有性能成本，许多数据库不愿意支付这个代价。

因此，系统通常使用较弱的隔离级别，这些级别可以防止_某些_并发问题，但不是全部。这些隔离级别更难理解，它们可能导致微妙的错误，但它们在实践中仍然被使用。

将研究实践中使用的几种弱（非可串行化）隔离级别，并详细讨论哪些竞态条件可以发生和不能发生，以便你可以决定哪个级别适合你的应用程序。

#### 读已提交

最基本的事务隔离级别是 _读已提交_ 。它提供两个保证：

1. 从数据库读取时，你只会看到已经提交的数据（没有 _脏读_ ）。
2. 写入数据库时，你只会覆盖已经提交的数据（没有 _脏写_ ）。

##### 没有脏读

**脏读**：一个事务已经向数据库写入了一些数据，但事务尚未提交或中止。另一个事务能看到那个未提交的数据吗？如果能，这称为 _脏读_ 。

在读已提交隔离级别下运行的事务必须防止脏读。这意味着事务的任何写入只有在该事务提交时才对其他人可见（然后它的所有写入立即变得可见）

![[file-20250819205440817.png]]
防止脏读是有用的：
- 如果事务需要更新多行，脏读意味着另一个事务可能看到某些更新但不是其他更新。
- 如果事务中止，它所做的任何写入都需要回滚。如果数据库允许脏读，这意味着事务可能看到后来被回滚的数据——即从未实际提交到数据库的数据。

##### 没有脏写

**_脏写_** ：如果前面的写入是尚未提交的事务的一部分，因此后面的写入覆盖了一个未提交的值，会发生什么？这称为 _脏写_ 。在读已提交隔离级别下运行的事务必须防止脏写，通常通过延迟第二个写入直到第一个写入的事务已提交或中止。

避免的相关问题：
- 如果事务更新多行，脏写可能导致糟糕的结果。销售被授予 Bryce（因为他对 `listings` 表执行了获胜的更新），但发票被发送给 Aaliyah（因为她对 `invoices` 表执行了获胜的更新）。读已提交防止了这种事故。
- _不_ 防止两个计数器递增之间的竞态条件。
![[file-20250821143607456.png]]

##### 实现读已提交

读已提交是一个非常流行的隔离级别。它是 Oracle Database、PostgreSQL、SQL Server 和许多其他数据库中的默认设置。

- **防止脏写**：数据库通过使用行级锁来防止脏写：当事务想要修改特定行（或文档或其他对象）时，它必须首先获取该行的锁。然后它必须持有该锁直到事务提交或中止。任何给定行只能有一个事务持有锁；如果另一个事务想要写入同一行，它必须等到第一个事务提交或中止后才能获取锁并继续。
- **防止脏读**：方法一是使用相同的锁，并要求任何想要读取行的事务短暂地获取锁，然后在读取后立即再次释放它。损害只读事务的响应时间。 IBM Db2 和 Microsoft SQL Server这样使用。方法二是对于每个被写入的行，数据库记住旧的已提交值和当前持有写锁的事务设置的新值。当事务正在进行时，任何其他读取该行的事务都只是被给予旧值。只有当新值被提交时，事务才会切换到读取新值。

#### 快照隔离与可重复读

读已提交隔离级别时，仍然有很多方式可能出现并发错误：


假设 Aaliyah 在银行有 1,000 美元的储蓄，分成两个账户，每个 500 美元。现在一笔事务从她的一个账户转账 100 美元到另一个账户。如果她不幸在该事务处理的同时查看她的账户余额列表，她可能会看到一个账户余额在收款到达之前（余额为 500 美元），另一个账户在转出之后（新余额为 400 美元）。对 Aaliyah 来说，现在她的账户总共只有 900 美元——似乎 100 美元凭空消失了。
![[file-20250821145032175.png]]

这种异常称为 _读偏斜_ ，_偏斜_ 这里它意味着 _时序异常_ 。读偏斜在读已提交隔离下被认为是可接受的。

某些情况不能容忍这种临时的不一致性：
- **备份**：进行备份需要复制整个数据库，对于大型数据库可能需要几个小时。在备份过程运行期间，写入将继续对数据库进行。如果你需要从这样的备份恢复，不一致性（如消失的钱）将变成永久性的。
- **分析查询和完整性检查**：如果这些查询在不同时间点观察数据库的不同部分，它们很可能返回无意义的结果。

**快照隔离**：每个事务从数据库的_一致快照_读取——也就是说，事务看到事务开始时数据库中已提交的所有数据。即使数据随后被另一个事务更改，每个事务也只能看到该特定时间点的旧数据。快照隔离对于长时间运行的只读查询（如备份和分析）来说是一个福音。

快照隔离是一个流行的功能：它的变体受到 PostgreSQL、使用 InnoDB 存储引擎的 MySQL、Oracle、SQL Server 等的支持，尽管详细行为因系统而异。

##### 多版本并发控制（MVCC）

快照隔离的实现通常使用写锁来防止脏写，快照隔离的一个关键原则是_读者永远不会阻塞写者，写者永远不会阻塞读者_。这允许数据库在一致快照上处理长时间运行的读查询，同时正常处理写入，两者之间没有任何锁争用。

_多版本并发控制_（MVCC）：数据库必须潜在地保留每行的几个不同的已提交版本，而不是每行的两个版本（已提交版本和被覆盖但尚未提交的版本），因为各种正在进行的事务可能需要在不同时间点看到数据库的状态。因为它并排维护一行的多个版本。

PostgreSQL 中如何实现基于 MVCC 的快照隔离。当事务启动时，它被赋予一个唯一的、始终递增的事务 ID（`txid`）。每当事务向数据库写入任何内容时，它写入的数据都用写入者的事务 ID 标记。（准确地说，PostgreSQL 中的事务 ID 是 32 位整数，因此它们在大约 40 亿个事务后溢出。清理过程执行清理以确保溢出不会影响数据。）
![[file-20250821183903795.png]]

表中的每一行都有一个 `inserted_by` 字段，包含将此行插入表中的事务的 ID。此外，每行都有一个 `deleted_by` 字段，最初为空。如果事务删除一行，该行实际上不会从数据库中删除，而是通过将 `deleted_by` 字段设置为请求删除的事务的 ID 来标记为删除。在稍后的某个时间，当确定没有事务可以再访问已删除的数据时，数据库中的垃圾收集过程会删除任何标记为删除的行并释放它们的空间。

行的所有版本都存储在同一个数据库堆中，无论写入它们的事务是否已提交。同一行的版本形成一个链表，从最新版本到最旧版本或相反，以便查询可以在内部迭代行的所有版本。

##### 观察一致快照的可见性规则

当事务从数据库读取时，事务 ID 用于决定它可以看到哪些行版本以及哪些是不可见的。
1. 在每个事务开始时，数据库列出当时正在进行（尚未提交或中止）的所有其他事务。这些事务所做的任何写入都被忽略，即使事务随后提交。这确保我们看到一个不受另一个事务提交影响的一致快照。
2. 具有较晚事务 ID（即在当前事务开始后开始，因此不包括在正在进行的事务列表中）的事务所做的任何写入都被忽略，无论这些事务是否已提交。
3. 中止事务所做的任何写入都被忽略，无论该中止何时发生。这样做的好处是，当事务中止时，我们不需要立即从存储中删除它写入的行，因为可见性规则会将它们过滤掉。垃圾收集过程可以稍后删除它们。
4. 所有其他写入对应用程序的查询可见。

换句话说，如果以下两个条件都为真，则行是可见的：

- 在读者事务开始时，插入该行的事务已经提交。
- 该行未标记为删除，或者如果是，请求删除的事务在读者事务开始时尚未提交。

##### 索引与快照隔离

索引如何在多版本数据库中工作？最常见的方法是每个索引条目指向与该条目匹配的行的一个版本。每个行版本可能包含对下一个最旧或下一个最新版本的引用。使用索引的查询必须迭代行以找到可见的行，并且值与查询要查找的内容匹配。

使用不可变 B 树，每个写事务（或事务批次）都会创建一个新的 B 树根，特定的根是创建时数据库的一致快照。不需要基于事务 ID 过滤行，因为后续写入无法修改现有的 B 树；它们只能创建新的树根。这种方法还需要后台进程进行压缩和垃圾收集。

##### 快照隔离、可重复读和命名混淆

MVCC 是数据库常用的实现技术，通常用于实现快照隔离。
快照隔离在 PostgreSQL 中称为"可重复读"，在 Oracle 中称为"可串行化；
 MySQL 中它意味着比快照隔离更弱一致性的 MVCC 实现

**这种命名混淆的原因是 SQL 标准没有快照隔离的概念**

SQL 标准对隔离级别的定义是有缺陷的——它是模糊的、不精确的

#### 防止丢失更新

读已提交和快照隔离级别主要是关于只读事务在并发写入存在的情况下可以看到什么的保证。
两个事务并发写入的问题，最著名的是 _丢失更新_ 问题。

如果应用程序从数据库读取某个值，修改它，然后写回修改后的值（_读-修改-写循环_），就会出现丢失更新问题。如果两个事务并发执行此操作，其中一个修改可能会丢失，因为第二个写入不包括第一个修改。（我们有时说后面的写入_覆盖_了前面的写入。）这种模式出现在各种不同的场景中：
- 递增计数器或更新账户余额（需要读取当前值，计算新值，并写回更新的值）
- 对复杂值进行本地更改，例如，向 JSON 文档中的列表添加元素

常见的解决方案

##### 原子写操作

许多数据库提供原子更新操作，消除了在应用程序代码中实现读-修改-写循环的需要。
``
```
UPDATE counters SET value = value + 1 WHERE key = 'foo';
```

上述指令在大多数关系数据库中是并发安全的。文档数据库（如 MongoDB）提供原子操作来对 JSON 文档的一部分进行本地修改，Redis 提供原子操作来修改数据结构（如优先级队列）。并非所有写入都可以轻松地用原子操作来表达

原子操作通常通过在读取对象时对其进行独占锁来实现，以便在应用更新之前没有其他事务可以读取它。

不幸的是，对象关系映射（ORM）框架很容易意外地编写执行不安全的读-修改-写循环的代码，而不是使用数据库提供的原子操作。

##### 显式锁定

应用程序显式锁定要更新的对象。然后应用程序可以执行读-修改-写循环，如果任何其他事务尝试并发更新或锁定同一对象，它将被迫等到第一个读-修改-写循环完成。

```sql
BEGIN TRANSACTION;

SELECT * FROM figures
    WHERE name = 'robot' AND game_id = 222
    FOR UPDATE; ❶

-- 检查移动是否有效，然后更新
-- 前一个 SELECT 返回的棋子的位置。
UPDATE figures SET position = 'c4' WHERE id = 1234;

COMMIT;
```

`FOR UPDATE` 子句表示数据库应该对此查询返回的所有行进行锁定。

这是有效的，但要正确执行，你需要仔细考虑你的应用程序逻辑。很容易忘记在代码中的某个地方添加必要的锁，从而引入竞态条件。

如果你锁定多个对象，则存在死锁的风险，其中两个或多个事务正在等待彼此释放锁。许多数据库会自动检测死锁，并中止涉及的事务之一，以便系统可以取得进展。你可以在应用程序级别通过重试中止的事务来处理这种情况。

##### 自动检测丢失的更新

原子操作和锁是通过强制读-修改-写循环按顺序发生来防止丢失更新的方法。另一种选择是允许它们并行执行，如果事务管理器检测到丢失的更新，则中止事务并强制它重试其读-修改-写循环。

这种方法的一个优点是数据库可以与快照隔离一起有效地执行此检查。实际上，PostgreSQL 的可重复读、Oracle 的可串行化和 SQL Server 的快照隔离级别会自动检测何时发生丢失的更新并中止有问题的事务。然而，MySQL/InnoDB 的可重复读不检测丢失的更新

##### 条件写入（比较并设置）

在不提供事务的数据库中，你有时会发现一个_条件写入_操作，它可以通过仅在值自你上次读取以来未更改时才允许更新来防止丢失的更新。如果当前值与你之前读取的不匹配，则更新无效，必须重试读-修改-写循环。它是许多 CPU 支持的原子_比较并设置_或_比较并交换_（CAS）指令的数据库等价物。
```sql
-- 这可能安全也可能不安全，取决于数据库实现
UPDATE wiki_pages SET content = 'new content'
    WHERE id = 1234 AND content = 'old content';
```

如果内容已更改并且不再匹配 `'old content'`，则此更新将无效，因此你需要检查更新是否生效并在必要时重试。你也可以使用在每次更新时递增的版本号列，并且仅在当前版本号未更改时才应用更新，而不是比较完整内容。这种方法有时称为 _乐观锁定_ 。

##### 冲突解决与复制

防止丢失的更新具有另一个维度：由于它们在多个节点上有数据副本，并且数据可能在不同节点上并发修改，因此需要采取一些额外的步骤来防止丢失的更新。

锁和条件写入操作假设有一个最新的数据副本。然而，具有多领导者或无领导者复制的数据库通常允许多个写入并发发生并异步复制它们，因此它们不能保证有一个最新的数据副本。因此，基于锁或条件写入的技术在此上下文中不适用。

此类复制数据库中的常见方法是允许并发写入创建值的多个冲突版本（也称为_兄弟节点_），并使用应用程序代码或特殊数据结构在事后解决和合并这些版本。

#### 写偏斜与幻读

 _脏写_ 和 _丢失更新_ ，这是当不同事务并发尝试写入相同对象时可能发生的两种竞态条件。还有一些其他的冲突。

例子：你正在为医生编写一个应用程序来管理他们在医院的值班班次。医院通常试图在任何时候都有几位医生值班，但绝对必须至少有一位医生值班。医生可以放弃他们的班次（例如，如果他们自己生病了），前提是该班次中至少有一位同事留在值班

想象 Aaliyah 和 Bryce 是特定班次的两位值班医生。两人都感觉不舒服，所以他们都决定请假。不幸的是，他们碰巧大约在同一时间点击了下班的按钮。

![[file-20250821192801931.png]]两个事务都提交，现在没有医生值班。你至少有一个医生值班的要求被违反了。

##### 描述写偏斜

**写倾斜**：既不是脏写也不是丢失的更新，因为两个事务正在更新两个不同的对象。如果两个事务一个接一个地运行，第二个医生将被阻止下班。异常行为只有在事务并发运行时才可能。

写偏斜可以视为丢失更新问题的概括。

如果两个事务读取相同的对象，然后更新其中一些对象（不同的事务可能更新不同的对象），就会发生写偏斜。在不同事务更新同一对象的特殊情况下，你会得到脏写或丢失更新异常（取决于时机）。

写偏斜处置方法的限制：
- 原子单对象操作没有帮助，因为涉及多个对象。
- 某些快照隔离实现中发现的丢失更新的自动检测也没有帮助，自动防止写偏斜需要真正的可串行化隔离
- 某些数据库允许你配置约束，然后由数据库强制执行
- 如果你不能使用可串行化隔离级别，在这种情况下，第二好的选择可能是显式锁定事务所依赖的行。

##### 更多写偏斜的例子

- **会议室预订系统**：假设你想强制同一会议室在同一时间不能有两个预订。当有人想要预订时，你首先检查是否有任何冲突的预订（即，具有重叠时间范围的同一房间的预订），如果没有找到，你就创建会议。
- **多人游戏**：锁不会阻止玩家将两个不同的棋子移动到棋盘上的同一位置，或者可能做出违反游戏规则的其他移动。根据你要执行的规则类型，你可能能够使用唯一约束，但否则你很容易受到写偏斜的影响。
- **声明用户名**：在每个用户都有唯一用户名的网站上，两个用户可能同时尝试使用相同的用户名创建账户。你可以使用事务来检查名称是否被占用，如果没有，使用该名称创建账户。但是，就像前面的例子一样，这在快照隔离下是不安全的。幸运的是，唯一约束在这里是一个简单的解决方案

##### 导致写偏斜的幻读

所有这些例子都遵循类似的模式：

1. `SELECT` 查询通过搜索匹配某些搜索条件的行来检查是否满足某些要求（至少有两个医生值班，该房间在该时间没有现有预订，棋盘上的位置还没有另一个棋子，用户名尚未被占用，账户中仍有钱）。
2. 根据第一个查询的结果，应用程序代码决定如何继续（也许继续操作，或者向用户报告错误并中止）。
3. 如果应用程序决定继续，它会向数据库进行写入（`INSERT`、`UPDATE` 或 `DELETE`）并提交事务。

其中一个事务中的写入改变另一个事务中搜索查询的结果，称为 _幻读_ 。
快照隔离避免了**只读查询中的幻读**，但在我们讨论的读写事务中，幻读可能导致特别棘手的写偏斜情况。ORM 生成的 SQL 也容易出现写偏斜。

##### 物化冲突

**物化冲突**：采用了幻读并将其转化为存在于数据库中的具体行集上的锁冲突。
很难且容易出错地弄清楚如何物化冲突，并且让并发控制机制泄漏到应用程序数据模型中是丑陋的。出于这些原因，如果没有其他选择，物化冲突应被视为最后的手段。在大多数情况下，可串行化隔离级别要好得多。

### 可串行化

**可串行化隔离**是最强的隔离级别。它保证即使事务可能并行执行，最终结果与它们_串行_执行（一次一个，没有任何并发）相同。因此，数据库保证如果事务在单独运行时行为正确，那么在并发运行时它们继续保持正确——换句话说，数据库防止了_所有_可能的竞态条件。

今天提供可串行化的大多数数据库使用以下三种技术之一，我们将在本章的其余部分探讨：
- 字面上串行执行事务
- 两阶段锁定
- 乐观并发控制技术，如可串行化快照隔离


#### 实际串行执行

避免并发问题的最简单方法是完全消除并发：在单个线程上按串行顺序一次执行一个事务。通过这样做，我们完全回避了检测和防止事务之间冲突的问题：所产生的隔离根据定义是可串行化的。

在过去 30 年中多线程并发被认为是获得良好性能的必要条件，那是什么改变使得单线程执行成为可能？
- RAM 变得足够便宜，对于许多用例，现在可以将整个活动数据集保存在内存中,事务的执行速度比必须等待从磁盘加载数据要快得多。
- 数据库设计者意识到 OLTP 事务通常很短，只进行少量读写,相比之下，长时间运行的分析查询通常是只读的，因此它们可以在串行执行循环之外的一致快照上运行

串行执行事务的方法在 VoltDB/H-Store、Redis 和 Datomic 等中实现。为单线程执行设计的系统有时可以比支持并发的系统性能更好，因为它可以避免锁定的协调开销。

##### 将事务封装在存储过程中

有一些问题：
- 每个数据库供应商都有自己的存储过程语言，它们看起来相当丑陋和过时，并且缺乏大多数编程语言中的库生态系统。
- 在数据库中运行的代码很难管理：与应用程序服务器相比，调试更困难，版本控制和部署更尴尬，测试更棘手
- 数据库通常比应用程序服务器对性能更敏感，因为单个数据库实例通常由许多应用程序服务器共享。数据库中编写不当的存储过程（例如，使用大量内存或 CPU 时间）可能比应用程序服务器中等效的编写不当的代码造成更多麻烦。

存储过程在应用程序逻辑无法轻松嵌入其他地方的情况下也很有用。使用存储过程和内存数据，在单个线程上执行所有事务变得可行。当存储过程不需要等待 I/O 并避免其他并发控制机制的开销时，它们可以在单个线程上实现相当好的吞吐量。
##### 分片

串行执行所有事务使并发控制变得简单得多，但将数据库的事务吞吐量限制为单台机器上单个 CPU 核心的速度。只读事务可以使用快照隔离在其他地方执行，但对于具有高写入吞吐量的应用程序，单线程事务处理器可能成为严重的瓶颈。

为了扩展到多个 CPU 核心和多个节点，你可以对数据进行分片。

由于跨分片事务具有额外的协调开销，因此它们比单分片事务慢得多。VoltDB 报告的跨分片写入吞吐量约为每秒 1,000 次，这比其单分片吞吐量低几个数量级，并且无法通过添加更多机器来增加。最近的研究探索了使多分片事务更具可伸缩性的方法。

##### 串行执行总结

串行执行事务已成为在某些约束条件下实现可串行化隔离的可行方法：
- 每个事务必须小而快，因为只需要一个缓慢的事务就可以阻止所有事务处理。
- 它最适合活动数据集可以适合内存的情况。很少访问的数据可能会移到磁盘，但如果需要在单线程事务中访问，系统会变得非常慢。
- 写入吞吐量必须足够低，可以在单个 CPU 核心上处理，否则事务需要分片而不需要跨分片协调。
- 跨分片事务是可能的，但它们的吞吐量很难扩展。

#### 两阶段锁定（2PL）

大约 30 年来，数据库中只有一种广泛使用的可串行化算法：_两阶段锁定_（2PL），有时称为_强严格两阶段锁定_（SS2PL），以区别于 2PL 的其他变体。

两阶段_锁定_（2PL）和两阶段_提交_（2PC）是两个**非常不同**的东西。**2PL 提供可串行化隔离**，而 **2PC 在分布式数据库中提供原子提交**。

两阶段锁定使锁要求更强。只要没有人写入，多个事务就可以并发读取同一对象。但是一旦有人想要写入（修改或删除）对象，就需要独占访问：
- 如果事务 A 已读取对象而事务 B 想要写入该对象，B 必须等到 A 提交或中止后才能继续。
- 如果事务 A 已写入对象而事务 B 想要读取该对象，B 必须等到 A 提交或中止后才能继续。

在 2PL 中，**写入者不仅阻塞其他写入者；它们还阻塞读者，反之亦然**。快照隔离有这样的口号：_**读者永远不会阻塞写者，写者永远不会阻塞读者_**。

 2PL 提供可串行化，它可以防止早期讨论的所有竞态条件，包括丢失的更新和写偏斜。

##### 两阶段锁定的实现

2PL 由 MySQL（InnoDB）和 SQL Server 中的可串行化隔离级别以及 Db2 中的可重复读隔离级别使用。

读者和写者的阻塞是通过在数据库中的每个对象上有一个锁来实现的。锁可以处于_共享模式_或_独占模式_（也称为_多读者单写者_锁）。锁的使用如下：

- 如果事务想要读取对象，它必须首先以共享模式获取锁。多个事务可以同时以共享模式持有锁，但如果另一个事务已经对该对象具有独占锁，则这些事务必须等待。
- 如果事务想要写入对象，它必须首先以独占模式获取锁。没有其他事务可以同时持有锁（无论是共享模式还是独占模式），因此如果对象上有任何现有锁，事务必须等待。
- 如果事务首先读取然后写入对象，它可以将其共享锁升级为独占锁。升级的工作方式与直接获取独占锁相同。
- 获取锁后，事务必须继续持有锁直到事务结束（提交或中止）。这就是"两阶段"名称的来源：第一阶段（事务执行时）是获取锁，第二阶段（事务结束时）是释放所有锁。

由于使用了如此多的锁，很容易发生事务 A 等待事务 B 释放其锁，反之亦然的情况。这种情况称为_死锁_。数据库自动检测事务之间的死锁并中止其中一个，以便其他事务可以取得进展。中止的事务需要由应用程序重试。

##### 两阶段锁定的性能

两阶段锁定的主要缺点是性能：在两阶段锁定下，事务吞吐量和查询响应时间明显比弱隔离下差。

例如，如果你有一个需要读取整个表的事务，该事务必须对整个表进行共享锁。因此，读取事务首先必须等到所有正在写入该表的进行中事务完成；然后，在读取整个表时（对于大表可能需要很长时间），所有想要写入该表的其他事务都被阻塞，直到大型只读事务提交。实际上，数据库在很长一段时间内无法进行写入。

运行 2PL 的数据库可能具有相当不稳定的延迟，如果工作负载中存在争用，它们在高百分位数可能非常慢。
可能只需要一个缓慢的事务，或者一个访问大量数据并获取许多锁的事务，就会导致系统的其余部分停滞不前。

当事务由于死锁而被中止并重试时，它需要重新完成所有工作。如果死锁频繁，这可能意味着大量的浪费努力。

##### 谓词锁

在“导致写偏斜的幻读”中，我们讨论了幻读的问题——即一个事务改变另一个事务的搜索查询结果。具有可串行化隔离的数据库必须防止幻读。

在会议室预订示例中，这意味着如果一个事务已经搜索了某个时间窗口内某个房间的现有预订，另一个事务不允许并发插入或更新同一房间和时间范围的另一个预订。

从概念上讲，我们需要一个 _谓词锁_ 。它的工作方式类似于前面描述的共享/独占锁，但它不属于特定对象（例如，表中的一行），而是属于匹配某些搜索条件的所有对象，例如：

```sql
SELECT * FROM bookings
 WHERE room_id = 123 AND
 end_time > '2025-01-01 12:00' AND
 start_time < '2025-01-01 13:00';
```


谓词锁限制访问如下：
- 如果事务 A 想要读取匹配某些条件的对象，就像在该 `SELECT` 查询中一样，它必须在查询条件上获取共享模式谓词锁。如果另一个事务 B 当前对匹配这些条件的任何对象具有独占锁，A 必须等到 B 释放其锁后才允许进行查询。
- 如果事务 A 想要插入、更新或删除任何对象，它必须首先检查旧值或新值是否匹配任何现有的谓词锁。如果存在事务 B 持有的匹配谓词锁，则 A 必须等到 B 提交或中止后才能继续。

这里的关键思想是，谓词锁甚至适用于数据库中尚不存在但将来可能添加的对象（幻读）。如果两阶段锁定包括谓词锁，数据库将防止所有形式的写偏斜和其他竞态条件，因此其隔离变为可串行化。

##### 索引范围锁

谓词锁的性能不佳：如果活动事务有许多锁，检查匹配锁变得耗时。因此，大多数具有 2PL 的数据库实际上实现了_索引范围锁定_（也称为_间隙锁_），这是谓词锁定的简化近似。

通过使谓词匹配更大的对象集来简化谓词是安全的。例如，如果你对中午到下午 1 点之间房间 123 的预订有谓词锁，你可以通过锁定房间 123 在任何时间的预订来近似它，或者你可以通过锁定中午到下午 1 点之间的所有房间（不仅仅是房间 123）来近似它。这是安全的，因为匹配原始谓词的任何写入肯定也会匹配近似。

在房间预订数据库中，你可能在 `room_id` 列上有索引，和/或在 `start_time` 和 `end_time` 上有索引（否则前面的查询在大型数据库上会非常慢）：

- 假设你的索引在 `room_id` 上，数据库使用此索引查找房间 123 的现有预订。现在数据库可以简单地将共享锁附加到此索引条目，表示事务已搜索房间 123 的预订。
- 或者，如果数据库使用基于时间的索引查找现有预订，它可以将共享锁附加到该索引中的值范围，表示事务已搜索与 2025 年 1 月 1 日中午到下午 1 点的时间段重叠的预订。

无论哪种方式，搜索条件的近似都附加到其中一个索引。现在，如果另一个事务想要插入、更新或删除同一房间和/或重叠时间段的预订，它将必须更新索引的相同部分。在这样做的过程中，它将遇到共享锁，并被迫等到锁被释放。

这提供了对幻读和写偏斜的有效保护。索引范围锁不如谓词锁精确（它们可能锁定比严格维护可串行化所需的更大范围的对象），但由于它们的开销要低得多，它们是一个很好的折衷。

#### 可串行化快照隔离（SSI）

一方面，我们有性能不佳（两阶段锁定）或扩展性不佳（串行执行）的可串行化实现。另一方面，我们有性能良好但容易出现各种竞态条件（丢失的更新、写偏斜、幻读等）的弱隔离级别。

一种称为_可串行化快照隔离_（SSI）的算法提供完全可串行化，与快照隔离相比只有很小的性能损失。SSI 相对较新：它于 2008 年首次描述。

SSI 和类似算法用于单节点数据库（PostgreSQL 中的可串行化隔离级别、SQL Server 的内存 OLTP/Hekaton 和 HyPer）、分布式数据库（CockroachDB 和 FoundationDB）以及嵌入式存储引擎（如 BadgerDB）。

##### 悲观并发控制与乐观并发控制

两阶段锁定是所谓的_悲观_并发控制机制：它基于这样的原则，即如果任何事情可能出错（如另一个事务持有的锁所示），最好等到情况再次安全后再做任何事情。它就像_互斥_，用于保护多线程编程中的数据结构。

可串行化快照隔离是一种 _乐观_ 并发控制技术。
**基本思想**：如果发生潜在危险的事情，事务不会阻塞，而是继续进行，希望一切都会好起来。当事务想要提交时，数据库会检查是否发生了任何不好的事情（即，是否违反了隔离）；如果是，事务将被中止并必须重试。只允许可串行执行的事务提交。

SSI 基于快照隔离——也就是说，事务中的所有读取都从数据库的一致快照进行。在快照隔离的基础上，SSI 添加了一种算法来检测读写之间的串行化冲突，并确定要中止哪些事务。

##### 基于过时前提的决策

事务基于 _前提_ （事务开始时为真的事实，例如，“当前有两名医生值班”）采取行动。后来，当事务想要提交时，原始数据可能已更改——前提可能不再为真。

为了提供可串行化隔离，数据库必须检测事务可能基于过时前提采取行动的情况，并在这种情况下中止事务。考虑两种情况：

- 检测陈旧的 MVCC 对象版本的读取（未提交的写入发生在读取之前）
- 检测影响先前读取的写入（写入发生在读取之后）

##### 检测陈旧的 MVCC 读取

当事务从 MVCC 数据库中的一致快照读取时，它会忽略在拍摄快照时尚未提交的任何其他事务所做的写入。

事务 43 看到 Aaliyah 的 `on_call = true`，因为事务 42（修改了 Aaliyah 的值班状态）未提交。但是，当事务 43 想要提交时，事务 42 已经提交。这意味着从一致快照读取时被忽略的写入现在已生效，事务 43 的前提不再为真。当写入者插入以前不存在的数据时，事情变得更加复杂

![[file-20250822181201762.png]]

为了防止这种异常，数据库需要跟踪事务由于 MVCC 可见性规则而忽略另一个事务的写入的时间。当事务想要提交时，数据库会检查是否有任何被忽略的写入现在已经提交。如果是，事务必须被中止。

##### 检测影响先前读取的写入

![[file-20250822181529424.png]]
事务 42 和 43 都在班次 `1234` 期间搜索值班医生。如果 `shift_id` 上有索引，数据库可以使用索引条目 1234 来记录事务 42 和 43 读取此数据的事实。此信息只需要保留一段时间：在事务完成（提交或中止）并且所有并发事务完成后，数据库可以忘记它读取的数据。

当事务写入数据库时，它必须在索引中查找最近读取受影响数据的任何其他事务。此过程类似于获取受影响键范围的写锁，但它不是阻塞直到读者提交，而是充当绊线：它只是通知事务它们读取的数据可能不再是最新的。

##### 可串行化快照隔离的性能

与两阶段锁定相比，可串行化快照隔离的主要优点是一个事务不需要阻塞等待另一个事务持有的锁。与快照隔离一样，写入者不会阻塞读者，反之亦然。这种设计原则使查询延迟更可预测且变化更少。特别是，只读查询可以在一致快照上运行而无需任何锁，这对于读取密集型工作负载非常有吸引力。

与串行执行相比，可串行化快照隔离不限于单个 CPU 核心的吞吐量。

与非可串行化快照隔离相比，检查可串行化违规的需要引入了一些性能开销。

中止率显著影响 SSI 的整体性能。例如，长时间读取和写入数据的事务可能会遇到冲突并中止，因此 SSI 要求读写事务相当短（长时间运行的只读事务是可以的）。但是，SSI 对慢事务的敏感性低于两阶段锁定或串行执行。

### 分布式事务

一致性和持久性在转向分布式事务时也没有太大变化。但是，原子性需要更多关注。

对于在单个数据库节点执行的事务，原子性通常由存储引擎实现。当客户端要求数据库节点提交事务时，数据库使事务的写入持久化，然后将提交记录附加到磁盘上的日志。如果数据库在此过程中崩溃，事务将在节点重新启动时从日志中恢复：如果提交记录在崩溃前成功写入磁盘，则事务被认为已提交；如果没有，该事务的任何写入都将回滚。

在单个节点上，事务提交关键取决于数据持久写入磁盘的 _顺序_ ，事务提交或中止的关键决定时刻是磁盘完成写入提交记录的时刻：在那一刻之前，仍然可能中止（由于崩溃），但在那一刻之后，事务已提交（即使数据库崩溃）。因此，是单个设备（连接到特定节点的特定磁盘驱动器的控制器）使提交成为原子的。

如果多个节点参与事务，向所有节点发送提交请求并在每个节点上独立提交事务是不够的。如所示，提交可能在某些节点上成功，在其他节点上失败：

- 某些节点可能检测到约束违规或冲突，需要中止，而其他节点能够成功提交。
- 某些提交请求可能在网络中丢失，最终由于超时而中止，而其他提交请求通过。
- 某些节点可能在提交记录完全写入之前崩溃并在恢复时回滚，而其他节点成功提交。
![[file-20250822182451706.png]]

确保参与事务的节点要么全部提交，要么全部中止，并防止两者的混合。确保这一点被称为 _原子提交_ 问题。

#### 两阶段提交（2PC）

两阶段提交是一种跨多个节点实现原子事务提交的算法。它是分布式数据库中的经典算法。2PC 在某些数据库内部使用，也以 _XA_ 事务的形式提供给应用程序（例如，Java 事务 API 支持）。

2PC 的基本流程，提交/中止过程分为两个阶段
![[file-20250822182837041.png]]

2PC 使用一个通常不会出现在单节点事务中的新组件：_协调器_ 。
协调器通常作为请求事务的同一应用程序进程中的库实现（例如，嵌入在 Java EE 容器中），但它也可以是单独的进程或服务。此类协调器的示例包括 Narayana、JOTM、BTM 或 MSDTC。

使用 2PC 时，分布式事务从应用程序在多个数据库节点上正常读写数据开始。我们称这些数据库节点为事务中的_参与者_。当应用程序准备提交时，协调器开始第 1 阶段：它向每个节点发送_准备_请求，询问它们是否能够提交。然后协调器跟踪参与者的响应：
- 如果所有参与者回复"是”，表示他们准备提交，那么协调器在第 2 阶段发出_提交_请求，提交实际发生。
- 如果任何参与者回复"否"，协调器在第 2 阶段向所有节点发送_中止_请求。

##### 系统性的承诺

2PC有效的原因：
1. 当应用程序想要开始分布式事务时，它从协调器请求事务 ID。此事务 ID 是全局唯一的。
2. 应用程序在每个参与者上开始单节点事务，并将全局唯一的事务 ID 附加到单节点事务。所有读写都在这在此阶段出现任何问题（例如，节点崩溃或请求超时），协调器或任何参与者都可以中止。
3. 当应用程序准备提交时，协调器向所有参与者发送准备请求，标记有全局事务 ID。如果这些请求中的任何一个失败或超时，协调器向所有参与者发送该事务 ID 的中止请求。
4. 当参与者收到准备请求时，它确保它可以在任何情况下明确提交事务。这包括将所有事务数据写入磁盘（崩溃、电源故障或磁盘空间不足不是稍后拒绝提交的可接受借口），并检查任何冲突或约束违规。通过向协调器回复"是"，节点承诺在请求时无错误地提交事务。换句话说，参与者放弃了中止事务的权利，但没有实际提交它。
5. 当协调器收到所有准备请求的响应时，它对是否提交或中止事务做出明确决定（仅当所有参与者投票"是"时才提交）。协调器必须将该决定写入其磁盘上的事务日志，以便在随后崩溃时知道它是如何决定的。这称为_提交点_。
6. 一旦协调器的决定被写入磁盘，提交或中止请求就会发送给所有参与者。如果此请求失败或超时，协调器必须永远重试，直到成功。没有回头路：如果决定是提交，那么必须执行该决定，无论需要多少次重试。如果参与者在此期间崩溃，事务将在恢复时提交——因为参与者投票"是"，它在恢复时不能拒绝提交。

该协议包含两个关键的"不归路"：当参与者投票"是"时，它承诺它肯定能够稍后提交（尽管协调器仍可能选择中止）；一旦协调器决定，该决定是不可撤销的。这些承诺确保了 2PC 的原子性。

##### 协调器故障

![[file-20250822184127242.png]]
没有协调器的消息，参与者无法知道是提交还是中止。原则上，参与者可以相互通信，了解每个参与者如何投票并达成某种协议，但这不是 2PC 协议的一部分。

2PC 完成的唯一方法是等待协调器恢复。这就是为什么协调器必须在向参与者发送提交或中止请求之前将其提交或中止决定写入磁盘上的事务日志：当协调器恢复时，它通过读取其事务日志来确定所有存疑事务的状态。协调器日志中没有提交记录的任何事务都将中止。因此，2PC 的提交点**归结为协调器上的常规单节点原子提交。**

##### 三阶段提交

由于 2PC 可能会卡住等待协调器恢复，因此两阶段提交被称为 _阻塞_ 原子提交协议。可以使原子提交协议 _非阻塞_ ，以便在节点失败时不会卡住。但是，在实践中使其工作并不那么简单。

作为 2PC 的替代方案，已经提出了一种称为 _三阶段提交_ （3PC）的算法。但是，3PC 假设具有有界延迟的网络和具有有界响应时间的节点；在大多数具有无界网络延迟和进程暂停的实际系统中，它无法保证原子性。

#### 跨不同系统的分布式事务

某些分布式事务的实现会带来沉重的性能损失。两阶段提交固有的大部分性能成本是由于崩溃恢复所需的额外磁盘强制（`fsync`）和额外的网络往返。

应该准确说明"分布式事务"的含义。两种完全不同类型的分布式事务经常被混淆：
- **数据库内部分布式事务**：某些分布式数据库（即，在其标准配置中使用复制和分片的数据库）支持该数据库节点之间的内部事务。
- **异构分布式事务**：在_异构_事务中，参与者是两个或多个不同的技术：

##### 精确一次消息处理

异构分布式事务允许以强大的方式集成各种系统。例如，当且仅当处理消息的数据库事务成功提交时，来自消息队列的消息才能被确认为已处理。
有了分布式事务支持，即使消息代理和数据库是在不同机器上运行的两种不相关的技术，这也是可能的。

##### XA 事务

_X/Open XA_（_eXtended Architecture_ 的缩写）是**跨异构技术实现两阶段提交**的标准。
它于 1991 年推出并得到广泛实现：XA 受到许多传统关系数据库（包括 PostgreSQL、MySQL、Db2、SQL Server 和 Oracle）和消息代理（包括 ActiveMQ、HornetQ、MSMQ 和 IBM MQ）的支持。

XA 不是网络协议——它只是用于与事务协调器接口的 C API。此 API 的绑定存在于其他语言中；例如，在 Java EE 应用程序的世界中，XA 事务使用 Java 事务 API（JTA）实现，而 JTA 又由许多使用 Java 数据库连接（JDBC）的数据库驱动程序和使用 Java 消息服务（JMS）API 的消息代理驱动程序支持。

XA 假设你的应用程序使用网络驱动程序或客户端库与参与者数据库或消息服务进行通信。如果驱动程序支持 XA，这意味着它调用 XA API 来确定操作是否应该是分布式事务的一部分——如果是，它将必要的信息发送到数据库服务器。驱动程序还公开回调，协调器可以通过回调要求参与者准备、提交或中止。

##### 存疑时持有锁

数据库事务通常对它们修改的任何行进行行级独占锁，以防止脏写。此外，如果你想要可串行化隔离，使用两阶段锁定的数据库还必须对事务_读取_的任何行进行共享锁。

数据库在事务提交或中止之前不能释放这些锁。因此，使用两阶段提交时，事务必须在存疑期间保持锁。如果协调器崩溃并需要 20 分钟才能重新启动，这些锁将保持 20 分钟。如果协调器的日志由于某种原因完全丢失，这些锁将永远保持——或者至少直到管理员手动解决情况。

当这些锁被持有时，没有其他事务可以修改这些行。根据隔离级别，其他事务甚至可能被阻止读取这些行。因此，其他事务不能简单地继续他们的业务——如果他们想要访问相同的数据，他们将被阻塞。这可能导致你的应用程序的大部分变得不可用，直到存疑事务得到解决。
##### 从协调器故障中恢复

如果协调器崩溃并重新启动，它应该从日志中干净地恢复其状态并解决任何存疑事务。
即使重新启动数据库服务器也无法解决此问题，因为 2PC 的正确实现必须即使在重新启动时也保留存疑事务的锁

唯一的出路是管理员手动决定是提交还是回滚事务。管理员必须检查每个存疑事务的参与者，确定是否有任何参与者已经提交或中止，然后将相同的结果应用于其他参与者。解决问题可能需要大量的手动工作，并且很可能需要在严重的生产中断期间在高压力和时间压力下完成。

许多 XA 实现都有一个名为_启发式决策_的紧急逃生舱口：允许参与者在没有协调器明确决定的情况下单方面决定中止或提交存疑事务

##### XA 事务的问题

单节点协调器是整个系统的单点故障，使其成为应用程序服务器的一部分也是有问题的，因为协调器在其本地磁盘上的日志成为持久系统状态的关键部分——与数据库本身一样重要。

原则上，XA 事务的协调器可以是高可用和复制的，就像我们对任何其他重要数据库的期望一样。不幸的是，这仍然不能解决 XA 的一个根本问题，即它没有为事务的协调器和参与者提供直接相互通信的方式。它们只能通过调用事务的应用程序代码以及调用参与者的数据库驱动程序进行通信。

另一个问题是，由于 XA 需要与各种数据系统兼容，它必然是最低公分母。例如，它无法检测跨不同系统的死锁（因为这需要系统交换有关每个事务正在等待的锁的信息的标准化协议），并且它不适用于 SSI。

#### 数据库内部的分布式事务

许多系统使用两阶段提交来确保写入多个分片的事务的原子性，但它们不会遇到与 XA 事务相同的问题。原因是，由于它们的分布式事务不需要与任何其他技术接口，它们避免了最低公分母陷阱——这些系统的设计者可以自由使用更可靠、更快的更好协议。

A 的最大问题可以通过以下方式解决：

- 复制协调器，如果主协调器崩溃，自动故障转移到另一个协调器节点；
- 允许协调器和数据分片直接通信，而不通过应用程序代码；
- 复制参与分片，以减少由于分片中的故障而必须中止事务的风险；以及
- 将原子提交协议与支持跨分片死锁检测和一致读取的分布式并发控制协议耦合。

共识算法通常用于复制协调器和数据库分片。

为分布式事务提供的隔离级别取决于系统，但跨分片的快照隔离和可串行化快照隔离都是可能的。

##### 再谈精确一次消息处理

分布式事务的一个重要用例是确保某些操作精确生效一次，即使在处理过程中发生崩溃并且需要重试处理。

你实际上不需要这样的分布式事务来实现精确一次语义。另一种方法如下，它只需要数据库中的事务：
1. 假设每条消息都有唯一的 ID，并且在数据库中有一个已处理消息 ID 的表。当你开始从代理处理消息时，你在数据库上开始一个新事务，并检查消息 ID。如果数据库中已经存在相同的消息 ID，你知道它已经被处理，因此你可以向代理确认消息并丢弃它。
2. 如果消息 ID 尚未在数据库中，你将其添加到表中。然后你处理消息，这可能会导致在同一事务中对数据库进行额外的写入。完成处理消息后，你提交数据库上的事务。
3. 一旦数据库事务成功提交，你就可以向代理确认消息。
4. 一旦消息成功确认给代理，你知道它不会再次尝试处理相同的消息，因此你可以从数据库中删除消息 ID（在单独的事务中）。

实现精确一次处理只需要数据库中的事务——跨数据库和消息代理的原子性对于此用例不是必需的。在数据库中记录消息 ID 使消息处理_幂等_，因此可以安全地重试消息处理而不会重复其副作用。

### 总结

| 隔离级别 | 脏读   | 读偏斜  | 幻读   | 丢失更新  | 写偏斜  |
| ---- | ---- | ---- | ---- | ----- | ---- |
| 读未提交 | ✗ 可能 | ✗ 可能 | ✗ 可能 | ✗ 可能  | ✗ 可能 |
| 读已提交 | ✓ 防止 | ✗ 可能 | ✗ 可能 | ✗ 可能  | ✗ 可能 |
| 快照隔离 | ✓ 防止 | ✓ 防止 | ✓ 防止 | ? 视情况 | ✗ 可能 |
| 可串行化 | ✓ 防止 | ✓ 防止 | ✓ 防止 | ✓ 防止  | ✓ 防止 |
相关概念：
- **脏读**：一个客户端在另一个客户端的写入提交之前读取它们。读已提交隔离级别和更强的级别防止脏读。
- **脏写**：一个客户端覆盖另一个客户端已写入但尚未提交的数据。几乎所有事务实现都防止脏写。
- **读偏斜**：客户端在不同时间点看到数据库的不同部分。某些读偏斜的情况也称为_不可重复读_。这个问题最常通过快照隔离来防止，它允许事务从对应于特定时间点的一致快照读取。它通常使用_多版本并发控制_（MVCC）实现。
- **丢失更新**：两个客户端并发执行读-修改-写循环。一个覆盖另一个的写入而不合并其更改，因此数据丢失。某些快照隔离的实现会自动防止此异常，而其他实现需要手动锁（`SELECT FOR UPDATE`）。
- **写偏斜**：事务读取某些内容，根据它看到的值做出决定，并将决定写入数据库。但是，在进行写入时，决策的前提不再为真。只有可串行化隔离才能防止此异常。
- **幻读**：事务读取匹配某些搜索条件的对象。另一个客户端进行影响该搜索结果的写入。快照隔离防止直接的幻读，但写偏斜上下文中的幻读需要特殊处理，例如索引范围锁。

## 9. 分布式系统的麻烦

在一个足够大的系统中，百万分之一的事件每天都在发生。经验丰富的系统操作员会告诉你，任何 _可能_ 出错的事情 _都会_ 出错。
使用分布式系统与在单台计算机上编写软件有着根本的不同 —— 主要区别在于有许多新的、令人兴奋的出错方式。

### 故障与部分失效

一台运行良好软件的单独计算机通常要么完全正常运行，要么完全故障，而不是介于两者之间。

如果发生内部故障，我们宁愿计算机完全崩溃而不是返回错误的结果，因为错误的结果很难处理且令人困惑。因此，计算机隐藏了它们所实现的模糊物理现实，并呈现一个以数学完美运行的理想化系统模型。

在分布式系统中，系统的某些部分可能以某种不可预测的方式出现故障，即使系统的其他部分工作正常。这被称为 _部分失效_。

但在我们实现容错之前，我们需要更多地了解我们应该容忍的故障。重要的是要考虑各种可能的故障。

### 不可靠的网络

本书中关注的分布式系统主要是 _无共享系统_：即通过网络连接的一组机器。网络是这些机器进行通信的唯一方式。

互联网和数据中心中的大多数内部网络（通常是以太网）都是 _异步分组网络_。

一个节点可以向另一个节点发送消息（数据包），但网络不保证它何时到达，或者是否会到达。如果你发送请求并期望响应，许多事情可能会出错：

1. 你的请求可能已经丢失（也许有人拔掉了网线）。
2. 你的请求可能在队列中等待，稍后将被交付（也许网络或接收方过载）。
3. 远程节点可能已经失效（也许它崩溃了或被关闭了）。
4. 远程节点可能暂时停止响应（也许它正在经历长时间的垃圾回收暂停；
![[file-20250825092827902.png]]

发送方甚至无法判断数据包是否已交付：唯一的选择是让接收方发送响应消息，而响应消息本身也可能丢失或延迟。在异步网络中，这些问题是无法区分的。

处理这个问题的常用方法是 **_超时_**：在一段时间后，你放弃等待并假设响应不会到达。然而，当超时发生时，你仍然不知道远程节点是否收到了你的请求。

#### TCP 的局限性

网络数据包有最大大小（通常为几千字节），但许多应用程序需要发送太大而无法装入一个数据包的消息（请求、响应）。这些应用程序最常使用 TCP（传输控制协议）来建立一个 _连接_，将大型数据流分解为单个数据包，并在接收端将它们重新组合起来。

TCP 通常被描述为提供 “可靠” 的交付，从某种意义上说，它检测并重传丢弃的数据包，检测重新排序的数据包并将它们恢复到正确的顺序，并使用简单的校验和检测数据包损坏。它还计算出可以发送数据的速度，以便尽快传输数据，但不会使网络或接收节点过载；这被称为 _拥塞控制_、_流量控制_ 或 _背压_。

- 如果 TCP 提供 “可靠性”，这是否意味着我们不再需要担心网络不可靠？不幸的是不是。如果在某个超时时间内没有收到确认，它会认为数据包一定已经丢失，但 TCP 也无法判断是出站数据包还是确认丢失了。
- 如果 TCP 连接因错误而关闭 —— 也许是因为远程节点崩溃了，或者是因为网络被中断了 —— 你不幸地无法知道远程节点实际处理了多少数据。

#### 网络故障的实践

网络问题可能出人意料地常见，即使在由一家公司运营的受控环境（如数据中心）中也是如此。

处理网络故障不一定意味着 _容忍_ 它们：如果你的网络通常相当可靠，一个有效的方法可能是在网络出现问题时简单地向用户显示错误消息。

#### 检测故障

许多系统需要自动检测故障节点。网络的不确定性使得很难判断节点是否正常工作。

关于远程节点宕机的快速反馈很有用，但你不能指望它。如果出了问题，你可能会在堆栈的某个级别收到错误响应，但通常你必须假设你根本不会收到任何响应。你可以重试几次，等待超时过去，如果在超时内没有收到回复，最终宣布节点死亡。

#### 超时和无界延迟

如果超时是检测故障的唯一可靠方法，那么超时应该多长？不幸的是，没有简单的答案。
长超时意味着在节点被宣布死亡之前需要长时间等待；短超时可以更快地检测故障，但当节点实际上只是遭受暂时的减速时，错误地宣布节点死亡的风险更高。

当节点被宣布死亡时，其职责需要转移到其他节点，这会给其他节点和网络带来额外的负载。如果系统已经在高负载下挣扎，过早地宣布节点死亡可能会使问题变得更糟。

想象一个虚构的系统，其网络保证数据包的最大延迟 —— 每个数据包要么在某个时间 _d_ 内交付，要么丢失，但交付从不会超过 _d_。此外，假设你可以保证未失效的节点总是在某个时间 _r_ 内处理请求。在这种情况下，你可以保证每个成功的请求在时间 2_d_ + _r_ 内收到响应 —— 如果你在该时间内没有收到响应，你就知道网络或远程节点不工作。如果这是真的，2_d_ + _r_ 将是一个合理的超时时间。

使用的大多数系统都没有这些保证：异步网络具有 _无界延迟_。大多数服务器实现无法保证它们可以在某个最大时间内处理请求

##### 网络拥塞和排队

计算机网络上数据包延迟的可变性最常是由于排队。
- 如果几个不同的节点同时尝试向同一目的地发送数据包，网络交换机必须将它们排队并逐个送入目标网络链路，在繁忙的网络链路上，数据包可能需要等待一段时间才能获得一个插槽
- 当数据包到达目标机器时，如果所有 CPU 核心当前都很忙，来自网络的传入请求会被操作系统排队
- 为了避免网络过载，TCP 限制发送数据的速率。这意味着在数据甚至进入网络之前，发送方就有额外的排队。

![[file-20250825095056019.png]]
此外，当 TCP 检测到并自动重传丢失的数据包时，尽管应用程序不会直接看到数据包丢失，但它确实会看到由此产生的延迟（等待超时到期，然后等待重传的数据包被确认）。

在这种环境中，你只能通过实验选择超时：在较长时间内和许多机器上测量网络往返时间的分布，以确定延迟的预期可变性。然后，考虑到你的应用程序的特征，你可以在故障检测延迟和过早超时风险之间确定适当的权衡。

#### 同步与异步网络

为什么我们不能在硬件级别解决这个问题，使网络可靠，这样软件就不需要担心它了？

要回答这个问题，比较数据中心网络与传统的固定电话网络（非蜂窝、非 VoIP）很有趣，后者极其可靠：延迟的音频帧和掉线非常罕见。电话通话需要持续的低端到端延迟和足够的带宽来传输你声音的音频样本。在计算机网络中拥有类似的可靠性和可预测性不是很好吗？

当你通过电话网络拨打电话时，它会建立一个 _电路_：在两个呼叫者之间的整个路线上分配固定、有保证的带宽量。该电路一直保持到通话结束

这种网络是 _同步的_：即使数据通过几个路由器，它也不会遭受排队，因为呼叫的 16 位空间已经在网络的下一跳中预留了。由于没有排队，网络的最大端到端延迟是固定的。我们称之为 _有界延迟_。

##### 我们不能简单地使网络延迟可预测吗？

电话网络中的电路与 TCP 连接非常不同：电路是固定数量的预留带宽，在电路建立期间其他人无法使用，而 TCP 连接的数据包则机会主义地使用任何可用的网络带宽。你可以给 TCP 一个可变大小的数据块（例如，电子邮件或网页），它会尝试在尽可能短的时间内传输它。当 TCP 连接空闲时，它不使用任何带宽。

如果数据中心网络和互联网是电路交换网络，那么在建立电路时就可以建立有保证的最大往返时间。然而，它们不是：以太网和 IP 是分组交换协议，会遭受排队，因此在网络中有无界延迟。这些协议没有电路的概念。

为什么数据中心网络和互联网使用分组交换？答案是它们针对 _突发流量_ 进行了优化。电路适合音频或视频通话，需要在通话期间传输相当恒定的每秒位数。另一方面，请求网页、发送电子邮件或传输文件没有任何特定的带宽要求 —— 我们只希望它尽快完成。

使用电路进行突发数据传输会浪费网络容量并使传输不必要地缓慢。相比之下，TCP 动态调整数据传输速率以适应可用的网络容量。

### 不可靠的时钟

时钟和时间很重要。应用程序以各种方式依赖时钟来回答问题（_持续时间_、_时间点_ ）

在分布式系统中，时间是一件棘手的事情，因为通信不是瞬时的：消息从一台机器通过网络传输到另一台机器需要时间。接收消息的时间总是晚于发送消息的时间，但由于网络中的可变延迟，我们不知道晚了多少。当涉及多台机器时，这个事实有时会使确定事情发生的顺序变得困难。

网络上的每台机器都有自己的时钟，这是一个实际的硬件设备：通常是石英晶体振荡器。这些设备并不完全准确，因此每台机器都有自己的时间概念，可能比其他机器稍快或稍慢。可以在某种程度上同步时钟：最常用的机制是网络时间协议（NTP），它允许根据一组服务器报告的时间调整计算机时钟。

#### 单调时钟与日历时钟

现代计算机至少有两种不同类型的时钟：_日历时钟_ 和 _单调时钟_。尽管它们都测量时间，但区分两者很重要，因为它们服务于不同的目的。

##### 日历时钟

日历时钟做你直观期望时钟做的事情：它根据某个日历返回当前日期和时间（也称为 _墙上时钟时间_）。

日历时钟通常与 NTP 同步，这意味着来自一台机器的时间戳（理想情况下）与另一台机器上的时间戳意思相同。如果本地时钟远远超前于 NTP 服务器，它可能会被强制重置并显示跳回到以前的时间点。这些跳跃，以及闰秒引起的类似跳跃，使日历时钟不适合测量经过的时间。

日历时钟可能会因夏令时（DST）的开始和结束而经历跳跃；这些可以通过始终使用 UTC 作为时区来避免，UTC 没有 DST。

##### 单调时钟

单调时钟适用于测量持续时间（时间间隔），例如超时或服务的响应时间。

你可以在某个时间点检查单调时钟的值，做一些事情，然后在稍后的时间再次检查时钟。两个值之间的 _差值_ 告诉你两次检查之间经过了多少时间 —— 更像秒表而不是挂钟。

如果 NTP 检测到计算机的本地石英晶体比 NTP 服务器运行得更快或更慢，它可能会调整单调时钟前进的频率（这被称为 _调整_ 时钟）。

在分布式系统中，使用单调时钟测量经过的时间（例如，超时）通常是可以的，因为它不假设不同节点的时钟之间有任何同步，并且对测量的轻微不准确不敏感。

#### 时钟同步和准确性

单调时钟不需要同步，但日历时钟需要根据 NTP 服务器或其他外部时间源设置才能有用。硬件时钟和 NTP 可能是反复无常的野兽。
- 硬件时钟和 NTP 可能是反复无常的野兽。
- 如果计算机的时钟与 NTP 服务器相差太多，它可能会拒绝同步，或者本地时钟将被强制重置
- NTP 同步只能与网络延迟一样好，因此当你在具有可变数据包延迟的拥塞网络上时，其准确性有限。
- 闰秒导致一分钟有 59 秒或 61 秒长，这会搞乱在设计时没有考虑闰秒的系统中的时序假设

如果你足够关心时钟精度并愿意投入大量资源，就可以实现非常好的时钟精度。例如，欧洲金融机构的 MiFID II 法规要求所有高频交易基金将其时钟同步到 UTC 的 100 微秒以内，以帮助调试市场异常（如 “闪崩”）并帮助检测市场操纵。这种精度可以通过一些特殊硬件（GPS 接收器和/或原子钟）、精确时间协议（PTP）以及仔细的部署和监控来实现。

#### 对同步时钟的依赖

尽管它们大部分时间工作得很好，但强健的软件需要准备好处理不正确的时钟。
- 一部分是不正确的时钟很容易被忽视。如果机器的 CPU 有缺陷或其网络配置错误，它很可能根本无法工作，因此会很快被注意到并修复。
- 另一方面，如果它的石英时钟有缺陷或其 NTP 客户端配置错误，大多数事情看起来会正常工作，即使它的时钟逐渐偏离现实越来越远。如果某些软件依赖于准确同步的时钟，结果更可能是静默和微妙的数据丢失，而不是戏剧性的崩溃

如果你使用需要同步时钟的软件，你还必须仔细监控所有机器之间的时钟偏移。任何时钟偏离其他节点太远的节点都应该被宣布死亡并从集群中移除。这种监控确保你在损坏的时钟造成太多损害之前注意到它们。

##### 用于事件排序的时间戳

在具有多主复制的数据库中日历时钟的危险使用，户端 A 在节点 1 上写入 _x_ = 1；写入被复制到节点 3；客户端 B 在节点 3 上递增 _x_（我们现在有 _x_ = 2）；最后，两个写入都被复制到节点 2。

![[file-20250825150132509.png]]

当写入被复制到其他节点时，它会根据写入起源节点上的日历时钟标记时间戳。此示例中的时钟同步非常好：节点 1 和节点 3 之间的偏差小于 3 毫秒，这可能比你在实践中可以期望的要好。

_最后写入胜利_（LWW）可以通过确保当值被覆盖时，新值总是具有比被覆盖值更高的时间戳来防止这个问题，即使该时间戳超前于写入者的本地时钟。然而，这会产生额外的读取成本来查找最大的现有时间戳。

即使通过保留最 “新” 的值并丢弃其他值来解决冲突很诱人，但重要的是要意识到 “新” 的定义取决于本地日历时钟，它很可能是不正确的。即使使用紧密 NTP 同步的时钟，你也可能在时间戳 100 毫秒（根据发送者的时钟）发送数据包，并让它在时间戳 99 毫秒（根据接收者的时钟）到达 —— 因此看起来数据包在发送之前就到达了，这是不可能的。

所谓的 **_逻辑时钟_** ，基于递增计数器而不是振荡石英晶体，是排序事件的更安全替代方案。逻辑时钟不测量一天中的时间或经过的秒数，只测量事件的相对顺序（一个事件是在另一个事件之前还是之后发生）。相比之下，日历时钟和单调时钟测量实际经过的时间，也称为 _物理时钟_。

##### 带置信区间的时钟读数

将时钟读数视为时间点是没有意义的 —— 它更像是一个时间范围，在置信区间内：例如，系统可能有 95% 的信心认为现在的时间在分钟后的 10.3 到 10.5 秒之间，但它不知道比这更精确的时间。

不确定性边界可以根据你的时间源计算。如果你有直接连接到计算机的 GPS 接收器或原子钟，预期误差范围由设备决定，对于 GPS，由来自卫星的信号质量决定。如果你从服务器获取时间，不确定性基于自上次与服务器同步以来的预期石英漂移，加上 NTP 服务器的不确定性，加上到服务器的网络往返时间

大多数系统不暴露这种不确定性
##### 用于全局快照的同步时钟

_多版本并发控制_（MVCC），这是数据库中非常有用的功能，需要支持小型、快速的读写事务和大型、长时间运行的只读事务（例如，用于备份或分析）。它允许只读事务看到数据库的 _快照_，即特定时间点的一致状态，而不会锁定和干扰读写事务。

MVCC 需要单调递增的事务 ID。如果写入发生在快照之后（即，写入的事务 ID 大于快照），则该写入对快照事务不可见。在单节点数据库上，简单的计数器就足以生成事务 ID。

当数据库分布在许多机器上，可能在多个数据中心时，全局单调递增的事务 ID（跨所有分片）很难生成，因为它需要协调。事务 ID 必须反映因果关系：如果事务 B 读取或覆盖先前由事务 A 写入的值，则 B 必须具有比 A 更高的事务 ID —— 否则，快照将不一致。对于大量小型、快速的事务，在**分布式系统中创建事务 ID** 成为难以承受的瓶颈。

Spanner同步日历时钟的时间戳作为事务 ID实现快照隔离，使用 TrueTime API 报告的时钟置信区间，并基于以下观察：如果你有两个置信区间，每个都由最早和最晚可能的时间戳组成（_A_ = [_A最早_, _A最晚_] 和 _B_ = [_B最早_, _B最晚_]），并且这两个区间不重叠（即，_A最早_ < _A最晚_ < _B最早_ < _B最晚_），那么 B 肯定发生在 A 之后 —— 毫无疑问。

#### 进程暂停

假设你有一个每个分片都有单个主节点的数据库。只有主节点被允许接受写入。节点如何知道它仍然是主节点（它没有被其他节点宣布死亡），并且它可以安全地接受写入？

让主节点从其他节点获取 _租约_，这类似于带有超时的锁。任何时候只有一个节点可以持有租约，存在问题：
- 租约的到期时间由不同的机器设置
- 想象线程在 `lease.isValid()` 行周围停止了 15 秒，然后才最终继续。在这种情况下，处理请求时租约很可能已经到期，另一个节点已经接管了主节点身份。（有各种原因）

在单台机器上编写多线程代码时，我们有相当好的工具来使其线程安全：互斥锁、信号量、原子计数器、无锁数据结构、阻塞队列等。不幸的是，这些工具不能直接转换到分布式系统，因为分布式系统没有共享内存 —— 只有通过不可靠网络发送的消息。

##### 响应时间保证

线程和进程可能会暂停无限长的时间。如果你足够努力，这些暂停的原因 _可以_ 被消除。
在这些系统中，有一个指定的 **_截止时间_**，软件必须在此之前响应；如果它没有达到截止时间，可能会导致整个系统的故障。这些被称为 **_硬实时_ 系统**。

对于大多数服务器端数据处理系统，实时保证根本不经济或不合适。因此，这些系统必须承受在非实时环境中运行带来的暂停和时钟不稳定性。

##### 限制垃圾回收的影响

垃圾回收曾经是进程暂停的最大原因之一。
GC 算法已经改进了很多：经过适当调整的回收器现在通常只会暂停几毫秒。Java 运行时提供了并发标记清除（CMS）、G1、Z 垃圾回收器（ZGC）、Epsilon 和 Shenandoah 等回收器。

如果你需要完全避免 GC 暂停，一个选择是使用根本没有垃圾回收器的语言。例如，Swift 使用自动引用计数来确定何时可以释放内存；Rust 和 Mojo 使用类型系统跟踪对象的生命周期，以便编译器可以确定必须分配内存多长时间。

### 知识、真相和谎言

在分布式系统中，我们可以陈述我们对行为（_系统模型_）的假设，并以这样的方式设计实际系统，使其满足这些假设。算法可以被证明在某个系统模型内正确运行。这意味着即使底层系统模型提供的保证很少，也可以实现可靠的行为。

#### 多数派原则

节点不一定能信任自己对情况的判断。分布式系统不能完全依赖单个节点，因为节点可能随时失效，可能使系统陷入困境并无法恢复。相反，许多分布式算法依赖于 _仲裁_，即节点之间的投票：决策需要来自几个节点的最少票数，以减少对任何一个特定节点的依赖。

最常见的是，仲裁是**超过半数节点的绝对多数**（尽管其他类型的仲裁也是可能的）。多数仲裁允许系统在少数节点故障时继续工作（三个节点可以容忍一个故障节点；五个节点可以容忍两个故障节点）

#### 分布式锁和租约

租约是一种超时的锁，如果旧所有者停止响应（可能是因为它崩溃了、暂停太久或与网络断开连接），可以分配给新所有者。你可以在系统需要只有一个某种东西的情况下使用租约。

由于锁的错误实现导致的数据损坏错误：
问题1：假设你想确保存储服务中的文件一次只能由一个客户端访问，因为如果多个客户端试图写入它，文件将被损坏。你尝试通过要求客户端在访问文件之前从锁服务获取租约来实现这一点。这种锁服务通常使用共识算法实现：
![[file-20250825184540617.png]]

如果持有租约的客户端暂停太久，其租约就会过期。另一个客户端可以获得同一文件的租约，并开始写入文件。当暂停的客户端回来时，它（错误地）认为它仍然有有效的租约，并继续写入文件。我们现在有了脑裂情况：客户端的写入冲突并损坏了文件。

问题2：就在客户端 1 崩溃之前，它向存储服务发送了一个写请求，但这个请求在网络中被延迟了很长时间。当写请求到达存储服务时，租约已经超时，允许客户端 2 获取它并发出自己的写入。

![[file-20250825184716717.png]]

##### 隔离僵尸进程和延迟请求

术语 _僵尸_ 有时用于描述尚未发现失去租约的前租约持有者，并且仍在充当当前租约持有者。由于我们不能完全排除僵尸，我们必须确保它们不能以脑裂的形式造成任何损害。这被称为 **_隔离_ 僵尸**。

![[file-20250826200914840.png]]假设每次锁服务授予锁或租约时，它还返回一个 **_隔离令牌_**，这是一个每次授予锁时都会增加的数字（例如，由锁服务递增）。然后我们可以要求客户端每次向存储服务发送写请求时，都必须包含其当前的隔离令牌。

如果 ZooKeeper 是你的锁服务，你可以使用事务 ID `zxid` 或节点版本 `cversion` 作为隔离令牌。

这种机制要求存储服务有某种方法来检查写入是否基于过时的令牌。或者，服务支持仅在对象自当前客户端上次读取以来未被另一个客户端写入时才成功的写入就足够了，类似于原子比较并设置（CAS）操作。

##### 多副本隔离

如果你的客户端只需要写入一个支持此类条件写入的存储服务，锁服务在某种程度上是多余。一旦你有了隔离令牌，你也可以将其用于多个服务或副本，并确保旧的租约持有者在所有这些服务上都被隔离。

一旦你有了隔离令牌，你也可以将其用于多个服务或副本，并确保旧的租约持有者在所有这些服务上都被隔离。

![[file-20250826201718939.png]]

可以使用隔离令牌来防止僵尸和延迟请求造成任何损害。

#### 拜占庭故障

隔离令牌可以检测并阻止 _无意中_ 出错的节点（例如，因为它尚未发现其租约已过期）。然而，如果节点故意想要破坏系统的保证，它可以通过发送带有虚假隔离令牌的消息轻松做到。

假设节点是不可靠但诚实的：它们可能很慢或从不响应（由于故障），它们的状态可能已过时（由于 GC 暂停或网络延迟），但我们假设如果节点 _确实_ 响应，它就是在说 “真话”：据它所知，它正在按协议规则行事。

如果节点可能 “撒谎”（发送任意错误或损坏的响应）的风险存在，分布式系统问题会变得更加困难 —— 例如，它可能在同一次选举中投出多个相互矛盾的票。这种行为被称为 **_拜占庭故障_**，在这种不信任环境中达成共识的问题被称为 **_拜占庭将军问题_** 

拜占庭是一个古希腊城市，后来成为君士坦丁堡，位于现在土耳其的伊斯坦布尔。没有任何历史证据表明拜占庭的将军比其他地方的将军更容易搞阴谋和密谋。相反，这个名字源自 _拜占庭_ 一词在 **_过于复杂、官僚、狡猾_** 的意义上的使用，这个词在计算机出现之前很久就在政治中使用了

如果即使某些节点发生故障并且不遵守协议，或者恶意攻击者干扰网络，系统仍能继续正确运行，则该系统是 **_拜占庭容错_** 的。（航天环境、比特币）

在大多数服务器端数据系统中，部署拜占庭容错解决方案的成本使它们不切实际。

Web 应用程序确实需要预期客户端在最终用户控制下的任意和恶意行为，例如 Web 浏览器。这就是输入验证、清理和输出转义如此重要的原因：例如，防止 SQL 注入和跨站脚本攻击。

##### 弱形式的谎言

尽管我们假设节点通常是诚实的，但向软件添加防范弱形式 “谎言” 的机制可能是值得的。由于硬件问题、软件错误和配置错误导致的无效消息。

#### 系统模型与现实

通过定义 _系统模型_ 描述算法可能假设什么事情的抽象。

时序假设，三种系统模型常用：
- **同步模型**：同步模型假设有界的网络延迟、有界的进程暂停和有界的时钟误差。这并不意味着精确同步的时钟或零网络延迟；它只是意味着你知道网络延迟、暂停和时钟漂移永远不会超过某个固定的上限
- **部分同步模型**：部分同步意味着系统 _大部分时间_ 表现得像同步系统，但有时会超过网络延迟、进程暂停和时钟漂移的界限
- **异步模型**：在这个模型中，算法不允许做出任何时序假设 —— 事实上，它甚至没有时钟（因此它不能使用超时）。

节点的一些常见系统模型是：
- **崩溃停止故障**：在 _崩溃停止_（或 _故障停止_）模型中，算法可以假设节点只能以一种方式失效，即崩溃。
- **崩溃恢复故障**：我们假设节点可能在任何时刻崩溃，并且可能在某个未知时间后再次开始响应。
- **性能下降和部分功能**：它们可能仍然能够响应健康检查请求，但速度太慢而无法完成任何实际工作。
- **拜占庭（任意）故障**：节点可能做任何事情，包括试图欺骗和欺骗其他节点。

##### 定义算法的正确性

为了定义算法 _正确_ 的含义，我们可以描述它的 _属性_。例如，排序算法的输出具有这样的属性：对于输出列表的任何两个不同元素，左边的元素小于右边的元素。这只是定义列表排序含义的正式方式。

分布式算法具有的属性，以定义正确的含义。例如，如果我们为锁生成隔离令牌，我们可能要求算法具有以下属性：

- 唯一性：没有两个隔离令牌请求返回相同的值。
- 单调序列：如果请求 _x_ 返回令牌 _t**x_，请求 _y_ 返回令牌 _t**y_，并且 _x_ 在 _y_ 开始之前完成，则 _t**x_ < _t**y_。
- 可用性：请求隔离令牌且不崩溃的节点最终会收到响应。
##### 安全性与活性

值得区分两种不同类型的属性：_安全性_ 和 _活性_ 属性。在刚才给出的例子中，_唯一性_ 和 _单调序列_ 是安全属性，但 _可用性_ 是活性属性。

个迹象是活性属性通常在其定义中包含 “最终” 一词。

安全性通常被非正式地定义为 _没有坏事发生_，活性被定义为 _好事最终会发生_。

区分安全性和活性属性的一个优点是它有助于我们处理困难的系统模型。

##### 将系统模型映射到现实世界

安全性和活性属性以及系统模型对于推理分布式算法的正确性非常有用。然而，在实践中实现算法时，现实的混乱事实又会回来咬你一口，很明显系统模型是现实的简化抽象。

仲裁算法依赖于节点记住它声称已存储的数据。如果节点可能患有健忘症并忘记先前存储的数据，那会破坏仲裁条件，从而破坏算法的正确性。也许需要一个新的系统模型，其中我们假设稳定存储大多在崩溃后幸存，但有时可能会丢失。但该模型随后变得更难推理。

算法的理论描述可以声明某些事情被简单地假设不会发生 —— 在非拜占庭系统中，我们确实必须对可能和不可能发生的故障做出一些假设。然而，真正的实现可能仍然必须包含代码来处理被假设为不可能的事情发生的情况，即使该处理归结为 `printf("Sucks to be you")` 和 `exit(666)` —— 即，让人类操作员清理烂摊子。（这是**计算机科学和软件工程之间的一个区别**。）

#### 形式化方法和随机测试

一种方法是通过数学描述算法来形式验证它，并使用证明技术来表明它在系统模型允许的所有情况下都满足所需的属性。

将理论分析与经验测试相结合以验证实现按预期运行是明智的。基于属性的测试、模糊测试和确定性模拟测试（DST）等技术使用随机化来在各种情况下测试系统。

##### 模型检查与规范语言

_模型检查器_ 是帮助验证算法或系统按预期运行的工具。模型检查器使用这些模型通过系统地尝试所有可能发生的事情来验证不变量在算法的所有状态中都成立。

模型检查器在易用性和查找非显而易见错误的能力之间取得了很好的平衡。CockroachDB、TiDB、Kafka 和许多其他分布式系统使用模型规范来查找和修复错误。

##### 故障注入

故障注入是一种有效（有时令人恐惧）的技术，用于验证系统的实现在出错时是否按预期工作。

故障注入测试通常在与系统将运行的生产环境非常相似的环境中运行。有些甚至直接将故障注入到他们的生产环境中。Netflix 通过他们的 Chaos Monkey 工具推广了这种方法 。生产故障注入通常被称为 **_混沌工程_**。

##### 确定性模拟测试

确定性模拟测试（DST）也已成为模型检查和故障注入的流行补充。它使用与模型检查器类似的状态空间探索过程，但它测试你的实际代码，而不是模型。

在 DST 中，模拟自动运行系统的大量随机执行。模拟期间的网络通信、I/O 和时钟时序都被模拟替换，允许模拟器控制事情发生的确切顺序，包括各种时序和故障场景。

### 确定性的力量

非确定性是我们在本章中讨论的所有分布式系统挑战的核心：并发性、网络延迟、进程暂停、时钟跳跃和崩溃都以不可预测的方式发生，从系统的一次运行到下一次运行都不同。相反，如果你能使系统确定性，那可以极大地简化事情。

使事物确定性是一个简单但强大的想法，在分布式系统设计中一再出现。除了确定性模拟测试，我们在过去的章节中已经看到了几种使用确定性的方法：
- 事件溯源
- 工作流引擎
- 状态机复制

使代码完全确定性需要小心。即使你已经删除了所有并发性并用确定性模拟替换了 I/O、网络通信、时钟和随机数生成器，非确定性元素可能仍然存在。
## 10. 一致性与共识

最佳容错工具之一是 _复制_。
在多个副本上拥有多份数据副本会带来不一致的风险。处理这些问题有两种相互竞争的理念：
- **最终一致性**：系统被复制这一事实对应用程序是可见的，作为应用程序开发者，你需要处理可能出现的不一致和冲突。
- **强一致性**：为应用程序不应该担心复制的内部细节，系统应该表现得就像单节点一样。这种方法的优点是对你（应用程序开发者）来说更简单。缺点是更强的一致性会带来性能成本。

将深入探讨强一致性方法，关注三个领域：
1.  一个挑战是"强一致性"相当模糊，因此我们将制定一个更精确的定义，明确我们想要实现什么：_线性一致性_。
2. 研究生成 ID 和时间戳的问题。这可能听起来与一致性无关，但实际上密切相关。
3. 探讨分布式系统如何在保持容错的同时实现线性一致性；答案是 _共识_ 算法。

### 线性一致性

**线性一致性**：希望复制的数据库尽可能简单易用，你应该让它表现得就像根本没有复制一样。然后用户就不必担心复制延迟、冲突和其他不一致性。（也称为 _原子一致性_ 、_强一致性_、_即时一致性_ 或 _外部一致性_ ）

在线性一致系统中，一旦一个客户端成功完成写入，所有从数据库读取的客户端都必须能够看到刚刚写入的值。

非线性一致系统的例子：

![[file-20250830133919026.png]]
#### 什么使系统具有线性一致性？

显示了三个客户端在线性一致数据库中并发读取和写入同一个对象 _x_。在分布式系统理论中，_x_ 被称为 _寄存器_——在实践中，它可能是键值存储中的一个键，关系数据库中的一行，或者文档数据库中的一个文档，例如。

![[file-20250830134013368.png]]在这个例子中，寄存器有两种类型的操作：
- _read_(_x_) ⇒ _v_ 表示客户端请求读取寄存器 _x_ 的值，数据库返回值 _v_。
- _write_(_x_, _v_) ⇒ _r_ 表示客户端请求将寄存器 _x_ 设置为值 _v_，数据库返回响应 _r_（可能是 _ok_ 或 _error_）。

Alice 观察到 x = 0 且 y = 1，而 Bob 观察到 x = 1 且 y = 0。就好像 Alice 和 Bob 的计算机对写入发生的顺序意见不一。
如果与写入并发的读取可以返回旧值或新值，那么读者可能会在写入进行时多次看到值在旧值和新值之间来回翻转。

为了使系统线性一致，我们需要添加另一个约束：

![[file-20250830134423629.png]]如果 Alice 和 Bob 有完美的时钟，线性一致性将要求返回 x = 1，因为 x 的读取在写入 x = 1 完成后开始。

在线性一致系统中，我们想象必须有某个时间点（在写入操作的开始和结束之间），_x_ 的值从 0 原子地翻转到 1。因此，如果一个客户端的读取返回新值 1，所有后续读取也必须返回新值，即使写入操作尚未完成。

可以进一步细化这个时序图，以可视化每个操作在某个时间点原子地生效。在这个例子中，除了 _read_ 和 _write_ 之外，我们添加了第三种操作类型：

- _cas_(_x_, _v_old, _v_new) ⇒_r_  表示客户端请求一个原子 _比较并设置_ 操作。如果寄存器 _x_ 的当前值等于 _v_old，它应该原子地设置为 _v_new。如果 _x_  的值与 _v_ old 不同，则操作应该保持寄存器不变并返回错误。

![[file-20250830134855061.png]]

线性一致性的要求是连接操作标记的线始终向前移动（从左到右），永不后退。这个要求确保了我们之前讨论的新鲜度保证：一旦写入或读取了新值，所有后续读取都会看到写入的值，直到它再次被覆盖。

这就是线性一致性背后的直觉；形式化定义 更精确地描述了它。可以（尽管计算成本高昂）通过记录所有请求和响应的时序，并检查它们是否可以排列成有效的顺序序列来测试系统的行为是否线性一致。

就像除了可串行化之外还有各种弱隔离级别用于事务，除了线性一致性之外，复制系统也有各种较弱的一致性模型

 _写后读_、_单调读_ 和 _一致性前缀读_ 属性就是这种较弱一致性模型的例子。线性一致性保证所有这些较弱的属性，以及更多。在本章中，我们将重点关注线性一致性，它是最常用的最强一致性模型。

线性一致性很容易与可串行化混淆，因为这两个词似乎都意味着类似"可以按顺序排列"的东西。然而，它们是完全不同的保证，区分它们很重要：
- **可串行化**：可串行化是事务的隔离属性，其中每个事务可能读取和写入 _多个对象_（行、文档、记录）。它保证事务的行为与它们按 _某种_ 串行顺序执行时相同：也就是说，就好像你首先执行一个事务的所有操作，然后执行另一个事务的所有操作，依此类推，而不交错它们。该串行顺序可以与事务实际运行的顺序不同。
- **线性一致性**：一致性是对寄存器（_单个对象_）的读写保证。它不将操作分组到事务中，因此它不能防止涉及多个对象的问题，如写偏差。线性一致性是一个 _新鲜度_ 保证：它要求如果一个操作在另一个操作开始之前完成，那么后一个操作必须观察到至少与前一个操作一样新的状态。可串行化没有这个要求：例如，可串行化允许过时读取。

数据库可能同时提供可串行化和线性一致性，这种组合称为 _严格可串行化_ 或 _强单副本可串行化_（_strong-1SR_），单节点数据库通常是线性一致的。对于使用乐观方法（如可串行化快照隔离）的分布式数据库。

也可以将较弱的隔离级别与线性一致性结合，或将较弱的一致性模型与可串行化结合；实际上，一致性模型和隔离级别可以在很大程度上相互独立地选择。

#### 依赖线性一致性

有几个领域中线性一致性是使系统正确工作的重要要求。
##### 锁定与领导者选举

使用单主复制的系统需要确保确实只有一个主节点，而不是多个（脑裂）。选举领导者的一种方法是使用租约：每个启动的节点都尝试获取租约，成功的节点成为领导者。无论这种机制如何实现，它都必须是线性一致的：两个不同的节点不应该能够同时获取租约。

像 Apache ZooKeeper 和 etcd 这样的协调服务通常用于实现分布式租约和领导者选举。它们使用共识算法以容错的方式实现线性一致的操作（我们将在本章后面讨论这些算法）。

分布式锁也在一些分布式数据库中以更细粒度的级别使用，例如 Oracle Real Application Clusters (RAC)。RAC 对每个磁盘页使用一个锁，多个节点共享对同一磁盘存储系统的访问。由于这些线性一致的锁位于事务执行的关键路径上，RAC 部署通常具有专用的集群互连网络用于数据库节点之间的通信。

##### 约束与唯一性保证

唯一性约束在数据库中很常见：例如，用户名或电子邮件地址必须唯一标识一个用户，在文件存储服务中不能有两个具有相同路径和文件名的文件。如果你想在数据写入时强制执行此约束（这样如果两个人同时尝试创建具有相同名称的用户或文件，其中一个将返回错误），你需要线性一致性。

硬唯一性约束，例如你通常在关系数据库中找到的约束，需要线性一致性。其他类型的约束，例如外键或属性约束，可以在没有线性一致性的情况下实现

##### 跨通道时序依赖

如果 Aaliyah 没有大声说出比分，Bryce 就不会知道他的查询结果是过时的。他只会在几秒钟后再次刷新页面，最终看到最终比分。线性一致性违规之所以被注意到，只是因为系统中有一个额外的通信通道（Aaliyah 的声音到 Bryce 的耳朵）。


假设你有一个网站，用户可以上传视频，后台进程将视频转码为较低质量，以便在慢速互联网连接上流式传输。
![[file-20250830141028894.png]]

如果文件存储服务是线性一致的，那么这个系统应该工作正常。如果它不是线性一致的，就存在竞态条件的风险：消息队列可能比存储服务内部的复制更快。在这种情况下，当转码器获取原始视频时，它可能会看到文件的旧版本，或者根本看不到任何内容。如果它处理视频的旧版本，文件存储中的原始视频和转码视频将永久不一致。

这个问题的出现是因为 Web 服务器和转码器之间有两个不同的通信通道：文件存储和消息队列。如果没有线性一致性的新鲜度保证，这两个通道之间可能存在竞态条件。

#### 实现线性一致性系统

由于线性一致性本质上意味着"表现得好像只有一份数据副本，并且对它的所有操作都是原子的"，最简单的答案是真的只使用一份数据副本。然而，这种方法将无法容忍故障：如果持有该副本的节点失败，数据将丢失，或者至少在节点重新启动之前无法访问。

审视复制方法，并比较它们是否可以实现线性一致：

- **单主复制**（可能线性一致）：单主复制系统中，主节点拥有用于写入的数据主副本，从节点在其他节点上维护数据的备份副本。只要你在主节点上执行所有读写操作，它们很可能是线性一致的。
- **共识算法**（可能线性一致）：一些共识算法本质上是带有自动领导者选举和故障切换的单主复制。它们经过精心设计以防止脑裂，使它们能够安全地实现线性一致的存储。ZooKeeper 使用 Zab 共识算法。仅仅因为系统使用共识并不能保证其上的所有操作都是线性一致的：如果它允许在不检查节点是否仍然是领导者的情况下在节点上读取，读取的结果可能是过时的，如果刚刚选出了新的领导者。
- **多主复制**（非线性一致）：具有多主复制的系统通常不是线性一致的，因为它们在多个节点上并发处理写入，并将它们异步复制到其他节点。
- **无主复制**（可能非线性一致）：可以通过要求仲裁读写（_w_ + _r_ > _n_）来获得"强一致性"。根据确切的算法，以及你如何定义强一致性，这并不完全正确。

##### 线性一致性与仲裁

直观地说，在 Dynamo 风格的模型中，仲裁读写似乎应该是线性一致的。然而，当我们有可变的网络延迟时，可能会出现竞态条件：

![[file-20250830141941650.png]]

可以使 Dynamo 风格的仲裁线性一致，但代价是降低性能：读者必须同步执行读修复，然后才能将结果返回给应用程序。此外，在写入之前，写入者必须读取节点仲裁的最新状态以获取任何先前写入的最新时间戳，并确保新写入具有更大的时间戳。然而，Riak 由于性能损失而不执行同步读修复。Cassandra 确实等待仲裁读取时的读修复完成 ，但由于它使用日历时钟作为时间戳而失去了线性一致性。

#### 线性一致性的代价

我们看到多主复制通常是多区域复制的良好选择:
![[file-20250830142532698.png]]
使用多主数据库，每个区域可以继续正常运行：由于来自一个区域的写入被异步复制到另一个区域，写入只是排队并在网络连接恢复时交换。

如果在单主设置中区域之间的网络中断，连接到从节点区域的客户端无法联系主节点，因此它们既不能对数据库进行任何写入，也不能进行任何线性一致的读取。它们仍然可以从从节点读取，但它们可能是过时的（非线性一致）。如果应用程序需要线性一致的读写，网络中断会导致应用程序在无法联系主节点的区域中变得不可用。

##### CAP 定理

- 如果你的应用程序 _需要_ 线性一致性，并且某些副本由于网络问题与其他副本断开连接，那么某些副本在断开连接时无法处理请求：它们必须等待网络问题修复，或者返回错误（无论哪种方式，它们都变得 _不可用_）。这种选择有时被称为 _CP_（在网络分区下一致）。
- 如果你的应用程序 _不需要_ 线性一致性，那么它可以以一种方式编写，使每个副本可以独立处理请求，即使它与其他副本断开连接（例如，多主）。在这种情况下，应用程序可以在面对网络问题时保持 _可用_，但其行为不是线性一致的。这种选择被称为 _AP_（在网络分区下可用）。

CAP 最初是作为经验法则提出的，没有精确的定义，目的是开始关于数据库中权衡的讨论。当时，许多分布式数据库专注于在具有共享存储的机器集群上提供线性一致语义，CAP 鼓励数据库工程师探索更广泛的分布式无共享系统设计空间。CAP 在这种文化转变方面值得称赞——它帮助触发了 NoSQL 运动，这是 2000 年代中期左右的一系列新数据库技术。

CAP 有时被表述为 _一致性、可用性、分区容错性：从 3 个中选择 2 个_。不幸的是，这样表述是误导性的，因为**网络分区是一种故障**，所以它们不是你可以选择的：无论你喜欢与否，它们都会发生。

当网络正常工作时，系统可以同时提供一致性（线性一致性）和完全可用性。当发生网络故障时，你必须在线性一致性或完全可用性之间进行选择。因此，CAP 的更好表述方式是 **_分区时要么一致要么可用_** 。更可靠的网络需要更少地做出这种选择，但在某个时候这种选择是不可避免的。

CP/AP 分类方案还有几个进一步的缺陷。_一致性_ 被形式化为线性一致性（定理没有说任何关于较弱一致性模型的内容），_可用性_ 的形式化 与该术语的通常含义不匹配。

总的来说，关于 CAP 有很多误解和混淆，它并不能帮助我们更好地理解系统，因此最好避免使用 CAP。

正式定义的 CAP 定理 范围非常狭窄：它只考虑一种一致性模型（即线性一致性）和一种故障（网络分区）。它没有说任何关于网络延迟、死节点或其他权衡的内容。因此，尽管 CAP 在历史上具有影响力，但对于设计系统几乎没有实际价值。

CAP 现在已被更精确的结果所取代，因此它今天主要具有历史意义。

##### 线性一致性与网络延迟

尽管线性一致性是一个有用的保证，但令人惊讶的是，实际上很少有系统是线性一致的。例如，即使现代多核 CPU 上的 RAM 也不是线性一致的。

许多选择不提供线性一致保证的分布式数据库也是如此：它们这样做主要是为了提高性能，而不是为了容错。线性一致性很慢——这在任何时候都是真的，不仅在网络故障期间。

能否找到更高效的线性一致存储实现？答案似乎是否定的。

### ID 生成器和逻辑时钟

在许多应用程序中，你需要在创建数据库记录时为它们分配某种唯一的 ID，这给了你一个可以引用这些记录的主键。在单节点数据库中，通常使用自增整数，它的优点是只需要 64 位（如果你确定永远不会有超过 40 亿条记录，甚至可以使用 32 位，但这是有风险的）来存储。

![[file-20250830151554948.png]]

这个单节点 ID 生成器是线性一致系统的另一个例子。每个获取 ID 的请求都是一个原子地递增计数器并返回旧计数器值的操作（_获取并增加_ 操作）；线性一致性不指定它们的 ID 必须如何排序，只要它们是唯一的。

内存中的单节点 ID 生成器很容易实现：你可以使用 CPU 提供的原子递增指令，它允许多个线程安全地递增同一个计数器。使计数器持久化需要更多的努力，这样节点就可以崩溃并重新启动而不重置计数器值，这将导致重复的 ID。但真正的问题是：

- 单节点 ID 生成器不具容错性，因为该节点是单点故障。
- 如果你想在另一个区域创建记录，速度会很慢，因为你可能必须往返地球的另一端才能获得 ID。
- 如果你有高写入吞吐量，该单个节点可能成为瓶颈。

你可以考虑各种 ID 生成器的替代选项：
- **分片 ID 分配**：可以有多个分配 ID 的节点——例如，一个只生成偶数，一个只生成奇数。一般来说，你可以在 ID 中保留一些位来包含分片编号。这些 ID 仍然紧凑，但你失去了排序属性
- **预分配 ID 块**：不是从单节点 ID 生成器请求单个 ID，它可以分发 ID 块。例如，节点 A 可能声明从 1 到 1,000 的 ID 块，节点 B 可能声明从 1,001 到 2,000 的块。然后每个节点可以独立地从其块中分发 ID，并在其序列号供应开始不足时从单节点 ID 生成器请求新块。但是，这种方案也不能确保正确的排序。
- **随机 UUID**：使用 _通用唯一标识符_（UUID），也称为 _全局唯一标识符_（GUID）。它们的一大优点是可以在任何节点上本地生成，无需通信，但它们需要更多空间（128 位）。有几种不同版本的 UUID；最简单的是版本 4，它本质上是一个如此长的随机数，以至于两个节点选择相同的可能性非常小。不幸的是，这些 ID 的顺序也是随机的
- **时钟时间戳使其唯一**：如果你的节点的日历时钟使用 NTP 保持大致正确，你可以通过将该时钟的时间戳放在最高有效位中，并用确保 ID 唯一的额外信息填充剩余位来生成 ID，即使时间戳不是——例如，分片编号和每分片递增序列号，或长随机值。

时钟时间戳最多只能提供近似排序：如果较早的写入从稍快的时钟获得时间戳，而较晚写入的时间戳来自稍慢的时钟，则时间戳顺序可能与事件实际发生的顺序不一致。由于使用非单调时钟而导致的时钟跳跃，即使单个节点生成的时间戳也可能排序错误。因此，基于时钟时间的 ID 生成器不太可能是线性一致的。

你可以通过依赖高精度时钟同步，使用原子钟或 GPS 接收器来减少这种排序不一致。但如果能够在不依赖特殊硬件的情况下生成唯一且正确排序的 ID 也会很好。这就是 _逻辑时钟_ 的用途。

#### 逻辑时钟

日历时钟和单调时钟。这两种都是 _物理时钟_：它们测量经过的秒数（或毫秒、微秒等）。

在分布式系统中，通常还使用另一种时钟，称为 _逻辑时钟_。物理时钟是计算已经过的秒数的硬件设备，而逻辑时钟是计算已发生事件的算法。来自逻辑时钟的时间戳因此不会告诉你现在几点，但你 _可以_ **比较来自逻辑时钟的两个时间戳**，以判断哪个更早，哪个更晚。

逻辑时钟的要求通常是：
- 其时间戳紧凑（大小为几个字节）且唯一；
- 可以比较任意两个时间戳（即它们是 _全序_ 的）
- 时间戳的顺序与因果关系 _一致_

单节点 ID 生成器满足这些要求，但我们刚刚讨论的分布式 ID 生成器不满足因果排序要求。

##### Lamport 时间戳

有一种生成逻辑时间戳的简单方法，它与因果关系 _一致_，你可以将其用作分布式 ID 生成器。它被称为 _Lamport 时钟_，由 Leslie Lamport 在 1978 年提出。

每个节点都有一个唯一标识符，在实践中可能是随机 UUID 或类似的东西。此外，每个节点都保留它已处理的操作数的计数器。Lamport 时间戳就是一对（_计数器_，_节点 ID_）。两个节点有时可能具有相同的计数器值，但通过在时间戳中包含节点 ID，每个时间戳都是唯一的。
![[file-20250830152220797.png]]
每次节点生成时间戳时，它都会递增其计数器值并使用新值。此外，每次节点看到来自另一个节点的时间戳时，如果该时间戳中的计数器值大于其本地计数器值，它会将其本地计数器增加到与时间戳中的值匹配。

要比较两个 Lamport 时间戳，我们首先比较它们的计数器值：例如，(2, “Bryce”) 大于 (1, “Aaliyah”)，也大于 (1, “Caleb”)。如果两个时间戳具有相同的计数器，我们改为比较它们的节点 ID，使用通常的字典序字符串比较。因此，此示例中的时间戳顺序是 (1, “Aaliyah”) < (1, “Caleb”) < (2, “Bryce”)。

##### 混合逻辑时钟
Lamport 时间戳擅长捕获事物发生的顺序，但它们有一些限制：
- 由于它们与物理时间没有直接关系，你不能使用它们来查找，比如说，在特定日期发布的所有消息——你需要单独存储物理时间。
- 如果两个节点从不通信，一个节点的计数器递增将永远不会反映在另一个节点的计数器中。

_混合逻辑时钟_ 结合了物理日历时钟的优势和 Lamport 时钟的排序保证。像物理时钟一样，它计算秒或微秒。像 Lamport 时钟一样，当一个节点看到来自另一个节点的时间戳大于其本地时钟值时，它将自己的本地值向前移动以匹配另一个节点的时间戳。因此，如果一个节点的时钟运行得很快，其他节点在通信时也会类似地向前移动它们的时钟。

每次生成混合逻辑时钟的时间戳时，它也会递增，这确保时钟单调向前移动，即使底层物理时钟由于 NTP 调整而向后跳跃。因此，混合逻辑时钟可能略微领先于底层物理时钟。算法的细节确保这种差异尽可能小。

##### Lamport/混合逻辑时钟 vs. 向量时钟

**快照隔离实现**：本质上，通过给每个事务一个事务 ID，并允许每个事务看到由 ID 较低的事务进行的写入，但使 ID 较高的事务的写入不可见。

果你想能够确定记录何时并发创建，你需要不同的算法，例如 _向量时钟_。缺点是向量时钟的时间戳要大得多——可能是系统中每个节点一个整数。

#### 线性一致的 ID 生成器

尽管 Lamport 时钟和混合逻辑时钟提供了有用的排序保证，但该排序仍然弱于我们之前讨论的线性一致单节点 ID 生成器。回想一下，线性一致性要求如果请求 A 在请求 B 开始之前完成，那么 B 必须具有更高的 ID，即使 A 和 B 从未相互通信。

非线性一致 ID 生成器如何导致问题：

想象一个社交媒体网站，用户 A 想要与朋友私下分享一张尴尬的照片。A 的账户最初是公开的，但使用他们的笔记本电脑，A 首先将他们的账户设置更改为私密。然后 A 使用他们的手机上传照片。由于 A 按顺序执行了这些更新，他们可能合理地期望照片上传受到新的、受限的账户权限的约束。
![[file-20250830155026967.png]]
想象一个社交媒体网站，用户 A 想要与朋友私下分享一张尴尬的照片。A 的账户最初是公开的，但使用他们的笔记本电脑，A 首先将他们的账户设置更改为私密。然后 A 使用他们的手机上传照片。由于 A 按顺序执行了这些更新，他们可能合理地期望照片上传受到新的、受限的账户权限的约束。

最简单的解决方案是使用线性一致的 ID 生成器。

##### 实现线性一致的 ID 生成器

确保 ID 分配线性一致的最简单方法实际上是为此目的**使用单个节点**。该节点只需要原子地递增计数器并在请求时返回其值，持久化计数器值（以便在节点崩溃并重新启动时不会生成重复的 ID），并使用单主复制进行容错复制。

你不能轻易地对 ID 生成器进行分片，因为如果你有多个分片独立分发 ID，你就无法再保证它们的顺序是线性一致的。

如果你不想使用单节点 ID 生成器，可以使用替代方案：你可以做 Google 的 Spanner 所做的，它依赖于物理时钟，该时钟不仅返回单个时间戳，还返回表示时钟读数不确定性的时间戳范围。然后它等待该不确定性间隔的持续时间过去后再返回。缺点是你需要硬件和软件支持，以使时钟紧密同步并计算必要的不确定性间隔。

##### 使用逻辑时钟强制约束

逻辑时钟或线性一致的 ID 生成器是否也足以实现这些东西？

不完全。当你有几个节点都试图获取同一个锁或注册同一个用户名时，你可以使用逻辑时钟为这些请求分配时间戳，并选择具有最低时间戳的请求作为获胜者。如果时钟是线性一致的，你知道任何未来的请求都将始终生成更大的时间戳，因此你可以确定没有未来的请求会收到比获胜者更低的时间戳。

要以容错方式实现锁、租约和类似构造，我们需要比逻辑时钟或 ID 生成器更强大的东西：我们需要共识。

### 共识

已经看到了几个只有单个节点时很容易，但如果你想要容错就会变得困难得多的例子：
- 如果该主节点失败，如何进行故障切换，同时避免脑裂？如何确保一个认为自己是主节点的节点实际上没有被投票罢免？
- 单节点上的线性一致 ID 生成器只是一个带有原子获取并增加指令的计数器，但如果它崩溃了怎么办？
- 原子比较并设置（CAS）操作对许多事情都很有用，例如当多个进程竞相获取它时决定谁获得锁或租约，或确保具有给定名称的文件或用户的唯一性。在单个节点上，CAS 可能就像一条 CPU 指令一样简单，但如何使其容错？

共识是分布式计算中最重要和最基本的问题之一；它也是出了名的难以正确实现

最著名的共识算法是 Viewstamped Replication、Paxos 、Raft  和 Zab。这些算法之间有相当多的相似之处，但它们并不相同。这些算法在非拜占庭系统模型中工作：也就是说，网络通信可能会被任意延迟或丢弃，节点可能会崩溃、重启和断开连接，但算法假设节点在其他方面正确遵循协议，不会恶意行为。

也有可以容忍某些拜占庭节点的共识算法，即不正确遵循协议的节点（例如，向其他节点发送矛盾消息）。一个常见的假设是少于三分之一的节点是拜占庭故障的 [26](https://ddia.vonng.com/ch10/#fn:26) [70](https://ddia.vonng.com/ch10/#fn:70)。这种 _拜占庭容错_（BFT）共识算法用于区块链。

#### 共识的多面性

共识可以用几种不同的方式表达：

- _单值共识_ 非常类似于原子 _比较并设置_ 操作，它可用于实现锁、租约和唯一性约束。
- 构建 _仅追加日志_ 也需要共识；它通常形式化为 _全序广播_。有了日志，你可以构建 _状态机复制_、基于主节点的复制、事件溯源和其他有用的东西。
- 多数据库或多分片事务的 _原子提交_ 要求所有参与者就是否提交或中止事务达成一致。

##### 单值共识

共识的标准表述涉及让多个节点就单个值达成一致。例如：

- 当具有单主复制的数据库首次启动时，或者当现有主节点失败时，多个节点可能会同时尝试成为主节点。同样，多个节点可能竞相获取锁或租约。共识允许它们决定哪一个获胜。
- 如果几个人同时尝试预订飞机上的最后一个座位，或剧院中的同一个座位，或尝试使用相同的用户名注册账户，那么共识算法可以确定哪一个应该成功。

更一般地说，一个或多个节点可能 _提议_ 值，共识算法 _决定_ 其中一个值。
在这种形式主义中，共识算法必须满足以下属性：
- **一致同意**：没有两个节点决定不同。
- **完整性**：一旦节点决定了一个值，它就不能通过决定另一个值来改变主意。
- **有效性**：如果节点决定值 _v_，那么 _v_ 是由某个节点提议的。
- **终止**：每个未崩溃的节点最终都会决定某个值。

一致同意和完整性属性定义了共识的核心思想：每个人都决定相同的结果，一旦你决定了，你就不能改变主意。有效性属性排除了琐碎的解决方案。

如果你不关心容错，那么满足前三个属性很容易：你可以硬编码一个节点作为"独裁者"，让该节点做出所有决定。然而，如果那个节点失败，那么系统就无法再做出任何决定——就像没有故障切换的单主复制一样。所有的困难都来自对容错的需求。

终止属性形式化了容错的想法。它本质上是说共识算法不能简单地坐着什么都不做——换句话说，它必须取得进展。即使某些节点失败，其他节点仍必须达成决定。

共识必须确保即使崩溃的节点突然消失并且永远不会回来，它也会做出决定。

当然，如果 _所有_ 节点都崩溃了，并且没有一个在运行，那么任何算法都不可能决定任何事情。算法可以容忍的故障数量是有限的：事实上，可以证明任何共识算法都需要至少大多数节点正常运行才能确保终止。该多数可以安全地形成仲裁

##### 比较并设置作为共识

比较并设置（CAS）操作检查某个对象的当前值是否等于某个期望值；如果是，它原子地将对象更新为某个新值；如果不是，它保持对象不变并返回错误。

如果你有容错、线性一致的 CAS 操作，很容易解决共识问题：最初将对象设置为空值；每个想要提议值的节点都使用期望值为空、新值为它想要提议的值（假设它是非空的）调用 CAS。然后决定的值就是对象设置的任何值。

CAS 和共识彼此等价。两者在单个节点上都很简单，但要使其容错则具有挑战性。线性一致的寄存器无法解决共识。

##### 共享日志作为共识

日志存储一系列 _日志条目_，任何读取它的人都会看到相同顺序的相同条目。有时日志有一个允许追加新条目的单个写入者，但 _共享日志_ 是多个节点可以请求追加条目的日志。单主复制的一个例子：任何客户端都可以要求主节点进行写入，主节点将其追加到复制日志，然后所有从节点按照与主节点相同的顺序应用写入。

更正式地说，共享日志支持两种操作：你可以请求将值添加到日志中，并且可以读取日志中的条目。它必须满足以下属性：
- **最终追加**：如果节点请求将某个值添加到日志中，并且节点不会崩溃，那么该节点最终必须在日志条目中读取该值。
- **可靠交付**：没有日志条目丢失：如果一个节点读取某个日志条目，那么最终每个未崩溃的节点也必须读取该日志条目。
- **仅追加**：一旦节点读取了某个日志条目，它就是不可变的，新的日志条目只能在它之后添加，而不能在之前。
- **一致性**：如果两个节点都读取某个日志条目 _e_，那么在 _e_ 之前，它们必须以相同的顺序读取完全相同的日志条目序列。
- **有效性**：如果节点读取包含某个值的日志条目，那么某个节点先前请求将该值添加到日志中。

如果你有共享日志的实现，很容易解决共识问题：每个想要提议值的节点都请求将其添加到日志中，第一个日志条目中读回的任何值就是决定的值。由于所有节点以相同的顺序读取日志条目，它们保证就首先交付哪个值达成一致。

共识等价于全序广播和共享日志。没有故障切换的单主复制不满足活性要求，因为如果主节点崩溃，它将停止传递消息。像往常一样，挑战在于安全地自动执行故障切换。

##### 获取并增加作为共识

可以使用获取并增加操作实现这样的 ID 生成器，该操作原子地递增计数器并返回旧的计数器值。

如果你有 CAS 操作，很容易实现获取并增加：首先读取计数器值，然后执行 CAS，其中期望值是你读取的值，新值是该值加一。如果 CAS 失败，你将重试整个过程，直到 CAS 成功。当存在争用时，这比本机获取并增加操作效率低，但在功能上是等效的。由于你可以使用共识实现 CAS，你也可以使用共识实现获取并增加。

##### 原子提交作为共识

共识和原子提交之间有什么关系？乍一看，它们似乎非常相似——两者都需要节点达成某种形式的一致。然而，有一个重要的区别：对于共识，可以决定提议的任何值，而对于原子提交，如果 _任何_ 参与者投票中止，算法 _必须_ 中止。更准确地说，原子提交需要以下属性：

- **一致同意**：没有两个节点决定不同的结果。
- **完整性**：一旦节点决定了一个结果，它就不能通过决定另一个结果来改变主意。
- **有效性**：如果节点决定提交，那么所有节点必须先前投票提交。如果任何节点投票中止，节点必须中止。
- **非平凡性**：如果所有节点都投票提交，并且没有发生通信超时，那么所有节点必须决定提交。
- **终止**：每个未崩溃的节点最终都会决定提交或中止。

有效性属性确保事务只有在所有节点都同意时才能提交；非平凡性属性确保算法不能简单地总是中止（但如果任何节点之间的通信超时，它允许中止）。其他三个属性基本上与共识相同。

原子提交和共识也是彼此等价的。

#### 共识的实践

单值共识、CAS、共享日志和原子提交都彼此等价：你可以将其中一个的解决方案转换为任何其他的解决方案。这是一个有价值的理论见解，但它没有回答这个问题：在实践中，这些许多共识表述中哪一个最有用？

答案是大多数共识系统提供共享日志，也称为全序广播。Raft、Viewstamped Replication 和 Zab 直接提供共享日志。Paxos 提供单值共识，但在实践中，大多数使用 Paxos 的系统实际上使用称为 Multi-Paxos 的扩展，它也提供共享日志。

##### 使用共享日志

共享日志非常适合数据库复制：如果每个日志条目代表对数据库的写入，并且每个副本使用确定性逻辑以相同的顺序处理相同的写入，那么副本将全部处于一致状态。这个想法被称为 _状态机复制_。

共享日志可用于实现可串行化事务，如果每个日志条目代表要作为存储过程执行的确定性事务，并且如果每个节点以相同的顺序执行这些事务，那么事务将是可串行化的。

共享日志也很强大，因为它可以很容易地适应其他形式的共识：

- 我们之前看到了如何使用它来实现单值共识和 CAS：只需决定日志中首先出现的值。
- 如果你想要许多单值共识实例（例如，几个人试图预订的剧院中每个座位一个），请在日志条目中包含座位编号，并决定包含给定座位编号的第一个日志条目。
- 如果你想要原子获取并增加，请将要添加到计数器的数字放入日志条目中，当前计数器值是到目前为止所有日志条目的总和。日志条目上的简单计数器可用于生成栅栏令牌

##### 从单主复制到共识

如果该节点失败如何提供容错。

传统上，具有单主复制的数据库没有解决这个问题：它们将主节点故障切换作为人类管理员必须手动执行的操作。对于共识，我们要求算法可以自动选择新的主节点。

有一个问题。我们之前讨论过脑裂的问题，我们需要共识来选举主节点，而我们需要主节点来解决共识。我们如何摆脱这个难题？

事实上，共识算法不要求在任何时候只有一个主节点。相反，它们做出了较弱的保证：它们定义了一个 _纪元编号_（在 Paxos 中称为 _投票编号_，在 Viewstamped Replication 中称为 _视图编号_，在 Raft 中称为 _任期编号_）并保证在每个纪元内，主节点是唯一的。

当节点因为在某个超时时间内没有收到主节点的消息而认为当前主节点已死时，它可能会开始投票选举新的主节点。这次选举被赋予一个大于任何先前纪元的新纪元编号。如果两个不同纪元中的两个不同主节点之间存在冲突（也许是因为先前的主节点实际上并没有死），那么具有更高纪元编号的主节点获胜。

在主节点被允许将下一个条目追加到共享日志之前，它必须首先检查是否有其他具有更高纪元编号的主节点可能追加不同的条目。它可以通过从节点仲裁收集投票来做到这一点——通常但不总是大多数节点 。只有在节点不知道任何其他具有更高纪元的主节点时，节点才会投赞成票。

因此，我们有两轮投票：一次选择主节点，第二次对主节点提议的下一个要追加到日志的条目进行投票。这两次投票的仲裁必须重叠：如果对提议的投票成功，投票支持它的节点中至少有一个也必须参与了最近成功的主节点选举 。因此，如果对提议的投票通过而没有透露任何更高编号的纪元，当前主节点可以得出结论，没有选出具有更高纪元编号的主节点，因此它可以安全地将提议的条目追加到日志中。

类似于两阶段提交，但它们是非常不同的协议。在共识算法中，任何节点都可以开始选举，它只需要节点仲裁的响应；在 2PC 中，只有协调器可以请求投票，它需要 _每个_ 参与者的"是"投票才能提交。

##### 共识的微妙之处

这个基本结构对于 Raft、Multi-Paxos、Zab 和 Viewstamped Replication 的所有都是通用的：节点仲裁的投票选举主节点，然后主节点想要追加到日志的每个条目都需要另一个仲裁投票。每个新的日志条目在确认给请求写入的客户端之前都会同步复制到节点仲裁。这确保如果当前主节点失败，日志条目不会丢。

魔鬼在细节中，这也是这些算法采用不同方法的地方。例如，当旧主节点失败并选出新主节点时，算法需要确保新主节点遵守旧主节点在失败之前已经追加的任何日志条目。Raft 通过只允许其日志至少与其大多数追随者一样最新的节点成为新主节点来做到这一点。相比之下，Paxos 允许任何节点成为新主节点，但要求它在开始追加自己的新条目之前使其日志与其他节点保持最新。

在某些情况下，你可能选择削弱共识属性，以便更快地从主节点故障中恢复。例如，Kafka 提供了启用 _不干净的主节点选举_ 的选项，它允许任何副本成为主节点，即使它不是最新的。此外，在具有异步复制的数据库中，当主节点失败时，你无法保证任何从节点是最新的。

如果你放弃新主节点必须是最新的要求，你可能会提高性能和可用性，但你是在薄冰上，因为共识理论不再适用。

对于使用共识算法进行复制的数据库，不仅写入需要转换为日志条目并复制到仲裁。如果你想保证线性一致的读取，它们也必须像写入一样通过仲裁投票，以确认认为自己是主节点的节点确实仍然是最新的。例如，etcd 中的线性一致读取就是这样工作的。

在其标准形式中，大多数共识算法假设一组固定的节点——也就是说，节点可能会宕机并重新启动，但允许投票的节点集在创建集群时是固定的。在实践中，通常需要在系统配置中添加新节点或删除旧节点。共识算法已经扩展了 _重新配置_ 功能，使这成为可能。这在向系统添加新区域或从一个位置迁移到另一个位置（通过首先添加新节点，然后删除旧节点）时特别有用。

##### 共识的利弊

共识算法是分布式系统的巨大突破。共识本质上是"正确完成的单主复制"，在主节点故障时自动故障切换，确保没有已提交的数据丢失，也不可能出现脑裂.

由于单主复制与自动故障切换本质上是共识的定义之一，任何提供自动故障切换但不使用经过验证的共识算法的系统都可能是不安全的.

然而，共识并不是到处都使用，因为好处是有代价的。共识系统总是需要严格的多数才能运行——容忍一个故障需要三个节点，或者容忍两个故障需要五个节点。每个操作都需要与仲裁通信，因此你不能通过添加更多节点来增加吞吐量。

共识系统通常依赖超时来检测失败的节点。在具有高度可变网络延迟的环境中，特别是跨多个地理区域分布的系统，调整这些超时可能很困难：如果它们太大，从故障中恢复需要很长时间；如果它们太小，可能会有很多不必要的主节点选举，导致糟糕的性能，因为系统最终花费更多时间选择主节点而不是做有用的工作。

对于想要高可用但不想接受共识成本的系统，唯一真正的选择是使用较弱的一致性模型。

# 第三部分：派生数据

从高层次上看，存储和处理数据的系统可以分为两大类：

- **权威记录系统**（System of record）：**记录系统**，也被称为 **真相源（source of truth）**，持有数据的权威版本。当新的数据进入时（例如，用户输入）首先会记录在这里。 每个事实正正好好表示一次（表示通常是 **正规化的**，即 normalized）。如果其他系统和 **记录系统** 之间存在任何差异，那么记录系统中的值是正确的（根据定义）。
- **派生数据系统**（Derived data systems）：**派生系统** 中的数据，通常是另一个系统中的现有数据以某种方式进行转换或处理的结果。如果丢失派生数据，可以从原始来源重新创建。 典型的例子是 **缓存（cache）**：如果数据在缓存中，就可以由缓存提供服务；如果缓存不包含所需数据，则降级由底层数据库提供。非规范化的值，索引和物化视图亦属此类。在推荐系统中，预测汇总数据通常派生自用户日志。

从技术上讲，派生数据是 **冗余的（redundant）**，因为它重复了已有的信息。但是派生数据对于获得良好的只读查询性能通常是至关重要的。它通常是非规范化的。可以从单个源头派生出多个不同的数据集，使你能从不同的 “视角” 洞察数据。

**记录系统和派生数据系统之间的区别不在于工具，而在于应用程序中的使用方式。**

## 第十一章：批处理


许多现有数据系统中都采用这种数据处理方式：你发送请求指令，一段时间后（我们期望）系统会给出一个结果。数据库、缓存、搜索索引、Web 服务器以及其他一些系统都以这种方式工作。

三种不同类型的系统：
- **服务（在线系统）**：服务等待客户的请求或指令到达。每收到一个，服务会试图尽快处理它，并发回一个响应。响应时间通常是服务性能的主要衡量指标，可用性通常非常重要
- **批处理系统（离线系统）**：一个批处理系统有大量的输入数据，跑一个 **作业（job）** 来处理它，并生成一些输出数据，这往往需要一段时间（从几分钟到几天），所以通常不会有用户等待作业完成。批处理作业的主要性能衡量标准通常是吞吐量（处理特定大小的输入所需的时间）。
- **流处理系统（准实时系统）**：流处理介于在线和离线（批处理）之间，所以有时候被称为 **准实时（near-real-time）** 或 **准在线（nearline）** 处理。像批处理系统一样，流处理消费输入并产生输出（并不需要响应请求）。但是，流式作业在事件发生后不久就会对事件进行操作，而批处理作业则需等待固定的一组输入数据。这种差异使流处理系统比起批处理系统具有更低的延迟。

批处理是构建可靠、可伸缩和可维护应用程序的重要组成部分。

### 使用Unix工具的批处理

假设你有一台 Web 服务器，每次处理请求时都会在日志文件中附加一行。例如，使用 nginx 默认的访问日志格式，日志的一行可能如下所示：

```bash
216.58.210.78 - - [27/Feb/2015:17:55:11 +0000] "GET /css/typography.css HTTP/1.1"
200 3377 "http://martin.kleppmann.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5)
AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36"
```

#### 简单日志分析

使用基本的 Unix 功能创建自己的工具

```bash
cat /var/log/nginx/access.log | #1
  awk '{print $7}' | #2
  sort             | #3
  uniq -c          | #4
  sort -r -n       | #5
  head -n 5          #6
```

1. 读取日志文件
2. 将每一行按空格分割成不同的字段，每行只输出第七个字段，恰好是请求的 URL。在我们的例子中是 `/css/typography.css`。
3. 按字母顺序排列请求的 URL 列表。如果某个 URL 被请求过 n 次，那么排序后，文件将包含连续重复出现 n 次的该 URL。
4. `uniq` 命令通过检查两个相邻的行是否相同来过滤掉输入中的重复行。`-c` 则表示还要输出一个计数器：对于每个不同的 URL，它会报告输入中出现该 URL 的次数。
5. 第二种排序按每行起始处的数字（`-n`）排序，这是 URL 的请求次数。然后逆序（`-r`）返回结果，大的数字在前。
6. 最后，只输出前五行（`-n 5`），并丢弃其余的。该系列命令的输出如下所示：

使用 awk、sed、grep、sort、uniq 和 xargs 的组合，可以在几分钟内完成许多数据分析，并且它们的性能相当的好

##### 命令链与自定义程序

还可以写一个简单的程序来做同样的事情。

```ruby
counts = Hash.new(0)         # 1
File.open('/var/log/nginx/access.log') do |file|
    file.each do |line|
        url = line.split[6]  # 2
        counts[url] += 1     # 3
    end
end

top5 = counts.map{|url, count| [count, url] }.sort.reverse[0...5] # 4
top5.each{|count, url| puts "#{count} #{url}" }                   # 5
```

1. `counts` 是一个存储计数器的哈希表，保存了每个 URL 被浏览的次数，默认为 0。
2. 逐行读取日志，抽取每行第七个被空格分隔的字段为 URL（这里的数组索引是 6，因为 Ruby 的数组索引从 0 开始计数）
3. 将日志当前行中 URL 对应的计数器值加一。
4. 按计数器值（降序）对哈希表内容进行排序，并取前五位。
5. 打印出前五个条目。

##### 排序 VS 内存中的聚合

- Ruby 脚本在内存中保存了一个 URL 的哈希表，将每个 URL 映射到它出现的次数。
- Unix 管道依赖于对 URL 列表的排序

GNU Coreutils（Linux）中的 `sort` 程序通过溢出至磁盘的方式来自动应对大于内存的数据集，并能同时使用多个 CPU 核进行并行排序。这意味着我们之前看到的简单的 Unix 命令链很容易伸缩至大数据集，且不会耗尽内存。瓶颈可能是从磁盘读取输入文件的速度。

#### Unix哲学

Unix 管道的发明者道格・麦克罗伊（Doug McIlroy）在 1964 年首先描述了这种情况：“我们需要一种类似园艺胶管的方式来拼接程序 —— 当我们需要将消息从一个程序传递另一个程序时，直接接上去就行。I/O 应该也按照这种方式进行 ”。水管的类比仍然在生效，通过管道连接程序的想法成为了现在被称为 **Unix 哲学** 的一部分 —— 这一组设计原则在 Unix 用户与开发者之间流行起来，该哲学在 1978 年表述如下：

1. 让每个程序都做好一件事。要做一件新的工作，写一个新程序，而不是通过添加 “功能” 让老程序复杂化。
2. 期待每个程序的输出成为另一个程序的输入。不要将无关信息混入输出。避免使用严格的列数据或二进制输入格式。不要坚持交互式输入。
3. 设计和构建软件时，即使是操作系统，也让它们能够尽早地被试用，最好在几周内完成。不要犹豫，扔掉笨拙的部分，重建它们。
4. 优先使用工具来减轻编程任务，即使必须绕道去编写工具，且在用完后很可能要扔掉大部分。

 自动化，快速原型设计，增量式迭代，对实验友好，将大型项目分解成可管理的块 —— 听起来非常像今天的敏捷开发和 DevOps 运动。

像 `bash` 这样的 Unix shell 可以让我们轻松地将这些小程序组合成令人讶异的强大数据处理任务。

##### 统一的接口

如果你希望能够将任何程序的输出连接到任何程序的输入，那意味着所有程序必须使用相同的 I/O 接口。

在 Unix 中，这种接口是一个 **文件**（file，更准确地说，是一个文件描述符）。一个文件只是一串有序的字节序列。这是一个非常简单的接口，所以可以使用相同的接口来表示许多不同的东西：文件系统上的真实文件，到另一个进程（Unix 套接字，stdin，stdout）的通信通道，设备驱动程序（比如 `/dev/audio` 或 `/dev/lp0`），表示 TCP 连接的套接字，等等。

按照惯例，许多（但不是全部）Unix 程序将这个字节序列视为 ASCII 文本。

没有多少软件能像 Unix 工具一样交互组合的这么好：你不能通过自定义分析工具轻松地将电子邮件帐户的内容和在线购物历史记录以管道传送至电子表格中，并将结果发布到社交网络或维基。今天，像 Unix 工具一样流畅地运行程序是一种例外，而不是规范。

即使是具有 **相同数据模型** 的数据库，将数据从一种数据库导出再导入到另一种数据库也并不容易。缺乏整合导致了数据的 **巴尔干化**。

##### 逻辑与布线相分离

Unix 工具的另一个特点是使用标准输入（`stdin`）和标准输出（`stdout`）。
管道允许你将一个进程的标准输出附加到另一个进程的标准输入。

shell 用户以任何他们想要的方式连接输入和输出；该程序不知道或不关心输入来自哪里以及输出到哪里。（人们可以说这是一种 **松耦合（loose coupling）**，**晚期绑定（late binding）**或 **控制反转（inversion of control）**）。将输入 / 输出布线与程序逻辑分开，可以将小工具组合成更大的系统。

##### 透明度和实验

使 Unix 工具如此成功的部分原因是，它们使查看正在发生的事情变得非常容易：

- Unix 命令的输入文件通常被视为不可变的。这意味着你可以随意运行命令，尝试各种命令行选项，而不会损坏输入文件。
- 你可以在任何时候结束管道，将管道输出到 `less`，然后查看它是否具有预期的形式。这种检查能力对调试非常有用。
- 你可以将一个流水线阶段的输出写入文件，并将该文件用作下一阶段的输入。这使你可以重新启动后面的阶段，而无需重新运行整个管道。

Unix 工具的最大局限在于它们只能在一台机器上运行 —— 而 Hadoop 这样的工具即应运而生。
### MapReduce和分布式文件系统

一个 MapReduce 作业可以和一个 Unix 进程相类比：它接受一个或多个输入，并产生一个或多个输出。

MapReduce 作业在分布式文件系统上读写文件。在 Hadoop 的 MapReduce 实现中，该文件系统被称为 **HDFS（Hadoop 分布式文件系统）**，一个 Google 文件系统（GFS）的开源实现。

与网络连接存储（NAS）和存储区域网络（SAN）架构的共享磁盘方法相比，HDFS 基于 **无共享** 原则。
- 共享磁盘存储由集中式存储设备实现，通常使用定制硬件和专用网络基础设施（如光纤通道）。
- 无共享方法不需要特殊的硬件，只需要通过传统数据中心网络连接的计算机。

HDFS 在每台机器上运行了一个守护进程，它对外暴露网络服务，允许其他节点访问存储在该机器上的文件。 **NameNode** 的中央服务器会跟踪哪个文件块存储在哪台机器上。因此，HDFS 在概念上创建了一个大型文件系统，可以使用所有运行有守护进程的机器的磁盘。

为了容忍机器和磁盘故障，文件块被复制到多台机器上。复制可能意味着多个机器上的相同数据的多个副本。

最大的 HDFS 部署运行在上万台机器上，总存储容量达数百 PB。

#### MapReduce作业执行

MapReduce 是一个编程框架，你可以使用它编写代码来处理 HDFS 等分布式文件系统中的大型数据集。数据处理模式：
1. 读取一组输入文件，并将其分解成 **记录（records）**。在 Web 服务器日志示例中，每条记录都是日志中的一行（即 `\n` 是记录分隔符）。
2. 调用 Mapper 函数，从每条输入记录中提取一对键值。在前面的例子中，Mapper 函数是 `awk '{print $7}'`：它提取 URL（`$7`）作为键，并将值留空。
3. 按键排序所有的键值对。在日志的例子中，这由第一个 `sort` 命令完成。
4. 调用 Reducer 函数遍历排序后的键值对。如果同一个键出现多次，排序使它们在列表中相邻，所以很容易组合这些值而不必在内存中保留很多状态。在前面的例子中，Reducer 是由 `uniq -c` 命令实现的，该命令使用相同的键来统计相邻记录的数量。

这四个步骤可以作为一个 MapReduce 作业执行。步骤 2（Map）和 4（Reduce）是你编写自定义数据处理代码的地方。步骤 1（将文件分解成记录）由输入格式解析器处理。步骤 3 中的排序步骤隐含在 MapReduce 中 —— 你不必编写它，因为 Mapper 的输出始终在送往 Reducer 之前进行排序。

要创建 MapReduce 作业，你需要实现两个回调函数，Mapper 和 Reducer，其行为如下：
- **Mapper**：Mapper 会在每条输入记录上调用一次，其工作是从输入记录中提取键值。对于每个输入，它可以生成任意数量的键值对（包括 None）。它不会保留从一个输入记录到下一个记录的任何状态，因此每个记录都是独立处理的。
- **Reducer**：MapReduce 框架拉取由 Mapper 生成的键值对，收集属于同一个键的所有值，并在这组值上迭代调用 Reducer。Reducer 可以产生输出记录（例如相同 URL 的出现次数）。

##### 分布式执行MapReduce

每个输入文件的大小通常是数百兆字节。MapReduce 调度器（图中未显示）试图在其中一台存储输入文件副本的机器上运行每个 Mapper，只要该机器有足够的备用 RAM 和 CPU 资源来运行 Mapper 任务。这个原则被称为 **将计算放在数据附近**：它节省了通过网络复制输入文件的开销，减少网络负载并增加局部性。

![[file-20250901154913803.png]]

只要当 Mapper 读取完输入文件，并写完排序后的输出文件，MapReduce 调度器就会通知 Reducer 可以从该 Mapper 开始获取输出文件。Reducer 连接到每个 Mapper，并下载自己相应分区的有序键值对文件。按 Reducer 分区，排序，从 Mapper 向 Reducer 复制分区数据，这一整个过程被称为 **混洗（shuffle）**。

Reducer 调用时会收到一个键，和一个迭代器作为参数，迭代器会顺序地扫过所有具有该键的记录（因为在某些情况可能无法完全放入内存中）。Reducer 可以使用任意逻辑来处理这些记录，并且可以生成任意数量的输出记录。这些输出记录会写入分布式文件系统上的文件中。

##### MapReduce工作流

单个 MapReduce 作业可以解决的问题范围很有限。以日志分析为例，单个 MapReduce 作业可以确定每个 URL 的页面浏览次数，但无法确定最常见的 URL，因为这需要第二轮排序。

将 MapReduce 作业链接成为 **工作流（workflow）** 中是极为常见的，例如，一个作业的输出成为下一个作业的输入。

这个链是通过目录名隐式实现的：第一个作业必须将其输出配置为 HDFS 中的指定目录，第二个作业必须将其输入配置为从同一个目录。从 MapReduce 框架的角度来看，这是两个独立的作业。

只有当作业成功完成后，批处理作业的输出才会被视为有效的（MapReduce 会丢弃失败作业的部分输出）。因此，工作流中的一项作业只有在先前的作业 —— 即生产其输入的作业 —— 成功完成后才能开始。为了处理这些作业之间的依赖，有很多针对 Hadoop 的工作流调度器被开发出来，包括 Oozie、Azkaban、Luigi、Airflow 和 Pinball。

这些调度程序还具有管理功能，在维护大量批处理作业时非常有用。在构建推荐系统时，由 50 到 100 个 MapReduce 作业组成的工作流是常见的。

#### Reduce侧连接与分组

在许多数据集中，一条记录与另一条记录存在关联是很常见的：关系模型中的 **外键**，文档模型中的 **文档引用** 或图模型中的 **边**。

在数据库中，如果执行只涉及少量记录的查询，数据库通常会使用 **索引** 来快速定位感兴趣的记录。如果查询涉及到连接，则可能涉及到查找多个索引。然而 MapReduce 没有索引的概念 —— 至少在通常意义上没有。

当 MapReduce 作业被赋予一组文件作为输入时，它读取所有这些文件的全部内容；数据库会将这种操作称为 **全表扫描**。如果你只想读取少量的记录，则全表扫描与索引查询相比，代价非常高昂。

当我们在批处理的语境中讨论连接时，我们指的是在数据集中解析某种关联的全量存在。

##### 示例：用户活动事件分析

给出了一个批处理作业中连接的典型例子。左侧是事件日志，描述登录用户在网站上做的事情，右侧是用户数据库。
![[file-20250901164105969.png]]

分析任务可能需要将用户活动与用户档案信息相关联。

实现这一连接的最简单方法是，逐个遍历活动事件，并为每个遇到的用户 ID 查询用户数据库（在远程服务器上）。这是可能的，但是它的性能可能会非常差：处理吞吐量将受限于受数据库服务器的往返时间，本地缓存的有效性很大程度上取决于数据的分布，并行运行大量查询可能会轻易压垮数据库。

为了在批处理过程中实现良好的吞吐量，计算必须（尽可能）限于单台机器上进行。为待处理的每条记录发起随机访问的网络请求实在是太慢了。而且，查询远程数据库意味着批处理作业变为 **非确定的（nondeterministic）**，因为远程数据库中的数据可能会改变。

更好的方法是获取用户数据库的副本，然后你可以将用户数据库存储在 HDFS 中的一组文件中，而用户活动记录存储在另一组文件中，并能用 MapReduce 将所有相关记录集中到同一个地方进行高效处理。

##### 排序合并连接

当 MapReduce 框架通过键对 Mapper 输出进行分区，然后对键值对进行排序时，效果是具有相同 ID 的所有活动事件和用户记录在 Reducer 输入中彼此相邻。Map-Reduce 作业甚至可以也让这些记录排序，使 Reducer 总能先看到来自用户数据库的记录，紧接着是按时间戳顺序排序的活动事件 —— 这种技术被称为 **二次排序（secondary sort）**

Reducer 可以容易地执行实际的连接逻辑：每个用户 ID 都会被调用一次 Reducer 函数，且因为二次排序，第一个值应该是来自用户数据库的出生日期记录。Reducer 将出生日期存储在局部变量中，然后使用相同的用户 ID 遍历活动事件，输出 **已观看网址** 和 **观看者年龄** 的结果对。随后的 Map-Reduce 作业可以计算每个 URL 的查看者年龄分布，并按年龄段进行聚集。

由于 Reducer 一次处理一个特定用户 ID 的所有记录，因此一次只需要将一条用户记录保存在内存中，而不需要通过网络发出任何请求。这个算法被称为 **排序合并连接（sort-merge join）**，因为 Mapper 的输出是按键排序的，然后 Reducer 将来自连接两侧的有序记录列表合并在一起。

##### 把相关数据放在一起

在排序合并连接中，Mapper 和排序过程确保了所有对特定用户 ID 执行连接操作的必须数据都被放在同一个地方：单次调用 Reducer 的地方。预先排好了所有需要的数据，Reducer 可以是相当简单的单线程代码，能够以高吞吐量和与低内存开销扫过这些记录。

Mapper 将 “消息” 发送给 Reducer。当一个 Mapper 发出一个键值对时，这个键的作用就像值应该传递到的目标地址。即使键只是一个任意的字符串，它表现的就像一个地址：所有具有相同键的键值对将被传递到相同的目标。

使用 MapReduce 编程模型，能将计算的物理网络通信层面（从正确的机器获取数据）从**应用逻辑中剥离**出来（获取数据后执行处理）。这种分离与数据库的典型用法形成了鲜明对比，从数据库中获取数据的请求经常出现在应用代码内部。

##### 分组

除了连接之外，“把相关数据放在一起” 的另一种常见模式是，按某个键对记录分组（如 SQL 中的 GROUP BY 子句）。所有带有相同键的记录构成一个组，而下一步往往是在每个组内进行某种聚合操作，例如：

- 统计每个组中记录的数量（例如在统计 PV 的例子中，在 SQL 中表示为 `COUNT(*)` 聚合）
- 对某个特定字段求和（SQL 中的 `SUM(fieldname)`）
- 按某种分级函数取出排名前 k 条记录。

使用 MapReduce 实现这种分组操作的最简单方法是设置 Mapper，以便它们生成的键值对使用所需的分组键。然后分区和排序过程将所有具有相同分区键的记录导向同一个 Reducer。因此在 MapReduce 之上实现分组和连接看上去非常相似。

##### 处理偏斜

如果存在与单个键关联的大量数据，则 “将具有相同键的所有记录放到相同的位置” 这种模式就被破坏了。

大多数用户可能会与几百人有连接，但少数名人可能有数百万的追随者。这种不成比例的活动数据库记录被称为 **关键对象（linchpin object）** 或 **热键（hot key）**。

在单个 Reducer 中收集与某个名人相关的所有活动（例如他们发布内容的回复）可能导致严重的 **偏斜**（也称为 **热点**，即 hot spot）

如果连接的输入存在热键，可以使用一些算法进行补偿。例如，Pig 中的 **偏斜连接（skewed join）** 方法首先运行一个抽样作业（Sampling Job）来确定哪些键是热键，连接实际执行时，Mapper 会将热键的关联记录 **随机**（相对于传统 MapReduce 基于键散列的确定性方法）发送到几个 Reducer 之一。

这种技术将处理热键的工作分散到多个 Reducer 上，这样可以使其更好地并行化，代价是需要将连接另一侧的输入记录复制到多个 Reducer 上。

Hive 的偏斜连接优化采取了另一种方法。它需要在表格元数据中显式指定热键，并将与这些键相关的记录单独存放，与其它文件分开。当在该表上执行连接时，对于热键，它会使用 Map 端连接。

#### Map侧连接

Reduce 侧方法的优点是不需要对输入数据做任何假设：无论其属性和结构如何，Mapper 都可以对其预处理以备连接。然而不利的一面是，排序，复制至 Reducer，以及合并 Reducer 输入，所有这些操作可能开销巨大。当数据通过 MapReduce 阶段时，数据可能需要落盘好几次，取决于可用的内存缓冲区。

如果你 **能** 对输入数据作出某些假设，则通过使用所谓的 Map 侧连接来加快连接速度是可行的。

##### 广播散列连接

如果 Map 侧连接的输入以相同的方式进行分区，则散列连接方法可以独立应用于每个分区。

如果分区正确无误，可以确定的是，所有你可能需要连接的记录都落在同一个编号的分区中。因此每个 Mapper 只需要从输入两端各读取一个分区就足够了。好处是每个 Mapper 都可以在内存散列表中少放点数据。

这种方法只有当连接两端输入有相同的分区数，且两侧的记录都是使用相同的键与相同的哈希函数做分区时才适用。如果输入是由之前执行过这种分组的 MapReduce 作业生成的，那么这可能是一个合理的假设。

分区散列连接在 Hive 中称为 **Map 侧桶连接（bucketed map joins）**

##### Map侧合并连接

如果输入数据集不仅以相同的方式进行分区，而且还基于相同的键进行 **排序**，则可适用另一种 Map 侧连接的变体。在这种情况下，输入是否小到能放入内存并不重要，因为这时候 Mapper 同样可以执行归并操作（通常由 Reducer 执行）的归并操作：按键递增的顺序依次读取两个输入文件，将具有相同键的记录配对。

如果能进行 Map 侧合并连接，这通常意味着前一个 MapReduce 作业可能一开始就已经把输入数据做了分区并进行了排序。

##### MapReduce工作流与Map侧连接

当下游作业使用 MapReduce 连接的输出时，选择 Map 侧连接或 Reduce 侧连接会影响输出的结构。Reduce 侧连接的输出是按照 **连接键** 进行分区和排序的，而 Map 端连接的输出则按照与较大输入相同的方式进行分区和排序。

在 Hadoop 生态系统中，这种关于数据集分区的元数据通常在 HCatalog 和 Hive Metastore 中维护

#### 批处理工作流的输出

在数据库查询的场景中，我们将事务处理（OLTP）与分析两种目的区分开来。我们看到，OLTP 查询通常根据键查找少量记录，使用索引，并将其呈现给用户（比如在网页上）。另一方面，分析查询通常会扫描大量记录，执行分组与聚合，输出通常有着报告的形式

批处理放哪里合适？它不属于事务处理，也不是分析。它和分析比较接近，因为批处理通常会扫过输入数据集的绝大部分。然而 MapReduce 作业工作流与用于分析目的的 SQL 查询是不同的。批处理过程的输出通常不是报表，而是一些其他类型的结构。

##### 建立搜索索引

Google 最初使用 MapReduce 是为其搜索引擎建立索引，其实现为由 5 到 10 个 MapReduce 作业组成的工作流。直至今日，Hadoop MapReduce 仍然是为 Lucene/Solr 构建索引的好方法。

如果需要对一组固定文档执行全文检索，则批处理是一种构建索引的高效方法：Mapper 根据需要对文档集合进行分区，每个 Reducer 构建该分区的索引，并将索引文件写入分布式文件系统。构建这样的文档分区索引并行处理效果拔群。
##### 键值存储作为批处理输出

搜索索引只是批处理工作流可能输出的一个例子。批处理的另一个常见用途是构建机器学习系统，例如分类器与推荐系统。

这些批处理作业的输出通常是某种数据库：例如，可以通过给定用户 ID 查询该用户推荐好友的数据库，或者可以通过产品 ID 查询相关产品的数据库。

这些数据库需要被处理用户请求的 Web 应用所查询，而它们通常是独立于 Hadoop 基础设施的。两种选择:
- 在 Mapper 或 Reducer 中使用你最爱的数据库的客户端库，并从批处理作业直接写入数据库服务器，一次写入一条记录。它能工作。但不太好，性能不好
- 在批处理作业 **内** 创建一个全新的数据库，并将其作为文件写入分布式文件系统中作业的输出目录，就像上节中的搜索索引一样。这些数据文件一旦写入就是不可变的，可以批量加载到处理只读查询的服务器中。

##### 批处理输出的哲学

 Unix 哲学鼓励以显式指明数据流的方式进行实验：程序读取输入并写入输出。在这一过程中，输入保持不变，任何先前的输出都被新输出完全替换，且没有其他副作用。

MapReduce 作业的输出处理遵循同样的原理。通过将输入视为不可变且避免副作用（如写入外部数据库），批处理作业不仅实现了良好的性能，而且更容易维护：
- 如果在代码中引入了一个错误，而输出错误或损坏了，则可以简单地回滚到代码的先前版本，然后重新运行该作业，输出将重新被纠正。
- 由于回滚很容易，比起在错误意味着不可挽回的伤害的环境，功能开发进展能快很多。这种 **最小化不可逆性（minimizing irreversibility）** 的原则有利于敏捷软件开发
- 如果 Map 或 Reduce 任务失败，MapReduce 框架将自动重新调度，并在同样的输入上再次运行它。
- 与 Unix 工具类似，MapReduce 作业将逻辑与布线（配置输入和输出目录）分离，这使得关注点分离，可以重用代码。

在这些领域，在 Unix 上表现良好的设计原则似乎也适用于 Hadoop，但 Unix 和 Hadoop 在某些方面也有所不同。例如，因为大多数 Unix 工具都假设输入输出是无类型文本文件，所以它们必须做大量的输入解析工作。在 Hadoop 上可以通过使用更结构化的文件格式消除一些低价值的语法转换：比如 Avro 和 Parquet 经常使用，因为它们提供了基于模式的高效编码，并允许模式随时间推移而演进。

#### Hadoop与分布式数据库的对比

Hadoop 有点像 Unix 的分布式版本，其中 HDFS 是文件系统，而 MapReduce 是 Unix 进程的怪异实现。

MPP 数据库专注于在一组机器上并行执行分析 SQL 查询，而 MapReduce 和分布式文件系统的组合则更像是一个可以运行任意程序的通用操作系统。

##### 存储多样性

数据库要求你根据特定的模型（例如关系或文档）来构造数据，而分布式文件系统中的文件只是字节序列，可以使用任何数据模型和编码来编写。

Hadoop 开放了将数据不加区分地转储到 HDFS 的可能性；相比之下，在将数据导入数据库专有存储格式之前，MPP 数据库通常需要对数据和查询模式进行仔细的前期建模。

实践经验表明，简单地使数据快速可用 —— 即使它很古怪，难以使用，使用原始格式 —— 也通常要比事先决定理想数据模型要更有价值。

将大型组织的各个部分的数据集中在一起是很有价值的，因为它可以跨越以前相互分离的数据集进行连接。MPP 数据库所要求的谨慎模式设计拖慢了集中式数据收集速度；以原始形式收集数据，稍后再操心模式的设计，能使数据收集速度加快（有时被称为 “**数据湖（data lake）**）

不加区分的数据转储转移了解释数据的负担：数据集的生产者不再需要强制将其转化为标准格式，数据的解释成为消费者的问题。如果生产者和消费者是不同优先级的不同团队，这可能是一种优势。甚至可能不存在一个理想的数据模型，对于不同目的有不同的合适视角。以原始形式简单地转储数据，可以允许多种这样的转换。这种方法被称为 **寿司原则（sushi principle）**：“原始数据更好”。

因此，Hadoop 经常被用于实现 ETL 过程：事务处理系统中的数据以某种原始形式转储到分布式文件系统中，然后编写 MapReduce 作业来清理数据，将其转换为关系形式，并将其导入 MPP 数据仓库以进行分析。数据建模仍然在进行，但它在一个单独的步骤中进行，与数据收集相解耦。这种解耦是可行的，因为分布式文件系统支持以任何格式编码的数据。

##### 处理模型的多样性

MPP 数据库是单体的，紧密集成的软件，负责磁盘上的存储布局，查询计划，调度和执行。由于这些组件都可以针对数据库的特定需求进行调整和优化，因此整个系统可以在其设计针对的查询类型上取得非常好的性能。而且，SQL 查询语言允许以优雅的语法表达查询，而无需编写代码，可以在业务分析师使用的可视化工具（例如 Tableau）中访问到。

MapReduce 使工程师能够轻松地在大型数据集上运行自己的代码。如果你有 HDFS 和 MapReduce，那么你 **可以** 在它之上建立一个 SQL 查询执行引擎，事实上这正是 Hive 项目所做的。

 MapReduce 对于某些类型的处理而言局限性很大，表现很差，因此在 Hadoop 之上其他各种处理模型也被开发出来。
 
 Hadoop 生态系统包括随机访问的 OLTP 数据库，如 HBase和 MPP 风格的分析型数据库，如 Impala。HBase 与 Impala 都不使用 MapReduce，但都使用 HDFS 进行存储。它们是迥异的数据访问与处理方法，但是它们可以共存，并被集成到同一个系统中。
##### 针对频繁故障设计

当比较 MapReduce 和 MPP 数据库时，两种不同的设计思路出现了：处理故障和使用内存与磁盘的方式。与在线系统相比，批处理对故障不太敏感，因为就算失败也不会立即影响到用户，而且它们总是能再次运行。

如果一个节点在执行查询时崩溃，大多数 MPP 数据库会**中止**整个查询，并让用户重新提交查询或自动重新运行它；MapReduce 可以**容忍**单个 Map 或 Reduce 任务的失败，而不会影响作业的整体，通过以单个任务的粒度重试工作。它也会非常急切地将数据写入磁盘，一方面是为了容错，另一部分是因为假设数据集太大而不能适应内存。

**MapReduce 方式更适用于较大的作业**：要处理如此之多的数据并运行很长时间的作业，以至于在此过程中很可能至少遇到一个任务故障。在这种情况下，由于单个任务失败而重新运行整个作业将是非常浪费的。即使以单个任务的粒度进行恢复引入了使得无故障处理更慢的开销，但如果任务失败率足够高，这仍然是一种合理的权衡。

**最初设计 MapReduce 的环境**：

Google 有着混用的数据中心，在线生产服务和离线批处理作业在同样机器上运行。每个任务都有一个通过容器强制执行的资源配给（CPU 核心、RAM、磁盘空间等）。每个任务也具有优先级，如果优先级较高的任务需要更多的资源，则可以终止（抢占）同一台机器上较低优先级的任务以释放资源。优先级还决定了计算资源的定价：团队必须为他们使用的资源付费，而优先级更高的进程花费更多。

由于 MapReduce 作业以低优先级运行，它们随时都有被抢占的风险，因为优先级较高的进程可能需要其资源。在高优先级进程拿走所需资源后，批量作业能有效地 “捡面包屑”，利用剩下的任何计算资源。

在谷歌，运行一个小时的 MapReduce 任务有大约有 5% 的风险被终止，为了给更高优先级的进程挪地方。这一概率比硬件问题、机器重启或其他原因的概率高了一个数量级。按照这种抢占率，如果一个作业有 100 个任务，每个任务运行 10 分钟，那么至少有一个任务在完成之前被终止的风险大于 50%。

这就是 MapReduce 被设计为容忍频繁意外任务终止的原因：不是因为硬件很不可靠，而是因为任意终止进程的自由有利于提高计算集群中的资源利用率。

### MapReduce之后

使用原始的 MapReduce API 来实现复杂的处理工作实际上是非常困难和费力的

#### 物化中间状态

在很多情况下，你知道一个作业的输出只能用作另一个作业的输入，这些作业由同一个团队维护。在这种情况下，分布式文件系统上的文件只是简单的 **中间状态（intermediate state）**：一种将数据从一个作业传递到下一个作业的方式。在一个用于构建推荐系统的，由 50 或 100 个 MapReduce 作业组成的复杂工作流中，存在着很多这样的中间状态.

将这个中间状态写入文件的过程称为 **物化（materialization）**。

开头的日志分析示例使用 Unix 管道将一个命令的输出与另一个命令的输入连接起来。管道并没有完全物化中间状态，而是只使用一个小的内存缓冲区，将输出增量地 **流（stream）** 向输入。

与 Unix 管道相比，MapReduce 完全物化中间状态的方法存在不足之处：
- MapReduce 作业只有在前驱作业（生成其输入）中的所有任务都完成时才能启动，而由 Unix 管道连接的进程会同时启动，输出一旦生成就会被消费。
- Mapper 通常是多余的：它们仅仅是读取刚刚由 Reducer 写入的同样文件，为下一个阶段的分区和排序做准备。
- 将中间状态存储在分布式文件系统中意味着这些文件被复制到多个节点，对这些临时数据这么搞就比较过分了。

##### 数据流引擎

为了解决 MapReduce 的这些问题，几种用于分布式批处理的新执行引擎被开发出来，其中最著名的是 Spark ，Tez和 Flink。它们的设计方式有很多区别，但有一个共同点：把整个工作流作为单个作业来处理，而不是把它分解为独立的子作业。

由于它们将工作流显式建模为数据从几个处理阶段穿过，所以这些系统被称为 **数据流引擎（dataflow engines）**。

与 MapReduce 不同，这些函数不需要严格扮演交织的 Map 与 Reduce 的角色，而是可以以更灵活的方式进行组合。我们称这些函数为 **算子（operators）**，数据流引擎提供了几种不同的选项来将一个算子的输出连接到另一个算子的输入：
- 一种选项是对记录按键重新分区并排序，就像在 MapReduce 的混洗阶段一样
- 可能是接受多个输入，并以相同的方式进行分区，但跳过排序。
- 对于广播散列连接，可以将一个算子的输出，发送到连接算子的所有分区。

与 MapReduce 模型相比，它有几个优点：
- 排序等昂贵的工作只需要在实际需要的地方执行，而不是默认地在每个 Map 和 Reduce 阶段之间出现。
- 没有不必要的 Map 任务，因为 Mapper 所做的工作通常可以合并到前面的 Reduce 算子中
- 调度程序能够总览全局，知道哪里需要哪些数据，因而能够利用局部性进行优化。
- 算子间的中间状态足以保存在内存中或写入本地磁盘

你可以使用数据流引擎执行与 MapReduce 工作流同样的计算，而且由于此处所述的优化，通常执行速度要明显快得多。既然算子是 Map 和 Reduce 的泛化，那么相同的处理代码就可以在任一执行引擎上运行：Pig，Hive 或 Cascading 中实现的工作流可以无需修改代码，可以通过修改配置，简单地从 MapReduce 切换到 Tez 或 Spark.

##### 容错

完全物化中间状态至分布式文件系统的一个优点是，它具有持久性，这使得 MapReduce 中的容错相当容易：如果一个任务失败，它可以在另一台机器上重新启动，并从文件系统重新读取相同的输入。

Spark、Flink 和 Tez 避免将中间状态写入 HDFS，因此它们采取了不同的方法来容错：如果一台机器发生故障，并且该机器上的中间状态丢失，则它会从其他仍然可用的数据重新计算

 Spark 使用 **弹性分布式数据集（RDD，Resilient Distributed Dataset）** 的抽象来跟踪数据的谱系，而 Flink 对算子状态存档，允许恢复运行在执行过程中遇到错误的算子。

在重新计算数据时，重要的是要知道计算是否是 **确定性的**.
为了避免这种级联故障，最好让算子具有确定性。

但需要注意的是，非确定性行为很容易悄悄溜进来：例如，许多编程语言在迭代哈希表的元素时不能对顺序作出保证，许多概率和统计算法显式依赖于使用随机数，以及用到系统时钟或外部数据源，这些都是都不确定性的行为。为了能可靠地从故障中恢复，需要消除这种不确定性因素，例如使用固定的种子生成伪随机数。

##### 关于物化的讨论

MapReduce 就像是将每个命令的输出写入临时文件，而数据流引擎看起来更像是 Unix 管道。

排序算子不可避免地需要消费全部的输入后才能生成任何输出，因为输入中最后一条输入记录可能具有最小的键，因此需要作为第一条记录输出。因此，任何需要排序的算子都需要至少暂时地累积状态。但是工作流的许多其他部分可以以流水线方式执行。

在使用数据流引擎时，HDFS 上的物化数据集通常仍是作业的输入和最终输出。和 MapReduce 一样，输入是不可变的，输出被完全替换。比起 MapReduce 的改进是，你不用再自己去将中间状态写入文件系统了。

#### 图与迭代处理

批处理上下文中的图也很有趣，其目标是在整个图上执行某种离线处理或分析。这种需求经常出现在机器学习应用（如推荐引擎）或排序系统中。例如，最着名的图形分析算法之一是 PageRank，它试图根据链接到某个网页的其他网页来估计该网页的流行度。它作为配方的一部分，用于确定网络搜索引擎呈现结果的顺序。

可以在分布式文件系统中存储图（包含顶点和边的列表的文件），但是这种 “重复至完成” 的想法不能用普通的 MapReduce 来表示，因为它只扫过一趟数据。这种算法因此经常以 **迭代** 的风格实现：
1. 外部调度程序运行批处理来计算算法的一个步骤。
2. 当批处理过程完成时，调度器检查它是否完成（基于完成条件 —— 例如，没有更多的边要跟进，或者与上次迭代相比的变化低于某个阈值）。
3. 如果尚未完成，则调度程序返回到步骤 1 并运行另一轮批处理。

这种方法是有效的，但是用 MapReduce 实现它往往非常低效，因为 MapReduce 没有考虑算法的迭代性质：它总是读取整个输入数据集并产生一个全新的输出数据集，即使与上次迭代相比，改变的仅仅是图中的一小部分。

##### Pregel处理模型

针对图批处理的优化 —— **批量同步并行（BSP，Bulk Synchronous Parallel）** 计算模型已经开始流行起来。其中，Apache Giraph，Spark 的 GraphX API 和 Flink 的 Gelly API 实现了它。它也被称为 **Pregel** 模型，因为 Google 的 Pregel 论文推广了这种处理图的方法。

Pregel：一个顶点可以向另一个顶点 “发送消息”，通常这些消息是沿着图的边发送的。

在每次迭代中，为每个顶点调用一个函数，将所有发送给它的消息传递给它 —— 就像调用 Reducer 一样。与 MapReduce 的不同之处在于，在 Pregel 模型中，顶点在一次迭代到下一次迭代的过程中会记住它的状态，所以这个函数只需要处理新的传入消息。如果图的某个部分没有被发送消息，那里就不需要做任何工作。

##### 容错

顶点只能通过消息传递进行通信（而不是直接相互查询）的事实有助于提高 Pregel 作业的性能，因为消息可以成批处理，且等待通信的次数也减少了。唯一的等待是在迭代之间：由于 Pregel 模型保证所有在一轮迭代中发送的消息都在下轮迭代中送达，所以在下一轮迭代开始前，先前的迭代必须完全完成，而所有的消息必须在网络上完成复制。

Pregel 的实现能保证在后续迭代中消息在其目标顶点恰好处理一次。

这种容错是通过在迭代结束时，定期存档所有顶点的状态来实现的，即将其全部状态写入持久化存储。如果某个节点发生故障并且其内存中的状态丢失，则最简单的解决方法是将整个图计算回滚到上一个存档点，然后重启计算。如果算法是确定性的，且消息记录在日志中，那么也可以选择性地只恢复丢失的分区。

##### 并行执行

由于编程模型一次仅处理一个顶点（有时称为 “像顶点一样思考”），所以框架可以以任意方式对图分区。理想情况下如果顶点需要进行大量的通信，那么它们最好能被分区到同一台机器上。然而找到这样一种优化的分区方法是很困难的 —— 在实践中，图经常按照任意分配的顶点 ID 分区，而不会尝试将相关的顶点分组在一起。

因此，图算法通常会有很多跨机器通信的额外开销，而中间状态（节点之间发送的消息）往往比原始图大。通过网络发送消息的开销会显著拖慢分布式图算法的速度。

#### 高级API和语言

自 MapReduce 开始流行的这几年以来，分布式批处理的执行引擎已经很成熟了。到目前为止，基础设施已经足够强大，能够存储和处理超过 10,000 台机器集群上的数 PB 的数据。由于在这种规模下物理执行批处理的问题已经被认为或多或少解决了，所以关注点已经转向其他领域：改进编程模型，提高处理效率，扩大这些技术可以解决的问题集。

如前所述，Hive、Pig、Cascading 和 Crunch 等高级语言和 API 变得越来越流行，因为手写 MapReduce 作业实在是个苦力活。随着 Tez 的出现，这些高级语言还有一个额外好处，可以迁移到新的数据流执行引擎，而无需重写作业代码。Spark 和 Flink 也有它们自己的高级数据流 API，通常是从 FlumeJava 中获取的灵感。

这些数据流 API 通常使用关系型构建块来表达一个计算：按某个字段连接数据集；按键对元组做分组；按某些条件过滤；并通过计数求和或其他函数来聚合元组。在内部，这些操作是使用本章前面讨论过的各种连接和分组算法来实现的。

##### 向声明式查询语言的转变

与硬写执行连接的代码相比，指定连接关系算子的优点是，框架可以分析连接输入的属性，并自动决定哪种上述连接算法最适合当前任务。Hive、Spark 和 Flink 都有基于代价的查询优化器可以做到这一点，甚至可以改变连接顺序，最小化中间状态的数量。

连接算法的选择可以对批处理作业的性能产生巨大影响，而无需理解和记住本章中讨论的各种连接算法。如果连接是以 **声明式（declarative）** 的方式指定的，那这就这是可行的

以声明方式表示这些简单的过滤和映射操作，那么查询优化器可以利用列式存储布局，只从磁盘读取所需的列。Hive、Spark DataFrames 和 Impala 还使用了向量化执行：在对 CPU 缓存友好的内部循环中迭代数据，避免函数调用。

通过在高级 API 中引入声明式的部分，并使查询优化器可以在执行期间利用这些来做优化，批处理框架看起来越来越像 MPP 数据库了。同时，通过拥有运行任意代码和以任意格式读取数据的可扩展性，它们保持了灵活性的优势。

##### 专业化的不同领域

传统上，MPP 数据库满足了商业智能分析和业务报表的需求，但这只是许多使用批处理的领域之一。

一个越来越重要的领域是统计和数值算法，它们是机器学习应用所需要的（例如分类器和推荐系统）。可重用的实现正在出现：例如，Mahout 在 MapReduce、Spark 和 Flink 之上实现了用于机器学习的各种算法，而 MADlib 在关系型 MPP 数据库（Apache HAWQ）中实现了类似的功能。

批处理引擎正被用于分布式执行日益广泛的各领域算法。随着批处理系统获得各种内置功能以及高级声明式算子，且随着 MPP 数据库变得更加灵活和易于编程，两者开始看起来相似了：最终，它们都只是存储和处理数据的系统。

批处理作业的显著特点是，它读取一些输入数据并产生一些输出数据，但不修改输入 —— 换句话说，输出是从输入派生出的。最关键的是，输入数据是 **有界的（bounded）**：它有一个已知的，固定的大小（例如，它包含一些时间点的日志文件或数据库内容的快照）。因为它是有界的，一个作业知道自己什么时候完成了整个输入的读取，所以一个工作在做完后，最终总是会完成的。

在下一章中，我们将转向流处理，其中的输入是 **无界的（unbounded）** —— 也就是说，你还有活儿要干，然而它的输入是永无止境的数据流。

## 第十二章：流处理

很多数据是 **无界限** 的，因为它随着时间的推移而逐渐到达：你的用户在昨天和今天产生了数据，明天他们将继续产生更多的数据。

日常批处理中的问题是，输入的变更**只会在一天之后**的输出中反映出来，这对于许多急躁的用户来说太慢了。为了减少延迟，我们可以更频繁地运行处理 —— 比如说，在每秒钟的末尾 —— 或者甚至更连续一些，完全抛开固定的时间切片，当事件发生时就立即进行处理，这就是 **流处理（stream processing）** 背后的想法。

把 **事件流（event stream）** 视为一种数据管理机制：无界限，增量处理，与上一章中的批量数据相对应。

### 传递事件流

当输入是一个文件（一个字节序列），第一个处理步骤通常是将其解析为一系列记录。在流处理的上下文中，记录通常被叫做 **事件（event）** ，但它本质上是一样的：一个小的、自包含的、不可变的对象，包含某个时间点发生的某件事情的细节。一个事件通常包含一个来自日历时钟的时间戳，以指明事件发生的时间。

在批处理中，文件被写入一次，然后可能被多个作业读取。
在流处理术语中，一个事件由 **生产者（producer）** （也称为 **发布者（publisher）** 或 **发送者（sender）** ）生成一次，然后可能由多个 **消费者（consumer）** （ **订阅者（subscribers）** 或 **接收者（recipients）** ）进行处理。

在文件系统中，文件名标识一组相关记录；
在流式系统中，相关的事件通常被聚合为一个 **主题（topic）** 或 **流（stream）** 。

数据库在传统上对这种通知机制支持的并不好，关系型数据库通常有 **触发器（trigger）** ，它们可以对变化（如，插入表中的一行）作出反应，但是它们的功能非常有限。

#### 消息传递系统

向消费者通知新事件的常用方式是使用 **消息传递系统（messaging system）**：生产者发送包含事件的消息，然后将消息推送给消费者。

在这个 **发布 / 订阅** 模式中，不同的系统采取各种各样的方法，并没有针对所有目的的通用答案。为了区分这些系统，问一下这两个问题会特别有帮助：
1. **如果生产者发送消息的速度比消费者能够处理的速度快会发生什么？** 一般来说，有三种选择：系统可以丢掉消息，将消息放入缓冲队列，或使用 **背压**。
2. **如果节点崩溃或暂时脱机，会发生什么情况？ —— 是否会有消息丢失？** 与数据库一样，持久性可能需要写入磁盘和 / 或复制的某种组合。

##### 直接从生产者传递给消费者

许多消息传递系统使用生产者和消费者之间的直接网络通信，而不通过中间节点：
- UDP 组播广泛应用于金融行业，例如股票市场，其中低时延非常重要
- 无代理的消息库，如 ZeroMQ 和 nanomsg 采取类似的方法

些直接消息传递系统在设计它们的环境中运行良好，但是它们通常要求应用代码意识到消息丢失的可能性。它们的容错程度极为有限
##### 消息代理

一种广泛使用的替代方法是通过 **消息代理**（message broker，也称为 **消息队列**，即 message queue）发送消息，消息代理实质上是一种针对处理消息流而优化的数据库。它作为服务器运行，生产者和消费者作为客户端连接到服务器。生产者将消息写入代理，消费者通过从代理那里读取来接收消息。

通过将数据集中在代理上，这些系统可以更容易地容忍来来去去的客户端（连接，断开连接和崩溃），而持久性问题则转移到代理的身上。

一些消息代理只将消息保存在内存中，而另一些消息代理（取决于配置）将其写入磁盘，以便在代理崩溃的情况下不会丢失。针对缓慢的消费者，它们通常会允许无上限的排队。

排队的结果是，消费者通常是 **异步（asynchronous）** 的：当生产者发送消息时，通常只会等待代理确认消息已经被缓存，而不等待消息被消费者处理。向消费者递送消息将发生在未来某个未定的时间点 —— 通常在几分之一秒之内，但有时当消息堆积时会显著延迟。

##### 消息代理与数据库的对比

有些消息代理甚至可以使用 XA 或 JTA 参与两阶段提交协议，这个功能与数据库在本质上非常相似，尽管消息代理和数据库之间仍存在实践上很重要的差异：

- 数据库通常保留数据直至显式删除，而大多数消息代理在消息成功递送给消费者时会自动删除消息。
- 由于它们很快就能删除消息，大多数消息代理都认为它们的工作集相当小 —— 即队列很短。
- 数据库通常支持次级索引和各种搜索数据的方式，而消息代理通常支持按照某种模式匹配主题，订阅其子集。
- 查询数据库时，结果通常基于某个时间点的数据快照；消息代理不支持任意查询，但是当数据发生变化时（即新消息可用时），它们会通知客户端。

这是关于消息代理的传统观点，它被封装在诸如 JMS和 AMQP 的标准中，并且被诸如 RabbitMQ、ActiveMQ、HornetQ、Qpid等实现。
##### 多个消费者

当多个消费者从同一主题中读取消息时，有两种主要的消息传递模式：

- **负载均衡（load balancing）**：每条消息都被传递给消费者 **之一**，所以处理该主题下消息的工作能被多个消费者共享。代理可以为消费者任意分配消息。
- **扇出（fan-out）**：每条消息都被传递给 **所有** 消费者。扇出允许几个独立的消费者各自 “收听” 相同的消息广播，而不会相互影响

![[Pasted image 20250902192008.png]]

两种模式可以组合使用：例如，两个独立的消费者组可以每组各订阅同一个主题，每一组都共同收到所有消息，但在每一组内部，每条消息仅由单个节点处理。

##### 确认与重新传递

消费者随时可能会崩溃，所以有一种可能的情况是：代理向消费者递送消息，但消费者没有处理，或者在消费者崩溃之前只进行了部分处理。为了确保消息不会丢失，消息代理使用 **确认（acknowledgments）**：客户端必须显式告知代理消息处理完毕的时间，以便代理能将消息从队列中移除。

如果与客户端的连接关闭，或者代理超出一段时间未收到确认，代理则认为消息没有被处理，因此它将消息再递送给另一个消费者。

当与负载均衡相结合时，这种重传行为对消息的顺序有种有趣的影响。消费者通常按照生产者发送的顺序处理消息。然而消费者 2 在处理消息 m3 时崩溃，与此同时消费者 1 正在处理消息 m4。未确认的消息 m3 随后被重新发送给消费者 1，结果消费者 1 按照 m4，m3，m5 的顺序处理消息。因此 m3 和 m4 的交付顺序与生产者 1 的发送顺序不同。
![[Pasted image 20250902192752.png]]
即使消息代理试图保留消息的顺序（如 JMS 和 AMQP 标准所要求的），负载均衡与重传的组合也不可避免地导致消息被重新排序。为避免此问题，你可以让每个消费者使用单独的队列（即不使用负载均衡功能）。如果消息是完全独立的，则消息顺序重排并不是一个问题。

#### 分区日志

批处理过程的一个关键特性是，你可以反复运行它们，试验处理步骤，不用担心损坏输入（因为输入是只读的）。而 AMQP/JMS 风格的消息传递并非如此：收到消息是具有破坏性的，因为确认可能导致消息从代理中被删除，因此你不能期望再次运行同一个消费者能得到相同的结果。

如果你将新的消费者添加到消息传递系统，通常只能接收到消费者注册之后开始发送的消息。先前的任何消息都随风而逝，一去不复返。作为对比，你可以随时为文件和数据库添加新的客户端，且能读取任意久远的数据。

杂交一下，既有数据库的持久存储方式，又有消息传递的低延迟通知？这就是 **基于日志的消息代理（log-based message brokers）** 背后的想法。

##### 使用日志进行消息存储

日志只是磁盘上简单的仅追加记录序列。同样的结构可以用于实现消息代理：生产者通过将消息追加到日志末尾来发送消息，而消费者通过依次读取日志来接收消息。如果消费者读到日志末尾，则会等待新消息追加的通知。Unix 工具 `tail -f` 能监视文件被追加写入的数据，基本上就是这样工作的。

为了伸缩超出单个磁盘所能提供的更高吞吐量，可以对日志进行 **分区**。不同的分区可以托管在不同的机器上，使得每个分区都有一份能独立于其他分区进行读写的日志。一个主题可以定义为一组携带相同类型消息的分区。
在每个分区内，代理为每个消息分配一个单调递增的序列号或 **偏移量**（offset）。这种序列号是有意义的，因为分区是仅追加写入的，所以分区内的消息是完全有序的。没有跨不同分区的顺序保证。
![[Pasted image 20250902193219.png]]
Apache Kafka 、Amazon Kinesis Streams和 Twitter 的 DistributedLog都是基于日志的消息代理。

尽管这些消息代理将所有消息写入磁盘，但通过跨多台机器分区，每秒能够实现数百万条消息的吞吐量，并通过复制消息来实现容错性

##### 日志与传统的消息传递相比

基于日志的方法天然支持扇出式消息传递，因为多个消费者可以独立读取日志，而不会相互影响 —— 读取消息不会将其从日志中删除。

然后每个客户端将消费被指派分区中的 **所有** 消息。通常情况下，当一个用户被指派了一个日志分区时，它会以简单的单线程方式顺序地读取分区中的消息。这种粗粒度的负载均衡方法有一些缺点：
- 共享消费主题工作的节点数，最多为该主题中的日志分区数，因为同一个分区内的所有消息被递送到同一个节点
- 如果某条消息处理缓慢，则它会阻塞该分区中后续消息的处理

因此在消息处理代价高昂，希望逐条并行处理，以及消息的顺序并没有那么重要的情况下，JMS/AMQP 风格的消息代理是可取的。另一方面，在消息吞吐量很高，处理迅速，顺序很重要的情况下，基于日志的方法表现得非常好。

##### 消费者偏移量

顺序消费一个分区使得判断消息是否已经被处理变得相当容易：所有偏移量小于消费者的当前偏移量的消息已经被处理，而具有更大偏移量的消息还没有被看到。因此，代理不需要跟踪确认每条消息，只需要定期记录消费者的偏移即可。

这种偏移量与单领导者数据库复制中常见的日志序列号非常相似。在数据库复制中，日志序列号允许跟随者断开连接后，重新连接到领导者，并在不跳过任何写入的情况下恢复复制。这里原理完全相同：消息代理表现得像一个主库，而消费者就像一个从库。

如果消费者节点失效，则失效消费者的分区将指派给其他节点，并从最后记录的偏移量开始消费消息。如果消费者已经处理了后续的消息，但还没有记录它们的偏移量，那么重启后这些消息将被处理两次。

##### 磁盘空间使用

如果只追加写入日志，则磁盘空间终究会耗尽。为了回收磁盘空间，日志实际上被分割成段，并不时地将旧段删除或移动到归档存储。

如果一个慢消费者跟不上消息产生的速率而落后得太多，它的消费偏移量指向了删除的段，那么它就会错过一些消息。实际上，日志实现了一个有限大小的缓冲区，当缓冲区填满时会丢弃旧消息，它也被称为 **循环缓冲区（circular buffer）** 或 **环形缓冲区（ring buffer）**。不过由于缓冲区在磁盘上，因此缓冲区可能相当的大。

不管保留多长时间的消息，日志的吞吐量或多或少保持不变，因为无论如何，每个消息都会被写入磁盘。

##### 当消费者跟不上生产者时

如果消费者无法跟上生产者发送信息的速度时，我们讨论了三种选择：丢弃信息，进行缓冲或施加背压。在这种分类法里，基于日志的方法是缓冲的一种形式，具有很大但大小固定的缓冲区。

即使消费者真的落后太多开始丢失消息，也只有那个消费者受到影响；它不会中断其他消费者的服务。

##### 重播旧消息

处理和确认消息是一个破坏性的操作，因为它会导致消息在代理上被删除。另一方面，在基于日志的消息代理中，使用消息更像是从文件中读取数据：这是只读操作，不会更改日志。

除了消费者的任何输出之外，处理的唯一副作用是消费者偏移量的前进。但偏移量是在消费者的控制之下的，所以如果需要的话可以很容易地操纵：例如你可以用昨天的偏移量跑一个消费者副本，并将输出写到不同的位置，以便重新处理最近一天的消息。你可以使用各种不同的处理代码重复任意次。

这一方面使得基于日志的消息传递更像上一章的批处理，其中派生数据通过可重复的转换过程与输入数据显式分离。它允许进行更多的实验，更容易从错误和漏洞中恢复，使其成为在组织内集成数据流的良好工具。

### 数据库与流

基于日志的消息代理已经成功地从数据库中获取灵感并将其应用于消息传递。

反过来：从消息传递和流中获取灵感，并将它们应用于数据库。

事件是某个时刻发生的事情的记录。发生的事情可能是用户操作（例如键入搜索查询）或读取传感器，但也可能是 **写入数据库**。某些东西被写入数据库的事实是可以被捕获、存储和处理的事件。

复制日志是一个由数据库写入事件组成的流，由主库在处理事务时生成。从库将写入流应用到它们自己的数据库副本，从而最终得到相同数据的精确副本。复制日志中的事件描述发生的数据更改。

#### 保持系统同步

没有一个系统能够满足所有的数据存储、查询和处理需求。在实践中，大多数重要应用都需要组合使用几种不同的技术来满足所有的需求：例如，使用 OLTP 数据库来为用户请求提供服务，使用缓存来加速常见请求，使用全文索引来处理搜索查询，使用数据仓库用于分析。每一种技术都有自己的数据副本，并根据自己的目的进行存储方式的优化。

由于相同或相关的数据出现在了不同的地方，因此相互间需要保持同步：
- 如果某个项目在数据库中被更新，它也应当在缓存、搜索索引和数据仓库中被更新。对于数据仓库，这种同步通常由 ETL 进程执行
- 如果周期性的完整数据库转储过于缓慢，有时会使用的替代方法是 **双写（dual write）**，其中应用代码在数据变更时明确写入每个系统

双写有一些严重的问题，其中一个是竞争条件。在这个例子中，两个客户端同时想要更新一个项目 X：客户端 1 想要将值设置为 A，客户端 2 想要将其设置为 B。两个客户端首先将新值写入数据库，然后将其写入到搜索索引。因为运气不好，这些请求的时序是交错的：数据库首先看到来自客户端 1 的写入将值设置为 A，然后来自客户端 2 的写入将值设置为 B，因此数据库中的最终值为 B。搜索索引首先看到来自客户端 2 的写入，然后是客户端 1 的写入，所以搜索索引中的最终值是 A。即使没发生错误，这两个系统现在也永久地不一致了。

![[Pasted image 20250902195539.png]]

双重写入的另一个问题是，其中一个写入可能会失败，而另一个成功。这是一个容错问题，而不是一个并发问题，但也会造成两个系统互相不一致的结果。确保它们要么都成功要么都失败，是原子提交问题的一个例子，解决这个问题的代价是昂贵的。

如果你只有一个单领导者复制的数据库，那么这个领导者决定了写入顺序，而状态机复制方法可以在数据库副本上工作。

#### 变更数据捕获

多数据库根本没有记录在档的获取变更日志的方式。由于这个原因，捕获数据库中所有的变更，然后将其复制到其他存储技术中是相当困难的。
 **变更数据捕获（change data capture, CDC）**：一种观察写入数据库的所有数据变更，并将其提取并转换为可以复制到其他系统中的形式的过程。CDC 是非常有意思的，尤其是当变更能在被写入后立刻用于流时。

![[Pasted image 20250902195827.png]]可以捕获数据库中的变更，并不断将相同的变更应用至搜索索引。如果变更日志以相同的顺序应用，则可以预期搜索索引中的数据与数据库中的数据是匹配的。

##### 变更数据捕获的实现

可以将日志消费者叫做 **派生数据系统**，存储在搜索索引和数据仓库中的数据，只是 **记录系统** 数据的额外视图。变更数据捕获是一种机制，可确保对记录系统所做的所有更改都反映在派生数据系统中，以便派生系统具有数据的准确副本。

从本质上说，变更数据捕获使得一个数据库成为领导者（被捕获变化的数据库），并将其他组件变为追随者。基于日志的消息代理非常适合从源数据库传输变更事件，因为它保留了消息的顺序。

类似于消息代理，变更数据捕获通常是异步的：记录数据库系统在提交变更之前不会等待消费者应用变更。这种设计具有的运维优势是，添加缓慢的消费者不会过度影响记录系统。

##### 初始快照

如果你拥有 **所有** 对数据库进行变更的日志，则可以通过重播该日志，来重建数据库的完整状态。但是在许多情况下，永远保留所有更改会耗费太多磁盘空间，且重播过于费时，因此日志需要被截断。

数据库的快照必须与变更日志中的已知位置或偏移量相对应，以便在处理完快照后知道从哪里开始应用变更。一些 CDC 工具集成了这种快照功能，而其他工具则把它留给你手动执行。

##### 日志压缩

如果你只能保留有限的历史日志，则每次要添加新的派生数据系统时，都需要做一次快照。但 **日志压缩（log compaction）** 提供了一个很好的备选方案。

在日志结构存储引擎中，具有特殊值 NULL（**墓碑**，即 tombstone）的更新表示该键被删除，并会在日志压缩过程中被移除。但只要键不被覆盖或删除，它就会永远留在日志中。

Apache Kafka 支持这种日志压缩功能。正如我们将在本章后面看到的，它允许消息代理被当成持久性存储使用，而不仅仅是用于临时消息。

##### 变更流的API支持

越来越多的数据库开始将变更流作为第一等的接口，而不像传统上要去做加装改造，或者费工夫逆向工程一个 CDC。

VoltDB 允许事务以流的形式连续地从数据库中导出数据。数据库将关系数据模型中的输出流表示为一个表，事务可以向其中插入元组，但不能查询。已提交事务按照提交顺序写入这个特殊表，而流则由该表中的元组日志构成。外部消费者可以异步消费该日志，并使用它来更新派生数据系统。

#### 事件溯源

与变更数据捕获类似，事件溯源涉及到 **将所有对应用状态的变更** 存储为变更事件日志。最大的区别是事件溯源将这一想法应用到了一个不同的抽象层次上：
- 在变更数据捕获中，应用以 **可变方式（mutable way）** 使用数据库，可以任意更新和删除记录。
- 在事件溯源中，应用逻辑显式构建在写入事件日志的不可变事件之上。在这种情况下，事件存储是仅追加写入的，更新与删除是不鼓励的或禁止的。事件被设计为旨在反映应用层面发生的事情，而不是底层的状态变更。

事件溯源是一种强大的数据建模技术：从应用的角度来看，将用户的行为记录为不可变的事件更有意义，而不是在可变数据库中记录这些行为的影响。事件溯源使得应用随时间演化更为容易，通过更容易理解事情发生的原因来帮助调试的进行，并有利于防止应用 Bug

事件溯源类似于 **编年史（chronicle）** 数据模型，事件日志与星型模式中的事实表之间也存在相似之处。

##### 从事件日志中派生出当前状态

事件日志本身并不是很有用，因为用户通常期望看到的是系统的当前状态，而不是变更历史。

因此，使用事件溯源的应用需要拉取事件日志（表示 **写入** 系统的数据），并将其转换为适合向用户显示的应用状态

与变更数据捕获一样，重播事件日志允许让你重新构建系统的当前状态。不过，日志压缩需要采用不同的方式处理：
- 用于记录更新的 CDC 事件通常包含记录的 **完整新版本**，因此主键的当前值完全由该主键的最近事件确定，而日志压缩可以丢弃相同主键的先前事件。
- 后面的事件通常不会覆盖先前的事件，所以你需要完整的历史事件来重新构建最终状态。这里进行同样的日志压缩是不可能的。

##### 命令和事件

事件溯源的哲学是仔细区分 **事件（event）** 和 **命令（command）**。当来自用户的请求刚到达时，它一开始是一个命令：在这个时间点上它仍然可能失败，比如，因为违反了一些完整性条件。应用必须首先验证它是否可以执行该命令。如果验证成功并且命令被接受，则它变为一个持久化且不可变的事件。

在事件生成的时刻，它就成为了 **事实（fact）**。

#### 状态、流和不变性

批处理因其输入文件不变性而受益良多，你可以在现有输入文件上运行实验性处理作业，而不用担心损坏它们。这种不变性原则也是使得事件溯源与变更数据捕获如此强大的原因。

无论状态如何变化，总是有一系列事件导致了这些变化。即使事情已经执行与回滚，这些事件出现是始终成立的。关键的想法是：可变的状态与不可变事件的仅追加日志相互之间并不矛盾：它们是一体两面，互为阴阳的。所有变化的日志 —— **变化日志（changelog）**，表示了随时间演变的状态。

应用状态是事件流对时间求积分得到的结果，而变更流是状态对时间求微分的结果
![[Pasted image 20250902201359.png]]
##### 不可变事件的优点

会计在几个世纪以来一直在财务记账中应用不变性。会计师不会删除或更改分类帐中的错误交易 —— 而是添加另一笔交易以补偿错误，例如退还一笔不正确的费用。不正确的交易将永远保留在分类帐中，对于审计而言可能非常重要。

##### 从同一事件日志中派生多个视图

添加从事件日志到数据库的显式转换，能够使应用更容易地随时间演进：如果你想要引入一个新功能，以新的方式表示现有数据，则可以使用事件日志来构建一个单独的、针对新功能的读取优化视图，无需修改现有系统而与之共存。并行运行新旧系统通常比在现有系统中执行复杂的模式迁移更容易。

通过将数据写入的形式与读取形式相分离，并允许几个不同的读取视图，你能获得很大的灵活性。这个想法有时被称为 **命令查询责任分离（command query responsibility segregation, CQRS）**

数据库和模式设计的传统方法是基于这样一种谬论，数据必须以与查询相同的形式写入。如果可以将数据从针对写入优化的事件日志转换为针对读取优化的应用状态，那么有关规范化和非规范化的争论就变得无关紧要了

##### 并发控制

事件溯源和变更数据捕获的最大缺点是，事件日志的消费者通常是异步的，所以可能会出现这样的情况：用户会写入日志，然后从日志派生视图中读取，结果发现他的写入还没有反映在读取视图中。

一种解决方案是将事件追加到日志时同步执行读取视图的更新。而将这些写入操作合并为一个原子单元需要 **事务**，所以要么将事件日志和读取视图保存在同一个存储系统中，要么就需要跨不同系统进行分布式事务。

另一方面，从事件日志导出当前状态也简化了并发控制的某些部分。许多对于多对象事务的需求源于单个用户操作需要在多个不同的位置更改数据。可以设计一个自包含的事件以表示一个用户操作。然后用户操作就只需要在一个地方进行单次写入操作 —— 即将事件附加到日志中 —— 这个还是很容易使原子化的。

##### 不变性的局限性

许多不使用事件溯源模型的系统也还是依赖不可变性：各种数据库在内部使用不可变的数据结构或多版本数据来支持时间点快照。

永远保持所有变更的不变历史，在多大程度上是可行的？答案取决于数据集的流失率。一些工作负载主要是添加数据，很少更新或删除；它们很容易保持不变。其他工作负载在相对较小的数据集上有较高的更新 / 删除率；在这些情况下，不可变的历史可能增至难以接受的巨大，碎片化可能成为一个问题，压缩与垃圾收集的表现对于运维的稳健性变得至关重要

除了性能方面的原因外，也可能有出于管理方面的原因需要删除数据的情况，尽管这些数据都是不可变的。在这种情况下，仅仅在日志中添加另一个事件来指明先前的数据应该被视为删除是不够的 —— 你实际上是想改写历史，并假装数据从一开始就没有写入。

### 流处理

可以用流做什么：
1. 你可以将事件中的数据写入数据库、缓存、搜索索引或类似的存储系统，然后能被其他客户端查询。
2. 能以某种方式将事件推送给用户，例如发送报警邮件或推送通知，或将事件流式传输到可实时显示的仪表板上。
3. 可以处理一个或多个输入流，并产生一个或多个输出流。流可能会经过由几个这样的处理阶段组成的流水线，最后再输出

将讨论选项 3：处理流以产生其他派生流。处理这样的流的代码片段，被称为 **算子（operator）** 或 **作业（job）**。和 MapReduce 作业密切相关，数据流的模式是相似的：一个流处理器以只读的方式使用输入流，并将其输出以仅追加的方式写入一个不同的位置。分区和并行化模式也非常类似。

与批量作业相比的一个关键区别是，流不会结束。这种差异会带来很多隐含的结果。正如本章开始部分所讨论的，排序对无界数据集没有意义，因此无法使用 **排序合并连接**。**容错机制**也必须改变：对于已经运行了几分钟的批处理作业，可以简单地从头开始重启失败任务，但是对于已经运行数年的流作业，重启后从头开始跑可能并不是一个可行的选项。

#### 流处理的应用

长期以来，流处理一直用于监控目的，如果某个事件发生，组织希望能得到警报。例如：
- 欺诈检测系统需要确定信用卡的使用模式是否有意外地变化，如果信用卡可能已被盗刷，则锁卡。
- 交易系统需要检查金融市场的价格变化，并根据指定的规则进行交易。
- 军事和情报系统需要跟踪潜在侵略者的活动，并在出现袭击征兆时发出警报。

##### 复合事件处理

**复合事件处理（complex event processing, CEP）** 是 20 世纪 90 年代为分析事件流而开发出的一种方法，尤其适用于需要搜索某些事件模式的应用。



