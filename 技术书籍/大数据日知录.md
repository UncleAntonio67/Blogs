作者： 张俊林                        
阅读日期：20250904——20250930
学习方式：书本阅读

# 前言

本书专注于与大数据处理有关的架构与算法。

# 目录

第0章——当谈论大数据时我们在谈什么
第1章——数据分片与路由
第2章——数据复制与一致性
第3章——大数据常用的算法与数据结构
第4章——集群资源管理与调度
第5章——分布式协调系统
第6章——分布式通信
第7章——数据通道
第8章——分布式文件系统
第9章——内存KV数据库
第10章——列式数据库
第11章——大规模批处理系统
第12章——流式计算
第13章——交互式数据分析
第14章——图数据库：架构与算法
第15章——机器学习：范型与架构
第16章——机器学习：分布式算法
第17章——增量计算

# 第0章——当谈论大数据时我们在谈什么

## 大数据是什么

多大才算大：数据量的衡量单位，从小到大依次为KB、MB、GB、TB、PB、EP和ZB

电子数据正在暴涨：2010年全世界信息总量是1ZB，最近3年人类产生的信息量已经超过了之前历史上人类产生的所有信息之和。

大数据的定义：
- 维基百科：数据量太大，手头的工具已经不便于管理
- IBM：3V（Volume、Velocity、Variety）+1V（Value）
- IDC：代表新一代的技术脚骨，能够高速获取数据进行分析挖掘，抽取价值信息
- Google：海量数据可广泛获得，所稀缺的是如何从中挖掘出智慧和观点。

## 大数据之翼：技术范型转换

关系型数据库——>并行数据库——>NoSql数据库

整体架构：
——>数据源
来源各异，形式不规整
——>数据管理
NoSQL不追求应用场景的统一，而且不同类型不同NoSQL库存储管理
——>数据分析
挖掘分析，利用数据挖掘、机器学习、时序分析
——>数据获取
数据可视化展现

大数据技术处理架构图
![[Pasted image 20250904182932.jpg]]

## 大数据商业炼金术

互联网、传统IT、金融、零售行业都在利于大数据提升企业收益

## 大数据在路上

概念最早由麦肯锡提出，巨型公司提供基础架构平台，中小型创业公司完善分布式计算生态系统，或者提供大数据服务等。

# 第1章——数据分片与路由

数据规模越来越大：
- 并行数据库通过纵向扩展解决问题（Scale Up），扩展机器
- 大数据系统通过横向扩展解决问题（Scale Out），增加机器

增加机器就带来了**数据分片**，分片后就必须找到数据，即**数据路由**。

- 数据分片：实现水平扩展
- 数据复制：保证数据高可用，因为服务器经常存在故障，所以需要存储多份副本

数据复制面临的问题：并发对数据更新时，如何保证数据的一致性。

常见的数据分片方法包括哈希分片与范围分片，先讲通用模型再将具体实现。

## 抽象模型

二级映射：第一级：key-partition，一个分片包含多个数据记录；第二级：partition-machine，一个物理机器容纳多个数据分片。

![[Pasted image 20250904184458.jpg]]

在做数据分片时，根据key-partition映射关系将大数据水平切割成众多数据分片，然后再按照partition-machine映射关系将数据分片放置到对应的物理机器上。

- 哈希分片：主要通过哈希函数来建立key-partition映射关系，**点查询**，不支持范围查询。Dynamo、Cassandra、Riak、Voldmort、Membase等都支持
- 范围分片：**支持点查询也可以支持范围查询**，包括Google的BigTable和微软的Azure等系统

## 哈希分片

通过哈希函数分片，有三种哈希分片方式：Round Robin、虚拟桶及一致性哈希方法
### **Round Robin**
就是哈希取模法，哈希函数：H(key) = hash(key) mod K。机器编号0-K-1，全部数据分配到K台物理机。

- 优点：非常简单
- 缺点：灵活性差，增加一个节点，就需要重分布

它是物理机和数据分片的二合一，机器和映射函数紧耦合，这是缺乏扩展灵活性的根本原因。

### 虚拟桶

MemBase对于数据分片管理提出了虚拟桶的实现方式：
记录和物理机引入虚拟桶，数据线到桶，再到物理机，都是多对一的映射。

虚拟桶层就是数据分片层，key-partition映射采用哈希函数，而partition-machine映射采用表格管理实现。
![[Pasted image 20250905104631.jpg]]
系统灵活性、扩展性更强

### 一致性哈希

分布式哈希表：哈希表的分布式扩展。
一致性哈希就是分布式哈希的一种实现方式。

“一致性哈希”算法将哈希数值空间按照大小组成一个首尾相接的环状序列。对于每台机器，可以根据其IP和端口号经过哈希函数映射到哈希数值空间内，这样不同的机器就成了环状序列中的不同节点。
![[Pasted image 20250908145043.jpg]]
**路由方面**：可以在每个机器节点配置路由表，路由表存储m 条路由信息，其中第i 项（0≤i ≤m −1）路由信息代表距离当前节点为2 i 的哈希空间数值所在的机器节点编号。
通常情况下，路由算法发送的消息不会多于m 条，因为这个过程类似于在0～(2 m −1)数值空间上的二分查找法。

**扩展方面**：首先增加节点对应的前驱和后继节点关系，然后还需要进行数据重分布。并发情况下需要进行稳定性检测。

**离开方面**：正常离开进行通知准备，进行数据重分布，异常离开通过数据多副本保障。

**虚拟节点**：两个问题，节点到环状结构的位置随机，容易存在负载不均衡的问题，另外不同机器间的性能也不同。所以引入虚拟节点，即将一个物理节点虚拟成若干虚拟节点，分别映射到一致性哈希的环状结构不同位置。这样一方面可以导致更佳的负载均衡，也可以兼顾到机器异质性问题。

**一致性哈希**：将集群机器数目这一变量从哈希函数中移出，转而将机器及记录主键都映射到哈希数值空间，解除了机器与数据分布函数之间的直接耦合。大大增强了数据分片的灵活性，但是维护成本很高。

### 范围分片

范围分片首先将所有记录的主键进行排序，然后在排好序的主键空间里将记录划分成数据分片，每个数据分片存储有序的主键空间片段内的所有记录。至于数据分片在物理机的管理方式往往采用LSM树，这是一种高效写入的数据索引结构。

![[Pasted image 20250908175559.jpg]]

很多大规模存储系统都支持上述范围分片模式，比如Yahoo的PNUTS和微软的Azure。Google的BigTable也基本遵循上述模式，不同点在于其数据分片映射表不是单层结构，而是组织成类似B+树的层次结构。

# 第2章——数据复制与一致性协议

大数据领域，增加系统高可用，即将数据进行多副本存储，但是多副本会带来兵法写入的一致性问题。

## 基本原则与设计理念

CAP、BASE、ACID理论

### 原教旨CAP主义

CAP是对“Consistency/Availability/Partition Tolerance”：
- **强一致性**：在分布式系统中的同一数据多副本情形下，对于数据的更新操作体现出的效果与只有单份数据是一样的。
- **可用性**：客户端在任何时刻对大规模数据系统的读／写操作都应该保证在限定延时内完成。
- **分区容忍性**：网络分区通讯有问题，仍然能够继续工作。

1999年Eric Brewer提出该理论，只能实现三个中的两个，一般不舍弃P，故只有AP或CP.

![[Pasted image 20250908180836.jpg]]
- 传统的关系数据库在三要素中选择CA两个因素，即强一致性、高可用性，但是可扩展性与容错性差。
 - NoSQL系统往往更关注AP因素，即高可扩展性和高可用性，但是往往以弱一致性作为代价.


2012年，Eric Brewer又提出P很低概率出现，正常情况下还是要兼顾CAP，在进行差异化、细粒度的CA取舍。修正后的示意图如下，在过程中满足CAP、AP、CAP因素：
![[Pasted image 20250908181729.jpg]]

### ACID原则

- **原子性** （Atomicity）：是指一个事务要么全部执行，要么完全不执行。也就是不允许一个事务只执行了一半就停止。
- **一致性** （Consistency）：事务在开始和结束时，应该始终满足一致性约束条件。
- **事务独立** （Isolation）：如果有多个事务同时执行，彼此之间不需要知晓对方的存在，而且执行时互不影响。
- **持久性** （Durability）：事务的持久性是指事务运行成功以后，对系统状态的更新是永久的，不会无缘由地回滚撤销。

### BASE原则

大多数大数据环境下的云存储系统和NoSQL系统则采纳BASE原则

- **基本可用** （Basically Available）。在绝大多数时间内系统处于可用状态，允许偶尔的失败，所 以称为基本可用。
- **软状态或者柔性状态** （Soft State），是指数据状态不要求在任意时刻都完全保持同步，到目前为止软状态并无一个统一明晰的定义，但是从概念上是可理解的，即处于有状态（State）和无状态（Stateless）之间的中间状态。
- **最终一致性** （Eventual Consistency）。与强一致性相比，最终一致性是一种弱一致性，尽管软状态不要求任意时刻数据保持一致同步，但是最终一致性要求在给定时间窗口内数据会达到一致状态。

### CAP/ACID/BASE三者关系

ACID强调一致性，BASE强调可用性，弱化一致性。CAP和ACID的区别：
- ACID的C是操作一致性约束，CAP的C是数据的强一致性。CAP中的C是ACID中的C所涵盖语义的子集。
- 出现网络分区后，ACID的I只能在某个分区执行
- 当出现网络分区时，多个分区都可以各自进行ACID中的数据持久化（D）操作。

### 幂等性

**分布式的幂等性**：调用方反复执行同一操作与只正确执行一次操作效果相同，即对分布式系统内部状态来说，同一操作调用一次与反复调用多次其状态保持相同。

## 一致性模型分类

理想情况下就只有强一致性
为了在分布式环境下追求高可用和高扩展，采用弱一致性模型，很多NoSQL系统采用弱一致性模型

一致性模型关系图：
![[Pasted image 20250909192340.jpg]]

### 强一致性

更新后所有的读都是更新的值

![[Pasted image 20250909192434.jpg]]
### 最终一致性

无法保证强一致性的时间片段被称为“不一致窗口”，这个窗口可能会看到旧的数值
![[Pasted image 20250909192520.jpg]]

### 因果一致性

有因果依赖的，保证数据的因果一致性，但不一致窗口内仍然会看到旧值
![[Pasted image 20250909192650.jpg]]

### 读你所写一致性

“读你所写”一致性是因果一致性的特例
![[Pasted image 20250909192812.jpg]]

### 会话一致性

读你所写的变体：回话一致性
![[Pasted image 20250909193509.jpg]]

### 单调读一致性

读到一次最新的，后面就都是最新的
![[Pasted image 20250909193611.jpg]]

### 单调写一致性

单调写一致性可以保证其多次写操作的序列化，如果没有这种保证，对于应用开发者来说是很难进行程序开发的。

## 副本更新策略

分布式存储下，数据冗余增加可用性，也增加读操作的并发性。但是一致性问题如何解决呢？

### 同时更新

具体又有两种类型：
- A：不通过任何一致性协议直接同时更新多个副本数据。会有潜在的一致性问题，多个update并发执行
- B：通过某种一致性协议预先处理，有处理成本，所以请求延时会有所增加。

### 主从式更新

所有的更新操作都先提到主副本，主副本再去更新从副本。根据主副本通知从副本的不同机制来区分，存在以下3种类型。
- **同步方式**：主副本等待所有从副本更新完成之后才确认更新操作完成，这可以确保数据的强一致性，但是会存在较大请求延时。
- **异步方式**：主副本在通知从副本更新之前即可确认更新操作。按照读操作又可以分为如下两类：
	- 如果所有读请求都要通过主副本来响应，即任意一个副本接收到读请求后将其转发给主副本；可以保障强一致性，但是会有请求延时。
	- 如果任意一个副本都可以响应读请求，那么请求延时将会大大降低，但是这可能导致读结果不一致的问题。
- **混合方式**：同步和异步混合起来用，按照读操作又可以分为如下两类：
	- 如果读操作的数据至少要从一个同步更新的节点中读出，比如类似于RWN协议的R+W>N，可以保证强一致性，会有请求延时。
	- 如果读操作不要求一定要从至少一个同步更新节点中读出，即RWN协议中的R+W<=N 的模式，会有不一致问题。

### 任意节点更新

数据更新请求可能发给多副本中的任意一个节点，然后由这个节点来负责通知其他副本进行数据更新。有可能有两个不同客户端在同一时刻对同一个数据发出数据更新请求，而此时有可能有两个不同副本各自响应。
- 类型A：同步通知其他副本：存在和“主从式更新”类型A相似的情况，延时更多
- 类型B：异步通知其他副本：存在和“同时更新”策略及“主从式更新”策略的类型B类似的问题。

## 一致性协议

### 两阶段提交协议2PC

两阶段提交协议是很常用的解决分布式事务问题的方式，它可以保证在分布式事务中，要么所有参与进程都提交事务，要么都取消事务，即实现ACID中的原子性（A）的常用手段。
2PC更多的是作为实现数据更新原子性手段出现。

存在两类不同实体：唯一的协调者（Coordinator）和众多的参与者（Participants）。协调者起到分布式事务的特殊的管理协调作用。
分为两个阶段：表决阶段（Voting）和提交阶段（Commit）。
- **表决阶段（Voting）**：协调者向所有参与者发送表决请求，参与者响应准备情况
- **提交阶段（Commit）**：协调者收集表决信息，如果参与者均认为可提交，则通知大家提交，如果有一个表决中止，通知大家取消。

![[file-20250910190231949.jpg|400]]
可以进一步转换为协调者和参与者的状态机：

![[file-20250910190813183.jpg|400]]![[file-20250910191009985.jpg|400]]
对于阻塞状态可以引入超时判断机制和参与者互询机制。
- 引入超时判断机制可以解决协调者的WAIT状态和参与者的INIT状态的长时阻塞情形
- 引入互询机制可以解决大部分情形下参与者READY状态的长时阻塞可能。

如果参与者Q是READY，P从Q无法获得更多信息，如果其它参与者都在READY，所有参与者必须长时间处于阻塞状态，等待崩溃的协调者重新启动。这种情形就是上文提到的**2PC无法解决的一种长时阻塞状态**。

三阶段提交协议（3PC）是学术界提出的用来解决2PC协议存在长时阻塞的办法，其核心思想是将2PC的提交阶段再次细分为两个阶段：预提交阶段和提交阶段。

![[file-20250910191727851.jpg|400]]
![[file-20250910191735909.jpg|400]]

该协议的本质在于通过引入PRECOMMIT状态，使得协调者和每个参与者都满足以下两个条件。
**条件一**：没有一个可以直接转换到COMMIT或者ABORT状态的单独状态（2PC中，协调者的WAIT状态和参与者的READY状态就是这种单独状态）。
**条件二**：不存在这样一个状态，即它不能做出最后决定，而且可以从它直接转到COMMIT状态（2PC中，协调者的WAIT状态和参与者的READY状态就是这种状态。3PC中的PRECOMMIT状态可直接转到COMMIT状态，但是它已经做出了提交决定）。

这两个条件是使得提交协议不阻塞的充要条件。3PC在实际系统中很少使用，一方面是由于2PC中长时阻塞情况很少发生，另外一方面是3PC效率过低。

### 向量时钟

向量时钟是在分布式环境下生成事件之间偏序关系的算法，偏序关系代表事件发生先后顺序导致的事件间因果依赖关系语义，通过将时间戳和事件绑定可以用来判定事件之间的因果相关性。

设分布式系统有A、B和C这3个进程，根据上述规则其各自对应的逻辑时钟随着时间演化情况如图所示。
![[file-20250910193411653.jpg|600]]
向量时钟的典型应用场景是用来判断分布式环境下不同事件之间是否存在因果关系。对于两个事件E
 和F，假设其各自的向量时钟分别是E.VC和F.VC，我们可以根据如下方法判断其是否存在因果关系：
 ![[file-20250910193536308.jpg]]
 即如果事件E的时钟向量各个维度的数值都小于等于事件F对应位置的数值且至少有一位是小于，那么可以称为事件E是事件F的原因，事件F是事件E的结果。

Dynamo中使用向量时钟进行数据版本管理，配合RWN协议共同完成数据一致性维护。

### RWN协议

“RWN协议”是亚马逊公司在实现Dynamo KV存储系统时提出的。这是一种通过对分布式环境
 下多备份数据如何读／写成功进行配置来保证达到数据一致性的简明分析和约束设置。
- N：在分布式存储系统中，有多少份备份数据。
- W：代表一次成功的更新操作要求至少有W份数据写入成功。
- R：代表一次成功的读操作要求至少有R份数据成功读取。

如果R+W>N，则满足“数据一致性协议”。成功写入的备份集合和成功读取的备份集合一定会存在交集，而这就可以保证数据的强一致性，即读取操作一定可以读到最新的数据版本。
![[file-20250910194226349.jpg|400]]

在满足数据一致性协议的前提下，R或者W设置得越大，则系统延迟越大，因为这取决于最慢的那份备份数据的响应时间。而如果R+W <=N，则无法保证数据的强一致性.

在具体实现系统时，仅仅依靠RWN协议还不能完成一致性保证，因为在上述过程中，当读取到多个备份数据时，需要判断哪些数据是最新的，如何判断数据的新旧？这需要向量时钟来配合。

### Paxos协议

“所有一致性协议本质上要么是Paxos，要么是其变体”

Paxos的难理解性在于是什么因素导致协议以此种方式呈现以及其正确性证明过程而非最终协议内容本身.

首先介绍副本状态机模型，之后介绍Paxos的一些基本概念，然后描述Paxos协议本身内容。

**副本状态机模型**（Replicated State Machines）

在分布式环境下，一致性协议的应用场景一般会采用副本状态机来表达，这是对各种不同应用场景的一种抽象化表述。

![[file-20250910195324044.jpg|500]]
集群中多台服务器各自保存一份Log副本及内部状态机，Log内顺序记载客户端发来的操作指令，服务器依次执行Log内的指令并将其体现到内部状态机上，如果保证每台机器内的Log副本内容完全一致，那么对应的状态机也可以保证整体状态一致。一致性协议的作用就是保证各个Log副本数据的一致性。

追求三个特性：
- 安全性（Safety）保证：即非拜占庭模型（此概念参考后面的内容）下，状态机从不返回错误的结果，多个提议中只会有一个被选中。
- 可用性（Available）保证：只要大多数服务器正常，则整个服务保持可用。比如副本状态机有5台服务器，那么最多可以容忍2台服务器发生故障，此时整个服务仍然可用，即对于2f+1台副本状态机的配置，最多可容忍f个状态机失效。
- 一般情况下，大多数状态机维护Log一致即可快速通知客户端操作成功，这样避免了少数最慢的状态机拖慢整个请求响应速度。

**Paxos基本概念

分为两种：单Paxos（Single-Decree Paxos）和多Paxos（Multi-Paxos）
- 单Paxos，即副本状态机中各个服务器 针对Log中固定某个位置的操作命令通过协议达成一致，因为可能某一时刻不同服务器的Log中相同位置的操作命令是不一样的，通过执行协议后使得各个服务器对应某个固定位置的操作命令达成一致。
- 多Paxos则是指这些服务器对应的Log内容中多个位置的操作命令序列通过协议保持一致。多Paxos往往是同时运行的多个单Paxos协议共同执行的结果。

Paxos协议下不同并行进程可能承担的3种角色如下：
- **倡议者**：倡议者可以提出提议（数值或操作命令等）以供投票表决。
- **接受者**：接受者可以对倡议者提出的提议进行投票表决，从众多提议中选出唯一确定的一个。
- **学习者**：学习者无倡议投票权，但是可以从接受者那里获知是哪个提议最终被选中。

在一致性协议框架中，一个并行进程可以同时承担以上多种角色。

Paxos协议以及很多一致性协议都是基于非拜占庭模型的，即在非拜占庭条件下，Paxos协议可以就不同提议达成一致，而在拜占庭模型下情况会更加复杂一些。

**Paxos一致性协议
- **阶段一：**
	1. 【倡议者视角】倡议者选择倡议编号n，然后向大多数（即超过半数以上）接受者发送Prepare请求，请求中附带倡议编号n
	2. 【接受者视角】对于某个接受者来说，如果接收到带有倡议编号n的Prepare请求，则做如下判断：若倡议编号n比此接受者之前响应过的任何其他Prepare请求附带的倡议编号都大，那么此接受者会给倡议者以响应，并承诺不会响应之后接收到的其他任何倡议编号小于n的请求，另外，如果接受者曾经响应过2.2阶段的Accept请求，则将所有响应的Accept请求中倡议编号最高的倡议内容发送给倡议者，倡议内容包括两项信息：Accept请求中的倡议编号以及其倡议值。若倡议编号n不比此接受者之前响应过的任何其他Prepare请求附带的倡议编号都大，那么此接受者不会给倡议者以响应。
- **阶段二：**
	1. 【倡议者视角】如果倡议者接收到大多数接受者关于带有倡议编号n的Prepare请求的响应，那么倡议者向这些接受者发送Accept请求，Accept请求附带两个信息：倡议编号n以及倡议值v。倡议值v的选择方式如下：如果在1.2阶段接受者返回了自己曾经接收的具有最高倡议编号Accept请求倡议内容，则从这些倡议内容里面选择倡议编号最高的并将其倡议值作为倡议值v；如果1.2阶段没有收到任何接受者的Accept请求倡议内容，则可以任意赋值给倡议值v。
	2. 【接受者视角】如果接受者接收到了任意倡议编号为n的Accept请求，则接受者接受此请求，除非在此期间接受者响应过具有比n更高编号的Prepare请求。

对于学习者来说，其需要从接受者那里获知到底是哪个倡议值被选出。一个直观的方法如下：每当接受者执行完2.2步骤，即接受某个Accept请求后，由其通知所有学习者其所接受的倡议，这样，学习者很快习得是哪个倡议被最终选出。有通讯成本，可以选出若干学习者作为代表，由这些代表从接受者那里获知最终倡议值，然后通知其他学习者。

通过以上流程，如果有多个并发进程提出各自的倡议值，Paxos就可以保证从中选出且只选出一个唯一确定的倡议值，以此来达到副本状态机保持状态一致的目标。

### Raft协议

Raft一致性协议最主要的目标有两个：
 - 首先是可理解性，在做技术决策和选型的时候，在达到相似功能前提下，首先以易于理解作为选型标准；
 - 其次是实现实际系统的确定性，鉴于之前提到的根据Paxos实现具体系统时的不统一，Raft追求每个技术细节的清晰界定与描述，以此达到实现具体系统时的明确性。

为了达到上述两个目的，主要采取了以下两个手段：
- 将整个一致性协议划分成明确且独立的3个子问题，即采取分解法。Raft将整个一致性协议划分为领导者选举、Log复制与安全性3个问题。
- 将Paxos的P2P模式改造为Master-Slave模式。Paxos的复杂性很大原因是由于其完全的P2P模式造成的，即多个并发进程之间无主次关系，都具有同等地位。

**Raft基本概念

首先是服务器状态，在任意时刻，集群中的服务器只能处于以下3种状态之一：Leader、Follower和Candidate。正常情况下，集群中只有一个处于Leader状态的服务器充当领导者，由其来负责响应所有客户端请求，其他服务器都处于Follower状态。处于Follower状态的服务器都是被动接收RPC消息，从不会主动发送任何RPC消息。Candidate状态是Follower状态服务器准备发起新的领导者选举前需要转换到的状态，即Candidate状态是Follower向Leader状态转换的中间状态。

![[file-20250910201124581.jpg]]

Raft将整个系统执行时间划分为由若干个不同时间间隔长度的时间片段构成的序列，每个时间片段被称为一个Term，以递增的数字来作为这个Term的标识。每个Term由“选举期间”（Election）开始，在这个时间内若干处于Candidate状态的服务器试图竞争成为新的领导者。如果某个服务器赢得了选举，则在这个Term接下来的时间里充当新的领导者。

![[file-20250910201216917.jpg]]


**Raft一致性协议

- **领导者选举**：Raft采用心跳机制来触发领导者选举过程。当整个系统启动时，所有服务器处于Follower状态，除非服务器接收到处于Leader或者Candidate状态服务器发出的RPC命令，否则其一直维持这个状态不变。Leader通过周期性地向其他服务器发送心跳来宣告并保持其领导者地位。
	在开始选举前，Follower增加其Term编号并转入Candidate状态。然后其向集群内所有其他服务器发出RequestVote RPC消息，之后一直处于Candidate状态，除非以下情况之一发生。
	- 赢得了本次选举
	- 另外一个服务器S宣称并确认自己是新的领导者；
	- 经过一定时间后，仍然没有新的领导者产生
- **Log复制**：当选出领导者后，之后所有客户端请求都由领导者来负责响应。领导者接收到客户端的操作命令后，将其作为新项目追加到Log尾部，然后向集群内所有其他服务器发出AppendEntries RPC请求，这引发其他服务器复制新的操作命令。当其他服务器安全复制了新的操作命令后，领导者将这个操作命令应用到内部状态机，并将执行结果返回给客户端。
	![[file-20250910201547474.jpg|400]]
- **安全性**：以上两个步骤，在一般情形下Raft已经可以正常运行，但是目前Raft还无法做到完全的安全性保证，即无法保证每个服务器的状态机都能够按照相同顺序执行相同操作命令。为了达到真正的安全性，Raft增加了如下两个约束条件。
	- 限制了哪些服务器可以被选举成为领导者，其要求只有其Log包含了所有已经提交的操作命令的那些服务器才有权被选举为新的领导者
	- 限制了哪些操作命令的提交可以被认为是真正的提交。对于新领导者来说，只有它自己已经提交过当前Term的操作命令才被认为是真正提交。

# 第3章——大数据常用的算法与数据结构

## 布隆过滤器

Bloom Filter（为了表达方便，后文简称BF）就是常说的布隆过滤器，是由Howard Bloom在1970年提出的二进制向量数据结构，它具有很好的空间和时间效率，尤其是空间效率极高，BF常常被用来检测某个元素是否是巨量数据集合中的成员。
### 基本原理

BF可以高效地表征集合数据，其使用长度为m的位数组来存储集合信息，同时使用k个相互独立的哈希函数将数据映射到位数组空间。

基本思想：

首先，将长度为m的位数组元素全部置为0。对于集合S中的某个成员a，分别使用k个哈希函数对其计算，如果hi(a)=x(1≤i≤k,1≤x≤m)，则将位数组的第x位置为1。

当查询某个成员a 是否在集合S中出现时，使用相同的k个哈希函数计算，如果其对应位数组中的w位（w≤k）都为1，则判断成员a属于集合S，只要w位中有任意一位为0，则判断成员a不属于集合S

### 误判率及相关计算

如果某个成员不在集合中，有可能BF会得出其在集合中的结论。

![[file-20250916194423371.jpg | 400]]

BF会产生误判，但是**不会发生漏判**（False Negative）的情况，即如果某个成员确实属于集合，那么BF一定能够给出正确判断。**在的肯定在，不在的不一定不在**

影响误判率的因素：包括集合大小n、哈希函数的个数k、 和位数组大小m。![[file-20250916194659885.jpg]]

### 改进：计数Bloom filter

基本的BF在使用时有个缺点：无法删除集合成员，只能增加成员并对其查询

计数BF的思路：基本信息单元由多个比特位来表示，一般情况采取3或4比特位为单元。这样，将集合成员加入位数组时，根据k个哈希函数计算，此时对应位置的信息单元由多个比特位构成，所以将原先的数值加1即可。查询集合成员时，只要对应位置的信息单元都不为0即可认为该成员属于集合。而删除成员，只要将对应位置的计数减1即可。

存在计数溢出的可能

### 应用

因为BF的极高空间利用率，其在各个领域获得了非常广泛的使用，尤其是数据量极大且容忍一定误判率的场合。

在BigTable中，BF对于读操作的效率提升有巨大帮助。BigTable中很多数据记录存储在磁盘的多个SSTable文件中，为了完成一次读操作，需要依次在这些SSTable中查找指定的Key，因为是磁盘操作且涉及多个文件，所以会对读操作效率有极大影响。BigTable将SSTable文件中包含的数据记录Key形成BF结构并将其放入内存，这样就能极高地提高查询速度，对于改善读操作有巨大的帮助作用。

## SkipList

SkipList由William Pugh于1990年提出，这是一种可替代平衡树的数据结构。其插入、删除、查找数据的时间复杂度都是O(log(N))

**核心思路**：

- 传统有序链表
![[file-20250916195612225.jpg]]
- 增加指针后的有序链表
![[file-20250916195653158.jpg]]
多跳几次：
![[file-20250916195659542.jpg]]

SkipList**依赖随机数**来以一定概率保持数据的平衡，具体而言，就是**在插入节点的时候，随机决定**该节点应该有多少个指向后续节点的指针，有几个指针就称这个节点是几层的（Level）。

插入过程：
![[file-20250916195931890.jpg]]

## LSM树

LSM树（Log-structured Merge-tree）的本质是将大量的随机写操作转换成批量的序列写，这样可以极大地提升磁盘数据写入速度，所以LSM树非常适合对写操作效率有高要求的应用场景。

LevelDB静态结构：
![[file-20250916200147293.jpg | 500]]

构成LevelDB静态结构的包括6个主要部分：内存中的MemTable和Immutable MemTable以及磁盘上的几种主要文件：Current文件、manifest文件、log文件以及SSTable文件。

- **写入**：LevelDB的log文件和MemTable与BigTable论文中介绍的是一致的，当应用写入一条Key：Value记录的时候，LevelDB会先往log文件里写入，成功后将记录插进MemTable中，这样基本就算完成了写入操作，因为一次写入操作只涉及一次磁盘顺序写和一次内存写入，**而且MemTable采用了维护有序记录快速插入查找的SkipList数据结构**，所以说LSM树是一种高速写入数据结构的主要原因。
- **到处**：当MemTable插入的数据占用内存到了一个界限后，需要将内存的记录导出到外存文件中，LevelDB会生成新的log文件和MemTable，原先的MemTable就成为Immutable MemTable，顾名思义，就是说这个MemTable的内容是不可更改的，只能读不能写入或者删除。新到来的数据被记入新的log文件和MemTable，LevelDB后台调度会将Immutable MemTable的数据导出到磁盘，形成一个新的SSTable文件。（SSTable有层次，故叫做LevelDB）
- **记录**：SSTable中的文件是主键有序的，也就是说，在文件中小key记录排在大key记录之前，各个Level的SSTable都是如此。manifest记载了SSTable各个文件的管理信息，比如属于哪个Level、文件名称、最小key和最大key各自是多少。![[file-20250916201110792.jpg | 400]]
- **合并**：3种类型的Compaction，分别是minor、major和full。所谓minor Compaction，就是把MemTable中的数据导出到SSTable文件中，major Compaction就是合并不同层级的SSTable文件，而full Compaction就是将所有SSTable进行合并。
	- **minor**：当MemTable中记录数量到了一定程度会转换为Immutable MemTable，此时不能往其中写入记录，只能从中读取KV内容。Immutable MemTable其实是一个SkipList多层级队列，其中的记录是根据key有序排列的。所以这个minor Compaction实现起来也很简单，就是按照Immutable MemTable中记录由小到大遍历，并依次写入一个Level 0的新建SSTable文件中，写完后建立文件的index数据，这样就完成了一次minor Compaction。
	- **major**：当某个Level下的SSTable文件数目超过一定设置值后，LevelDB会从这个Level的SSTable中选择一个文件（Level>0），将其和高一层级的Level+1的SSTable文件合并，这就是major Compaction。对多个文件采用多路归并排序的方式，依次找出其中最小的key记录，也就是对多个文件中的所有记录重新进行排序。之后采取一定的标准判断这个key是否还需要保存，如果判断没有保存价值，那么直接抛掉，如果觉得还需要继续保存，那么就将其写入Level L+1层中新生成的SSTable文件中

由内存中的MemTable和磁盘上的各级SSTable文件就形成了LSM树。
LSM树的本质，即将大量随机写转换为批量的序列写。

## Merkle哈希树

Merkle哈希树由Ralph Merkle于1979年发明。Merkle树最初用于高效Lamport签名验证，后来被广泛应用在分布式领域，主要用来在海量数据下快速定位少量变化的数据内容

### Merkle树基本原理
![[file-20250917210229248.jpg]]
子节点是每个数据项或者一批数据项（数据块）对应的哈希值，中间节点则保存对其所有子节点哈希值再次进行哈希运算后的值，依次由下往上类推，直到根节点，其保存的Top Hash代表整棵树的哈希值，也就是所有数据的整体哈希值。具体使用的时候，既可以像例子中一样是一个二叉树，也可以是多叉树。

Merkle树常用于快速侦测部分数据正常或者异常的变动。当某个底层数据发生变化时，其对应Merkle树的子节点哈希值会跟着变化，子节点的父节点哈希值也随之变化。

通过Merkle树，可以在O(log(n))时间内**快速定位变化**的数据内容。

### Dynamo中的应用

Dynamo结合Merkle树和Gossip协议来对副本数据进行同步。

Dynamo可以快速定位到数据副本不同内容，且只须同步两者的差异部分即可实现副本数据同步，这样有效地减少了网络传输数据量，增加了数据同步效率。

### 比特币中的应用

比特币中，主要使用Merkle树来对交易进行验证，以此来判断某个交易是否是合法交易。

通过引入Merkle树和上述的链表结构，比特币采用少量计算及比较操作即可完成交易的验证过程。

## Snappy和LZSS算法

Snappy是Google开源出的高效数据压缩与解压缩算法库，其目标并非是最高的数据压缩率，而是在合理的压缩率基础上追求尽可能快的压缩和解压缩速度。
- 其压缩和解压缩速度极快，可以在单核处理器上达到250MB/s的压缩效率和500MB/s的解压缩效率。
- Snappy相比其他压缩方案占用CPU时间更少。

Snappy是基于LZSS算法的

### LZSS算法

LZ77是一种动态词典编码（Dictionary Coding）

**基本思路**：文本中的词用它在词典中表示位置的号码代替的无损数据压缩方法，一般分为静态词典方法和动态词典方法两种。采用静态词典编码技术时，编码器需要事先构造词典，解码器要事先知道词典。采用动态辞典编码技术时，编码器将从被压缩的文本中自动导出词典，解码器解码时边解码边构造解码词典。

动态词典编码的基本思路：p替代了abc
![[file-20250917211111486.jpg | 500]]

**LZ77算法**：描述了一种基于滑动窗口缓存的技术，该缓存用于保存最近刚刚处理的文本，而动态词典就是由滑动窗口内的文本构造出来的。

LZ77是动态词典编码方法的开创者，后来所有动态词典编码压缩方法都是基于LZ77进行改造和优
化的，比如我们熟知的GZip、WinZip、RAR、Compress等都采用了LZ系列算法。

LZ77的压缩算法使用了滑动窗口和前向缓冲区的概念：
- 滑动窗口：前面处理过的若干源字符
- 前向缓冲区：包含了输入数据流中将要处理的所有后续字符。
![[file-20250917211500784.jpg | 500]]

LZSS对LZ77做出了改进：增加了最小匹配长度限制，当匹配字符串小于指定的最小匹配限制时，并不进行压缩输出，而是仍然滑动窗口右移一个字符。实例如下：

![[file-20250917211546710.jpg | 500]]
### Snappy

Snappy在整体框架上基本遵循LZSS的压缩编码与解码方案。首先，Snappy设定最小匹配长度为4，即只有匹配长度大于等于4的字符串才进行压缩，相应地，其设定哈希表内的字符串片段固定长度也为4。

Snappy做了一些相对独特的优化。比如其在压缩数据时，将整个数据切割成32KB大小的数据块分别进行压缩，数据块之间独立无关联，这样两个字节即可表示匹配字符串的相对位置。

## Cuckoo哈希

Cuckoo哈希由Rasmus Pagh和Flemming Friche Rodler于2001年提出，使用它可以有效解决哈希冲突（Hash Collisions）问题。Cuckoo哈希具有很多优良特性，比如可以在O（1）时间复杂度查找和删除数据，可以在常数时间内插入数据等。其有大约50%的哈希空间利用率。

### 基本原理

Cuckoo哈希同时使用两个不同的哈希函数H1(x )和H 2(x )。当插入数据x时，同时计算H1(x)和H2(x)，如果对应的哈希空间中任意一个桶（Bucket）为空，则可以将x插入相应位置；如果两者都不空，则选择一个桶，将已经占据这个位置的值y踢出去，由x来占据这个位置。y继续重新计算。

可能会无限循环，需要设定最大替换次数。

对于查找操作来说，只需要查找两个哈希函数映射到的哈希空间对应位置，要么存在要么不存在，是唯一确定的，所以可以在O(1)时间内完成。与传统的哈希方式相比较，Cuckoo哈希省去了当哈希冲突时进行冲突解决的过程，所以查找效率非常高。

# 第4章 集群资源管理与调度



