作者： 张俊林                        
阅读日期：20250904——20250930
学习方式：书本阅读

# 前言

本书专注于与大数据处理有关的架构与算法。

# 目录

第0章——当谈论大数据时我们在谈什么
第1章——数据分片与路由
第2章——数据复制与一致性
第3章——大数据常用的算法与数据结构
第4章——集群资源管理与调度
第5章——分布式协调系统
第6章——分布式通信
第7章——数据通道
第8章——分布式文件系统
第9章——内存KV数据库
第10章——列式数据库
第11章——大规模批处理系统
第12章——流式计算
第13章——交互式数据分析
第14章——图数据库：架构与算法
第15章——机器学习：范型与架构
第16章——机器学习：分布式算法
第17章——增量计算

# 第0章——当谈论大数据时我们在谈什么

## 大数据是什么

多大才算大：数据量的衡量单位，从小到大依次为KB、MB、GB、TB、PB、EP和ZB

电子数据正在暴涨：2010年全世界信息总量是1ZB，最近3年人类产生的信息量已经超过了之前历史上人类产生的所有信息之和。

大数据的定义：
- 维基百科：数据量太大，手头的工具已经不便于管理
- IBM：3V（Volume、Velocity、Variety）+1V（Value）
- IDC：代表新一代的技术脚骨，能够高速获取数据进行分析挖掘，抽取价值信息
- Google：海量数据可广泛获得，所稀缺的是如何从中挖掘出智慧和观点。

## 大数据之翼：技术范型转换

关系型数据库——>并行数据库——>NoSql数据库

整体架构：
——>数据源
来源各异，形式不规整
——>数据管理
NoSQL不追求应用场景的统一，而且不同类型不同NoSQL库存储管理
——>数据分析
挖掘分析，利用数据挖掘、机器学习、时序分析
——>数据获取
数据可视化展现

大数据技术处理架构图
![[Pasted image 20250904182932.jpg]]

## 大数据商业炼金术

互联网、传统IT、金融、零售行业都在利于大数据提升企业收益

## 大数据在路上

概念最早由麦肯锡提出，巨型公司提供基础架构平台，中小型创业公司完善分布式计算生态系统，或者提供大数据服务等。

# 第1章——数据分片与路由

数据规模越来越大：
- 并行数据库通过纵向扩展解决问题（Scale Up），扩展机器
- 大数据系统通过横向扩展解决问题（Scale Out），增加机器

增加机器就带来了**数据分片**，分片后就必须找到数据，即**数据路由**。

- 数据分片：实现水平扩展
- 数据复制：保证数据高可用，因为服务器经常存在故障，所以需要存储多份副本

数据复制面临的问题：并发对数据更新时，如何保证数据的一致性。

常见的数据分片方法包括哈希分片与范围分片，先讲通用模型再将具体实现。

## 抽象模型

二级映射：第一级：key-partition，一个分片包含多个数据记录；第二级：partition-machine，一个物理机器容纳多个数据分片。

![[Pasted image 20250904184458.jpg]]

在做数据分片时，根据key-partition映射关系将大数据水平切割成众多数据分片，然后再按照partition-machine映射关系将数据分片放置到对应的物理机器上。

- 哈希分片：主要通过哈希函数来建立key-partition映射关系，**点查询**，不支持范围查询。Dynamo、Cassandra、Riak、Voldmort、Membase等都支持
- 范围分片：**支持点查询也可以支持范围查询**，包括Google的BigTable和微软的Azure等系统

## 哈希分片

通过哈希函数分片，有三种哈希分片方式：Round Robin、虚拟桶及一致性哈希方法
### **Round Robin**
就是哈希取模法，哈希函数：H(key) = hash(key) mod K。机器编号0-K-1，全部数据分配到K台物理机。

- 优点：非常简单
- 缺点：灵活性差，增加一个节点，就需要重分布

它是物理机和数据分片的二合一，机器和映射函数紧耦合，这是缺乏扩展灵活性的根本原因。

### 虚拟桶

MemBase对于数据分片管理提出了虚拟桶的实现方式：
记录和物理机引入虚拟桶，数据线到桶，再到物理机，都是多对一的映射。

虚拟桶层就是数据分片层，key-partition映射采用哈希函数，而partition-machine映射采用表格管理实现。
![[Pasted image 20250905104631.jpg]]
系统灵活性、扩展性更强

### 一致性哈希

分布式哈希表：哈希表的分布式扩展。
一致性哈希就是分布式哈希的一种实现方式。

“一致性哈希”算法将哈希数值空间按照大小组成一个首尾相接的环状序列。对于每台机器，可以根据其IP和端口号经过哈希函数映射到哈希数值空间内，这样不同的机器就成了环状序列中的不同节点。
![[Pasted image 20250908145043.jpg]]
**路由方面**：可以在每个机器节点配置路由表，路由表存储m 条路由信息，其中第i 项（0≤i ≤m −1）路由信息代表距离当前节点为2 i 的哈希空间数值所在的机器节点编号。
通常情况下，路由算法发送的消息不会多于m 条，因为这个过程类似于在0～(2 m −1)数值空间上的二分查找法。

**扩展方面**：首先增加节点对应的前驱和后继节点关系，然后还需要进行数据重分布。并发情况下需要进行稳定性检测。

**离开方面**：正常离开进行通知准备，进行数据重分布，异常离开通过数据多副本保障。

**虚拟节点**：两个问题，节点到环状结构的位置随机，容易存在负载不均衡的问题，另外不同机器间的性能也不同。所以引入虚拟节点，即将一个物理节点虚拟成若干虚拟节点，分别映射到一致性哈希的环状结构不同位置。这样一方面可以导致更佳的负载均衡，也可以兼顾到机器异质性问题。

**一致性哈希**：将集群机器数目这一变量从哈希函数中移出，转而将机器及记录主键都映射到哈希数值空间，解除了机器与数据分布函数之间的直接耦合。大大增强了数据分片的灵活性，但是维护成本很高。

### 范围分片

范围分片首先将所有记录的主键进行排序，然后在排好序的主键空间里将记录划分成数据分片，每个数据分片存储有序的主键空间片段内的所有记录。至于数据分片在物理机的管理方式往往采用LSM树，这是一种高效写入的数据索引结构。

![[Pasted image 20250908175559.jpg]]

很多大规模存储系统都支持上述范围分片模式，比如Yahoo的PNUTS和微软的Azure。Google的BigTable也基本遵循上述模式，不同点在于其数据分片映射表不是单层结构，而是组织成类似B+树的层次结构。

# 第2章——数据复制与一致性协议

大数据领域，增加系统高可用，即将数据进行多副本存储，但是多副本会带来兵法写入的一致性问题。

## 基本原则与设计理念

CAP、BASE、ACID理论

### 原教旨CAP主义

CAP是对“Consistency/Availability/Partition Tolerance”：
- **强一致性**：在分布式系统中的同一数据多副本情形下，对于数据的更新操作体现出的效果与只有单份数据是一样的。
- **可用性**：客户端在任何时刻对大规模数据系统的读／写操作都应该保证在限定延时内完成。
- **分区容忍性**：网络分区通讯有问题，仍然能够继续工作。

1999年Eric Brewer提出该理论，只能实现三个中的两个，一般不舍弃P，故只有AP或CP.

![[Pasted image 20250908180836.jpg]]
- 传统的关系数据库在三要素中选择CA两个因素，即强一致性、高可用性，但是可扩展性与容错性差。
 - NoSQL系统往往更关注AP因素，即高可扩展性和高可用性，但是往往以弱一致性作为代价.


2012年，Eric Brewer又提出P很低概率出现，正常情况下还是要兼顾CAP，在进行差异化、细粒度的CA取舍。修正后的示意图如下，在过程中满足CAP、AP、CAP因素：
![[Pasted image 20250908181729.jpg]]

### ACID原则

- **原子性** （Atomicity）：是指一个事务要么全部执行，要么完全不执行。也就是不允许一个事务只执行了一半就停止。
- **一致性** （Consistency）：事务在开始和结束时，应该始终满足一致性约束条件。
- **事务独立** （Isolation）：如果有多个事务同时执行，彼此之间不需要知晓对方的存在，而且执行时互不影响。
- **持久性** （Durability）：事务的持久性是指事务运行成功以后，对系统状态的更新是永久的，不会无缘由地回滚撤销。

### BASE原则

大多数大数据环境下的云存储系统和NoSQL系统则采纳BASE原则

- **基本可用** （Basically Available）。在绝大多数时间内系统处于可用状态，允许偶尔的失败，所 以称为基本可用。
- **软状态或者柔性状态** （Soft State），是指数据状态不要求在任意时刻都完全保持同步，到目前为止软状态并无一个统一明晰的定义，但是从概念上是可理解的，即处于有状态（State）和无状态（Stateless）之间的中间状态。
- **最终一致性** （Eventual Consistency）。与强一致性相比，最终一致性是一种弱一致性，尽管软状态不要求任意时刻数据保持一致同步，但是最终一致性要求在给定时间窗口内数据会达到一致状态。

### CAP/ACID/BASE三者关系

ACID强调一致性，BASE强调可用性，弱化一致性。CAP和ACID的区别：
- ACID的C是操作一致性约束，CAP的C是数据的强一致性。CAP中的C是ACID中的C所涵盖语义的子集。
- 出现网络分区后，ACID的I只能在某个分区执行
- 当出现网络分区时，多个分区都可以各自进行ACID中的数据持久化（D）操作。

### 幂等性

**分布式的幂等性**：调用方反复执行同一操作与只正确执行一次操作效果相同，即对分布式系统内部状态来说，同一操作调用一次与反复调用多次其状态保持相同。

## 一致性模型分类

理想情况下就只有强一致性
为了在分布式环境下追求高可用和高扩展，采用弱一致性模型，很多NoSQL系统采用弱一致性模型

一致性模型关系图：
![[Pasted image 20250909192340.jpg]]

### 强一致性

更新后所有的读都是更新的值

![[Pasted image 20250909192434.jpg]]
### 最终一致性

无法保证强一致性的时间片段被称为“不一致窗口”，这个窗口可能会看到旧的数值
![[Pasted image 20250909192520.jpg]]

### 因果一致性

有因果依赖的，保证数据的因果一致性，但不一致窗口内仍然会看到旧值
![[Pasted image 20250909192650.jpg]]

### 读你所写一致性

“读你所写”一致性是因果一致性的特例
![[Pasted image 20250909192812.jpg]]

### 会话一致性

读你所写的变体：回话一致性
![[Pasted image 20250909193509.jpg]]

### 单调读一致性

读到一次最新的，后面就都是最新的
![[Pasted image 20250909193611.jpg]]

### 单调写一致性

单调写一致性可以保证其多次写操作的序列化，如果没有这种保证，对于应用开发者来说是很难进行程序开发的。

## 副本更新策略

分布式存储下，数据冗余增加可用性，也增加读操作的并发性。但是一致性问题如何解决呢？

### 同时更新

具体又有两种类型：
- A：不通过任何一致性协议直接同时更新多个副本数据。会有潜在的一致性问题，多个update并发执行
- B：通过某种一致性协议预先处理，有处理成本，所以请求延时会有所增加。

### 主从式更新

所有的更新操作都先提到主副本，主副本再去更新从副本。根据主副本通知从副本的不同机制来区分，存在以下3种类型。
- **同步方式**：主副本等待所有从副本更新完成之后才确认更新操作完成，这可以确保数据的强一致性，但是会存在较大请求延时。
- **异步方式**：主副本在通知从副本更新之前即可确认更新操作。按照读操作又可以分为如下两类：
	- 如果所有读请求都要通过主副本来响应，即任意一个副本接收到读请求后将其转发给主副本；可以保障强一致性，但是会有请求延时。
	- 如果任意一个副本都可以响应读请求，那么请求延时将会大大降低，但是这可能导致读结果不一致的问题。
- **混合方式**：同步和异步混合起来用，按照读操作又可以分为如下两类：
	- 如果读操作的数据至少要从一个同步更新的节点中读出，比如类似于RWN协议的R+W>N，可以保证强一致性，会有请求延时。
	- 如果读操作不要求一定要从至少一个同步更新节点中读出，即RWN协议中的R+W<=N 的模式，会有不一致问题。

### 任意节点更新

数据更新请求可能发给多副本中的任意一个节点，然后由这个节点来负责通知其他副本进行数据更新。有可能有两个不同客户端在同一时刻对同一个数据发出数据更新请求，而此时有可能有两个不同副本各自响应。
- 类型A：同步通知其他副本：存在和“主从式更新”类型A相似的情况，延时更多
- 类型B：异步通知其他副本：存在和“同时更新”策略及“主从式更新”策略的类型B类似的问题。

## 一致性协议

### 两阶段提交协议2PC

两阶段提交协议是很常用的解决分布式事务问题的方式，它可以保证在分布式事务中，要么所有参与进程都提交事务，要么都取消事务，即实现ACID中的原子性（A）的常用手段。
2PC更多的是作为实现数据更新原子性手段出现。

存在两类不同实体：唯一的协调者（Coordinator）和众多的参与者（Participants）。协调者起到分布式事务的特殊的管理协调作用。
分为两个阶段：表决阶段（Voting）和提交阶段（Commit）。
- **表决阶段（Voting）**：协调者向所有参与者发送表决请求，参与者响应准备情况
- **提交阶段（Commit）**：协调者收集表决信息，如果参与者均认为可提交，则通知大家提交，如果有一个表决中止，通知大家取消。

![[file-20250910190231949.jpg|400]]
可以进一步转换为协调者和参与者的状态机：

![[file-20250910190813183.jpg|400]]![[file-20250910191009985.jpg|400]]
对于阻塞状态可以引入超时判断机制和参与者互询机制。
- 引入超时判断机制可以解决协调者的WAIT状态和参与者的INIT状态的长时阻塞情形
- 引入互询机制可以解决大部分情形下参与者READY状态的长时阻塞可能。

如果参与者Q是READY，P从Q无法获得更多信息，如果其它参与者都在READY，所有参与者必须长时间处于阻塞状态，等待崩溃的协调者重新启动。这种情形就是上文提到的**2PC无法解决的一种长时阻塞状态**。

三阶段提交协议（3PC）是学术界提出的用来解决2PC协议存在长时阻塞的办法，其核心思想是将2PC的提交阶段再次细分为两个阶段：预提交阶段和提交阶段。

![[file-20250910191727851.jpg|400]]
![[file-20250910191735909.jpg|400]]

该协议的本质在于通过引入PRECOMMIT状态，使得协调者和每个参与者都满足以下两个条件。
**条件一**：没有一个可以直接转换到COMMIT或者ABORT状态的单独状态（2PC中，协调者的WAIT状态和参与者的READY状态就是这种单独状态）。
**条件二**：不存在这样一个状态，即它不能做出最后决定，而且可以从它直接转到COMMIT状态（2PC中，协调者的WAIT状态和参与者的READY状态就是这种状态。3PC中的PRECOMMIT状态可直接转到COMMIT状态，但是它已经做出了提交决定）。

这两个条件是使得提交协议不阻塞的充要条件。3PC在实际系统中很少使用，一方面是由于2PC中长时阻塞情况很少发生，另外一方面是3PC效率过低。

### 向量时钟

向量时钟是在分布式环境下生成事件之间偏序关系的算法，偏序关系代表事件发生先后顺序导致的事件间因果依赖关系语义，通过将时间戳和事件绑定可以用来判定事件之间的因果相关性。

设分布式系统有A、B和C这3个进程，根据上述规则其各自对应的逻辑时钟随着时间演化情况如图所示。
![[file-20250910193411653.jpg|600]]
向量时钟的典型应用场景是用来判断分布式环境下不同事件之间是否存在因果关系。对于两个事件E
 和F，假设其各自的向量时钟分别是E.VC和F.VC，我们可以根据如下方法判断其是否存在因果关系：
 ![[file-20250910193536308.jpg]]
 即如果事件E的时钟向量各个维度的数值都小于等于事件F对应位置的数值且至少有一位是小于，那么可以称为事件E是事件F的原因，事件F是事件E的结果。

Dynamo中使用向量时钟进行数据版本管理，配合RWN协议共同完成数据一致性维护。

### RWN协议

“RWN协议”是亚马逊公司在实现Dynamo KV存储系统时提出的。这是一种通过对分布式环境
 下多备份数据如何读／写成功进行配置来保证达到数据一致性的简明分析和约束设置。
- N：在分布式存储系统中，有多少份备份数据。
- W：代表一次成功的更新操作要求至少有W份数据写入成功。
- R：代表一次成功的读操作要求至少有R份数据成功读取。

如果R+W>N，则满足“数据一致性协议”。成功写入的备份集合和成功读取的备份集合一定会存在交集，而这就可以保证数据的强一致性，即读取操作一定可以读到最新的数据版本。
![[file-20250910194226349.jpg|400]]

在满足数据一致性协议的前提下，R或者W设置得越大，则系统延迟越大，因为这取决于最慢的那份备份数据的响应时间。而如果R+W <=N，则无法保证数据的强一致性.

在具体实现系统时，仅仅依靠RWN协议还不能完成一致性保证，因为在上述过程中，当读取到多个备份数据时，需要判断哪些数据是最新的，如何判断数据的新旧？这需要向量时钟来配合。

### Paxos协议

“所有一致性协议本质上要么是Paxos，要么是其变体”

Paxos的难理解性在于是什么因素导致协议以此种方式呈现以及其正确性证明过程而非最终协议内容本身.

首先介绍副本状态机模型，之后介绍Paxos的一些基本概念，然后描述Paxos协议本身内容。

**副本状态机模型**（Replicated State Machines）

在分布式环境下，一致性协议的应用场景一般会采用副本状态机来表达，这是对各种不同应用场景的一种抽象化表述。

![[file-20250910195324044.jpg|500]]
集群中多台服务器各自保存一份Log副本及内部状态机，Log内顺序记载客户端发来的操作指令，服务器依次执行Log内的指令并将其体现到内部状态机上，如果保证每台机器内的Log副本内容完全一致，那么对应的状态机也可以保证整体状态一致。一致性协议的作用就是保证各个Log副本数据的一致性。

追求三个特性：
- 安全性（Safety）保证：即非拜占庭模型（此概念参考后面的内容）下，状态机从不返回错误的结果，多个提议中只会有一个被选中。
- 可用性（Available）保证：只要大多数服务器正常，则整个服务保持可用。比如副本状态机有5台服务器，那么最多可以容忍2台服务器发生故障，此时整个服务仍然可用，即对于2f+1台副本状态机的配置，最多可容忍f个状态机失效。
- 一般情况下，大多数状态机维护Log一致即可快速通知客户端操作成功，这样避免了少数最慢的状态机拖慢整个请求响应速度。

**Paxos基本概念

分为两种：单Paxos（Single-Decree Paxos）和多Paxos（Multi-Paxos）
- 单Paxos，即副本状态机中各个服务器 针对Log中固定某个位置的操作命令通过协议达成一致，因为可能某一时刻不同服务器的Log中相同位置的操作命令是不一样的，通过执行协议后使得各个服务器对应某个固定位置的操作命令达成一致。
- 多Paxos则是指这些服务器对应的Log内容中多个位置的操作命令序列通过协议保持一致。多Paxos往往是同时运行的多个单Paxos协议共同执行的结果。

Paxos协议下不同并行进程可能承担的3种角色如下：
- **倡议者**：倡议者可以提出提议（数值或操作命令等）以供投票表决。
- **接受者**：接受者可以对倡议者提出的提议进行投票表决，从众多提议中选出唯一确定的一个。
- **学习者**：学习者无倡议投票权，但是可以从接受者那里获知是哪个提议最终被选中。

在一致性协议框架中，一个并行进程可以同时承担以上多种角色。

Paxos协议以及很多一致性协议都是基于非拜占庭模型的，即在非拜占庭条件下，Paxos协议可以就不同提议达成一致，而在拜占庭模型下情况会更加复杂一些。

**Paxos一致性协议
- **阶段一：**
	1. 【倡议者视角】倡议者选择倡议编号n，然后向大多数（即超过半数以上）接受者发送Prepare请求，请求中附带倡议编号n
	2. 【接受者视角】对于某个接受者来说，如果接收到带有倡议编号n的Prepare请求，则做如下判断：若倡议编号n比此接受者之前响应过的任何其他Prepare请求附带的倡议编号都大，那么此接受者会给倡议者以响应，并承诺不会响应之后接收到的其他任何倡议编号小于n的请求，另外，如果接受者曾经响应过2.2阶段的Accept请求，则将所有响应的Accept请求中倡议编号最高的倡议内容发送给倡议者，倡议内容包括两项信息：Accept请求中的倡议编号以及其倡议值。若倡议编号n不比此接受者之前响应过的任何其他Prepare请求附带的倡议编号都大，那么此接受者不会给倡议者以响应。
- **阶段二：**
	1. 【倡议者视角】如果倡议者接收到大多数接受者关于带有倡议编号n的Prepare请求的响应，那么倡议者向这些接受者发送Accept请求，Accept请求附带两个信息：倡议编号n以及倡议值v。倡议值v的选择方式如下：如果在1.2阶段接受者返回了自己曾经接收的具有最高倡议编号Accept请求倡议内容，则从这些倡议内容里面选择倡议编号最高的并将其倡议值作为倡议值v；如果1.2阶段没有收到任何接受者的Accept请求倡议内容，则可以任意赋值给倡议值v。
	2. 【接受者视角】如果接受者接收到了任意倡议编号为n的Accept请求，则接受者接受此请求，除非在此期间接受者响应过具有比n更高编号的Prepare请求。

对于学习者来说，其需要从接受者那里获知到底是哪个倡议值被选出。一个直观的方法如下：每当接受者执行完2.2步骤，即接受某个Accept请求后，由其通知所有学习者其所接受的倡议，这样，学习者很快习得是哪个倡议被最终选出。有通讯成本，可以选出若干学习者作为代表，由这些代表从接受者那里获知最终倡议值，然后通知其他学习者。

通过以上流程，如果有多个并发进程提出各自的倡议值，Paxos就可以保证从中选出且只选出一个唯一确定的倡议值，以此来达到副本状态机保持状态一致的目标。

### Raft协议

Raft一致性协议最主要的目标有两个：
 - 首先是可理解性，在做技术决策和选型的时候，在达到相似功能前提下，首先以易于理解作为选型标准；
 - 其次是实现实际系统的确定性，鉴于之前提到的根据Paxos实现具体系统时的不统一，Raft追求每个技术细节的清晰界定与描述，以此达到实现具体系统时的明确性。

为了达到上述两个目的，主要采取了以下两个手段：
- 将整个一致性协议划分成明确且独立的3个子问题，即采取分解法。Raft将整个一致性协议划分为领导者选举、Log复制与安全性3个问题。
- 将Paxos的P2P模式改造为Master-Slave模式。Paxos的复杂性很大原因是由于其完全的P2P模式造成的，即多个并发进程之间无主次关系，都具有同等地位。

**Raft基本概念

首先是服务器状态，在任意时刻，集群中的服务器只能处于以下3种状态之一：Leader、Follower和Candidate。正常情况下，集群中只有一个处于Leader状态的服务器充当领导者，由其来负责响应所有客户端请求，其他服务器都处于Follower状态。处于Follower状态的服务器都是被动接收RPC消息，从不会主动发送任何RPC消息。Candidate状态是Follower状态服务器准备发起新的领导者选举前需要转换到的状态，即Candidate状态是Follower向Leader状态转换的中间状态。

![[file-20250910201124581.jpg]]

Raft将整个系统执行时间划分为由若干个不同时间间隔长度的时间片段构成的序列，每个时间片段被称为一个Term，以递增的数字来作为这个Term的标识。每个Term由“选举期间”（Election）开始，在这个时间内若干处于Candidate状态的服务器试图竞争成为新的领导者。如果某个服务器赢得了选举，则在这个Term接下来的时间里充当新的领导者。

![[file-20250910201216917.jpg]]


**Raft一致性协议

- **领导者选举**：Raft采用心跳机制来触发领导者选举过程。当整个系统启动时，所有服务器处于Follower状态，除非服务器接收到处于Leader或者Candidate状态服务器发出的RPC命令，否则其一直维持这个状态不变。Leader通过周期性地向其他服务器发送心跳来宣告并保持其领导者地位。
	在开始选举前，Follower增加其Term编号并转入Candidate状态。然后其向集群内所有其他服务器发出RequestVote RPC消息，之后一直处于Candidate状态，除非以下情况之一发生。
	- 赢得了本次选举
	- 另外一个服务器S宣称并确认自己是新的领导者；
	- 经过一定时间后，仍然没有新的领导者产生
- **Log复制**：当选出领导者后，之后所有客户端请求都由领导者来负责响应。领导者接收到客户端的操作命令后，将其作为新项目追加到Log尾部，然后向集群内所有其他服务器发出AppendEntries RPC请求，这引发其他服务器复制新的操作命令。当其他服务器安全复制了新的操作命令后，领导者将这个操作命令应用到内部状态机，并将执行结果返回给客户端。
	![[file-20250910201547474.jpg|400]]
- **安全性**：以上两个步骤，在一般情形下Raft已经可以正常运行，但是目前Raft还无法做到完全的安全性保证，即无法保证每个服务器的状态机都能够按照相同顺序执行相同操作命令。为了达到真正的安全性，Raft增加了如下两个约束条件。
	- 限制了哪些服务器可以被选举成为领导者，其要求只有其Log包含了所有已经提交的操作命令的那些服务器才有权被选举为新的领导者
	- 限制了哪些操作命令的提交可以被认为是真正的提交。对于新领导者来说，只有它自己已经提交过当前Term的操作命令才被认为是真正提交。

# 第3章——大数据常用的算法与数据结构

## 布隆过滤器

Bloom Filter（为了表达方便，后文简称BF）就是常说的布隆过滤器，是由Howard Bloom在1970年提出的二进制向量数据结构，它具有很好的空间和时间效率，尤其是空间效率极高，BF常常被用来检测某个元素是否是巨量数据集合中的成员。
### 基本原理

BF可以高效地表征集合数据，其使用长度为m的位数组来存储集合信息，同时使用k个相互独立的哈希函数将数据映射到位数组空间。

基本思想：

首先，将长度为m的位数组元素全部置为0。对于集合S中的某个成员a，分别使用k个哈希函数对其计算，如果hi(a)=x(1≤i≤k,1≤x≤m)，则将位数组的第x位置为1。

当查询某个成员a 是否在集合S中出现时，使用相同的k个哈希函数计算，如果其对应位数组中的w位（w≤k）都为1，则判断成员a属于集合S，只要w位中有任意一位为0，则判断成员a不属于集合S

### 误判率及相关计算

如果某个成员不在集合中，有可能BF会得出其在集合中的结论。

![[file-20250916194423371.jpg | 400]]

BF会产生误判，但是**不会发生漏判**（False Negative）的情况，即如果某个成员确实属于集合，那么BF一定能够给出正确判断。**在的肯定在，不在的不一定不在**

影响误判率的因素：包括集合大小n、哈希函数的个数k、 和位数组大小m。![[file-20250916194659885.jpg]]

### 改进：计数Bloom filter

基本的BF在使用时有个缺点：无法删除集合成员，只能增加成员并对其查询

计数BF的思路：基本信息单元由多个比特位来表示，一般情况采取3或4比特位为单元。这样，将集合成员加入位数组时，根据k个哈希函数计算，此时对应位置的信息单元由多个比特位构成，所以将原先的数值加1即可。查询集合成员时，只要对应位置的信息单元都不为0即可认为该成员属于集合。而删除成员，只要将对应位置的计数减1即可。

存在计数溢出的可能

### 应用

因为BF的极高空间利用率，其在各个领域获得了非常广泛的使用，尤其是数据量极大且容忍一定误判率的场合。

在BigTable中，BF对于读操作的效率提升有巨大帮助。BigTable中很多数据记录存储在磁盘的多个SSTable文件中，为了完成一次读操作，需要依次在这些SSTable中查找指定的Key，因为是磁盘操作且涉及多个文件，所以会对读操作效率有极大影响。BigTable将SSTable文件中包含的数据记录Key形成BF结构并将其放入内存，这样就能极高地提高查询速度，对于改善读操作有巨大的帮助作用。

## SkipList

SkipList由William Pugh于1990年提出，这是一种可替代平衡树的数据结构。其插入、删除、查找数据的时间复杂度都是O(log(N))

**核心思路**：

- 传统有序链表
![[file-20250916195612225.jpg]]
- 增加指针后的有序链表
![[file-20250916195653158.jpg]]
多跳几次：
![[file-20250916195659542.jpg]]

SkipList**依赖随机数**来以一定概率保持数据的平衡，具体而言，就是**在插入节点的时候，随机决定**该节点应该有多少个指向后续节点的指针，有几个指针就称这个节点是几层的（Level）。

插入过程：
![[file-20250916195931890.jpg]]

## LSM树

LSM树（Log-structured Merge-tree）的本质是将大量的随机写操作转换成批量的序列写，这样可以极大地提升磁盘数据写入速度，所以LSM树非常适合对写操作效率有高要求的应用场景。

LevelDB静态结构：
![[file-20250916200147293.jpg | 500]]

构成LevelDB静态结构的包括6个主要部分：内存中的MemTable和Immutable MemTable以及磁盘上的几种主要文件：Current文件、manifest文件、log文件以及SSTable文件。

- **写入**：LevelDB的log文件和MemTable与BigTable论文中介绍的是一致的，当应用写入一条Key：Value记录的时候，LevelDB会先往log文件里写入，成功后将记录插进MemTable中，这样基本就算完成了写入操作，因为一次写入操作只涉及一次磁盘顺序写和一次内存写入，**而且MemTable采用了维护有序记录快速插入查找的SkipList数据结构**，所以说LSM树是一种高速写入数据结构的主要原因。
- **到处**：当MemTable插入的数据占用内存到了一个界限后，需要将内存的记录导出到外存文件中，LevelDB会生成新的log文件和MemTable，原先的MemTable就成为Immutable MemTable，顾名思义，就是说这个MemTable的内容是不可更改的，只能读不能写入或者删除。新到来的数据被记入新的log文件和MemTable，LevelDB后台调度会将Immutable MemTable的数据导出到磁盘，形成一个新的SSTable文件。（SSTable有层次，故叫做LevelDB）
- **记录**：SSTable中的文件是主键有序的，也就是说，在文件中小key记录排在大key记录之前，各个Level的SSTable都是如此。manifest记载了SSTable各个文件的管理信息，比如属于哪个Level、文件名称、最小key和最大key各自是多少。![[file-20250916201110792.jpg | 400]]
- **合并**：3种类型的Compaction，分别是minor、major和full。所谓minor Compaction，就是把MemTable中的数据导出到SSTable文件中，major Compaction就是合并不同层级的SSTable文件，而full Compaction就是将所有SSTable进行合并。
	- **minor**：当MemTable中记录数量到了一定程度会转换为Immutable MemTable，此时不能往其中写入记录，只能从中读取KV内容。Immutable MemTable其实是一个SkipList多层级队列，其中的记录是根据key有序排列的。所以这个minor Compaction实现起来也很简单，就是按照Immutable MemTable中记录由小到大遍历，并依次写入一个Level 0的新建SSTable文件中，写完后建立文件的index数据，这样就完成了一次minor Compaction。
	- **major**：当某个Level下的SSTable文件数目超过一定设置值后，LevelDB会从这个Level的SSTable中选择一个文件（Level>0），将其和高一层级的Level+1的SSTable文件合并，这就是major Compaction。对多个文件采用多路归并排序的方式，依次找出其中最小的key记录，也就是对多个文件中的所有记录重新进行排序。之后采取一定的标准判断这个key是否还需要保存，如果判断没有保存价值，那么直接抛掉，如果觉得还需要继续保存，那么就将其写入Level L+1层中新生成的SSTable文件中

由内存中的MemTable和磁盘上的各级SSTable文件就形成了LSM树。
LSM树的本质，即将大量随机写转换为批量的序列写。

## Merkle哈希树

Merkle哈希树由Ralph Merkle于1979年发明。Merkle树最初用于高效Lamport签名验证，后来被广泛应用在分布式领域，主要用来在海量数据下快速定位少量变化的数据内容

### Merkle树基本原理
![[file-20250917210229248.jpg]]
子节点是每个数据项或者一批数据项（数据块）对应的哈希值，中间节点则保存对其所有子节点哈希值再次进行哈希运算后的值，依次由下往上类推，直到根节点，其保存的Top Hash代表整棵树的哈希值，也就是所有数据的整体哈希值。具体使用的时候，既可以像例子中一样是一个二叉树，也可以是多叉树。

Merkle树常用于快速侦测部分数据正常或者异常的变动。当某个底层数据发生变化时，其对应Merkle树的子节点哈希值会跟着变化，子节点的父节点哈希值也随之变化。

通过Merkle树，可以在O(log(n))时间内**快速定位变化**的数据内容。

### Dynamo中的应用

Dynamo结合Merkle树和Gossip协议来对副本数据进行同步。

Dynamo可以快速定位到数据副本不同内容，且只须同步两者的差异部分即可实现副本数据同步，这样有效地减少了网络传输数据量，增加了数据同步效率。

### 比特币中的应用

比特币中，主要使用Merkle树来对交易进行验证，以此来判断某个交易是否是合法交易。

通过引入Merkle树和上述的链表结构，比特币采用少量计算及比较操作即可完成交易的验证过程。

## Snappy和LZSS算法

Snappy是Google开源出的高效数据压缩与解压缩算法库，其目标并非是最高的数据压缩率，而是在合理的压缩率基础上追求尽可能快的压缩和解压缩速度。
- 其压缩和解压缩速度极快，可以在单核处理器上达到250MB/s的压缩效率和500MB/s的解压缩效率。
- Snappy相比其他压缩方案占用CPU时间更少。

Snappy是基于LZSS算法的

### LZSS算法

LZ77是一种动态词典编码（Dictionary Coding）

**基本思路**：文本中的词用它在词典中表示位置的号码代替的无损数据压缩方法，一般分为静态词典方法和动态词典方法两种。采用静态词典编码技术时，编码器需要事先构造词典，解码器要事先知道词典。采用动态辞典编码技术时，编码器将从被压缩的文本中自动导出词典，解码器解码时边解码边构造解码词典。

动态词典编码的基本思路：p替代了abc
![[file-20250917211111486.jpg | 500]]

**LZ77算法**：描述了一种基于滑动窗口缓存的技术，该缓存用于保存最近刚刚处理的文本，而动态词典就是由滑动窗口内的文本构造出来的。

LZ77是动态词典编码方法的开创者，后来所有动态词典编码压缩方法都是基于LZ77进行改造和优
化的，比如我们熟知的GZip、WinZip、RAR、Compress等都采用了LZ系列算法。

LZ77的压缩算法使用了滑动窗口和前向缓冲区的概念：
- 滑动窗口：前面处理过的若干源字符
- 前向缓冲区：包含了输入数据流中将要处理的所有后续字符。
![[file-20250917211500784.jpg | 500]]

LZSS对LZ77做出了改进：增加了最小匹配长度限制，当匹配字符串小于指定的最小匹配限制时，并不进行压缩输出，而是仍然滑动窗口右移一个字符。实例如下：

![[file-20250917211546710.jpg | 500]]
### Snappy

Snappy在整体框架上基本遵循LZSS的压缩编码与解码方案。首先，Snappy设定最小匹配长度为4，即只有匹配长度大于等于4的字符串才进行压缩，相应地，其设定哈希表内的字符串片段固定长度也为4。

Snappy做了一些相对独特的优化。比如其在压缩数据时，将整个数据切割成32KB大小的数据块分别进行压缩，数据块之间独立无关联，这样两个字节即可表示匹配字符串的相对位置。

## Cuckoo哈希

Cuckoo哈希由Rasmus Pagh和Flemming Friche Rodler于2001年提出，使用它可以有效解决哈希冲突（Hash Collisions）问题。Cuckoo哈希具有很多优良特性，比如可以在O（1）时间复杂度查找和删除数据，可以在常数时间内插入数据等。其有大约50%的哈希空间利用率。

### 基本原理

Cuckoo哈希同时使用两个不同的哈希函数H1(x )和H 2(x )。当插入数据x时，同时计算H1(x)和H2(x)，如果对应的哈希空间中任意一个桶（Bucket）为空，则可以将x插入相应位置；如果两者都不空，则选择一个桶，将已经占据这个位置的值y踢出去，由x来占据这个位置。y继续重新计算。

可能会无限循环，需要设定最大替换次数。

对于查找操作来说，只需要查找两个哈希函数映射到的哈希空间对应位置，要么存在要么不存在，是唯一确定的，所以可以在O(1)时间内完成。与传统的哈希方式相比较，Cuckoo哈希省去了当哈希冲突时进行冲突解决的过程，所以查找效率非常高。

# 第4章——集群资源管理与调度

**静态资源划分**：将集群中的所有资源做出静态划分，将划分后的固定的硬件资源指定给固定的计算框架使用，各个框架之间各行其是，互不干扰。资源的整体利用率不高，经常会出现集群中有些计算系统资源不足

**发展趋势**：在集群硬件层之上抽象出一个功能独立的集群资源管理系统，将所有可用资源当作一个整体来进行管理，并对其他所有计算任务提供统一的资源管理与调度框架和接口，计算任务按需向其申请资源，使用完毕释放给资源管理系统。

**优势**：
- 集群整体资源利用率高
- 可增加数据共享能力
- 支持多类型计算框架和多版本计算框架

## 资源管理抽象模型

### 概念模型

从概念上讲，资源管理与调度系统的主要目的是将集群中的各种资源通过一定策略分配给用户提交到系统里的各种任务，常见的资源主要包括内存、CUP、网络资源与磁盘I/O资源4类。

三要素：资源组织模型、调度策略和任务组织模型
- **资源组织模型**：将集群中当前可用的各种资源采用一定的方式组织起来，以方便后续的资源分配过程。
- **调度策略**：以一定方式将资源分配给提交到系统的任务，常见的调度策略包括FIFO、公平调度、能力调度、延迟调度等
- **任务组织模型**：将多用户提交的多任务通过一定方式组织起来，以方便后续资源分配。
![[file-20250922141748994.jpg | 500]]

### 通用架构

- **节点管理器**：集群中每台机器上会配置节点管理器，其主要职责是不断地向资源收集器汇报目前本机资源使用状况，并负责容器的管理工作。当某个任务被分配到本节点执行时，节点管理器负责将其纳入某个容器执行并对该容器进行资源隔离，以避免不同容器内任务的相互干扰。

- **通用调度器**：由资源收集器和资源调度策略构成，同时管理资源池和工作队列数据结构。资源收集器不断地从集群内各个节点收集和更新资源状态信息，并将其最新状况反映到资源池中，资源池列出了目前可用的系统资源。资源调度策略是具体决定如何将资源池中的可用资源分配给工作队列的方法，常见的策略包括FIFO、公平调度策略和能力调度策略等。资源调度策略模块往往是可插拔的。
![[file-20250922141938153.jpg | 500]]

## 调度系统设计的基本问题

### 资源异质性与工作负载异质性

异质性往往指的是组成元素构成的多元性和相互之间较大的差异性。

- **资源异质性**：比如数据中心的机器很难保证采用完全相同的配置，总会有些机器高配置，拥有大量的内存和计算以及存储资源，也会有很多低配硬件。在做资源分配的时候，必须要考虑这种硬件的资源差异性，一般通过将资源分配单位细粒度划分为较小单元来解决这个问题。
- **工作负载异质性**：各种服务和功能特性各异，对资源的需求差异也很大。比如对外服务强调高可用性以及资源的充分优先保障，而后台运行的批处理作业往往是由很多短任务构成的，所以需要调度决策过程要尽可能快，等等。

### 数据局部性

**大数据基本设计原则**：计算任务推送到数据所在地进行而不是反过来。因为海量数据分布在大规模集群的不同机器中，如果移动数据会产生大量低效的数据网络传输开销

在资源管理与调度语境下，有3种类型的数据局部性：节点局部性（Node Locality）、机架局部性（Rack Locality）和全局局部性（Global Locality）

- **节点局部性**：是指可以将计算任务分配到数据所在的机器节点，这是数据局部性最优的一种情形，因为完成计算无须任何数据传输。
- **机架局部性**：指的是虽然计算任务和所需数据分属两个不同的计算节点，但是这两个节点在同一个机架中，这也是效率较高的一种数据局部性，因为机架内机器节点间网络传输速度要明显高于机架间网络传输速度。
- **全局局部性**：其他的情况都属于，此时需要跨机架进行网络传输，会产生较大的网络传输开销。

节点局部性>机架局部性>全局局部性

### 抢占式调度与非抢占式调度

- **抢占式调度**：对于某个计算任务来说，如果空闲资源不足或者出现不同任务共同竞争同一资源，调度系统可以从比当前计算任务优先级低的其他任务中获取已分配资源，而被抢占资源的计算任务则需出让资源停止计算。
- **非抢占式调度**：只允许从空闲资源中进行分配，如果当前空闲资源不足，则须等待其他任务释放资源后才能继续向前推进。
 
### 资源分配粒度

大数据场景下的计算任务往往由两层结构构成：作业级（Job）和任务级（Task）。一个作业由多个并发的任务构成，任务之间的依赖关系往往形成有向无环图（DAG），典型的MapReduce任务则是一种比较特殊的DAG关系。

- 一种极端的情况是需要将作业的所有所需资源一次性分配完成，这常被称为“群体分配”（Gang Scheduler）或者“全分或不分”（All-or-Nothing）策略。MPI任务就是一种典型的需要采纳群体分配策略的任务类型。
- 另外一种分配粒度是采取增量满足式分配策略，即对于某个作业来说，只要分配部分资源就能启动一些任务开始运行，随着空闲资源的不断出现，可以逐步增量式分配给作业其他任务以维持作业不断地向后推进，以MapReduce为代表的批处理任务一般采用增量满足式分配策略。
- 有一种特殊的增量满足式分配策略被称作“资源储备”（Resource Hoarding）策略。这是指只有分配到一定量的资源作业才能启动，但是在未获得足够资源的时候，作业可以先持有目前已分配的资源，并等待其他作业释放资源，这样从调度系统不断获取新资源并进行储备和累积，直到分配到的资源量达到最低标准后开始运行。

### 饿死与死锁问题

- “**饿死**”现象：这个计算任务持续长时间无法获得开始执行所需的最少资源量，导致一直处于等待执行的状态。比如在资源紧张的情形下，有些低优先级的任务始终无法获得资源分配机会
- **死锁问题**：是由于资源调度不当导致整个调度系统无法继续正常执行。

调度系统出现死锁必然表现为某些作业处于“饿死”状态，但是有计算任务处于“饿死”情形并不一定意味着调度系统处于死锁状态。

### 资源隔离方法

目前对于资源隔离最常用的手段是Linux容器（Linux Container，LXC），YARN和Mesos都采用了这种方式来实现资源隔离。

**LXC**是一种轻量级的内核虚拟化技术，可以用来进行资源和进程运行的隔离，通过LXC可以在一台物理主机上隔离出多个相互隔离的容器，目前有开源版本。LXC在资源管理方面依赖于Linux内核的cgroups子系统，cgroups子系统是Linux内核提供的一个基于进程组的资源管理的框架，可以为特定的进程组限定可以使用的资源。

## 资源管理与调度系统范型

3种资源管理与调度系统范型：集中式调度器、两级调度器与状态共享调度器

![[file-20250922143954531.jpg]]

### 集中式调度器

集中式调度器在整个系统中只运行一个全局的中央调度器实例，所有之上的框架或者计算任务的资源请求全部经由中央调度器来满足，因此，整个调度系统缺乏并发性且所有调度逻辑全部由中央调度器来实现。

- **单路径调度器**：指不论计算任务是何种类型，都采取统一的调度策略来进行资源管理与调度，这种类型调度器在高性能计算系统（HPC）中非常常见，比如Maui以及Moab等系统都采用此种方式。
- **多路径调度器**：可以支持多种调度策略，比如针对批处理类任务采取某种调度策略，对于在线服务类任务采取另外一种调度策略等。

集中式调度器实现逻辑复杂，系统可扩展性差，支持不同类型的调度策略缺乏灵活性。

### 两级调度器

两级调度器将整个系统的调度工作划分为两个级别：中央调度器和框架调度器。
- **中央调度器**：可以看到集群中所有机器的可用资源并管理其状态，它可以按照一定策略将集群中的所有资源分配给各个计算框架，中央调度器级别的资源调度是一种粗粒度的资源调度方式。
- **框架调度器**：各个计算框架在接收到所需资源后，可以根据自身计算任务的特性，使用自身的调度策略来进一步细粒度地分配从中央调度器获得的各种资源。

只有中央调度器能够观察到所有集群资源的状态，而每个框架并无全局资源概念，只能看到由中央调度器分配给自己的资源。
Mesos、YARN和Hadoop On Demand系统是3个典型的两级调度器系统。

两级调度器由于在计算框架层面存在第二级资源调度，而这可以提供一种比较天然的并发性，所以整体调度性能较好，也适合大规模集群下的多任务高负载计算情形，具有较好的可扩展性。

### 状态共享调度器

在这种调度范型中，每个**计算框架可以看到整个集群中的所有资源**，并采用**相互竞争**的方式去获取自己所需的资源，根据自身特性采取不同的具体资源调度策略，同时系统采用了乐观并发控制手段解决不同框架在资源竞争过程中出现的需求冲突。

与两级调度器对照可以看出，其实两者的根本区别在于中央调度器功能强弱不同。

两级调度器依赖中央调度器来进行第一次资源分配，而Omega则严重弱化中央调度器的功能，只是维护一份可恢复的集群资源状态信息主副本，这份数据被称作“单元状态”（Cell State）。

果两个不同框架竞争同一份资源，因其决策过程都是各自在自己的私有数据上做出的，并通过原子事务进行提交，系统保证此种情形下只有一个竞争胜出者，而失败者可以后续继续重新申请资源，所以这是一种**类似于MVCC**的乐观并发控制手段，可以增加系统的整体并发性能。

状态共享调度器将两级调度器的中央调度器功**能弱化成了维护持久化可恢复的集群资源状态信息**，只要所有框架具有关于相互之间优先级高低可比的共识，就可以采取自由竞争的方式实现抢占式整体资源管理与调度。

- 集中式调度器比较适合小规模集群下的资源调度与管理
- 两级调度器比较适合负载同质的大规模集群应用场景
- 状态共享调度器则更适合负载异质性较强且资源冲突不多的大规模集群应用场景。

## 资源调度策略

### FIFO调度策略

FIFO策略是最简单的资源调度策略，提交的作业按照提交时间先后顺序或者根据优先级次序将其放入线性队列相应位置，在资源调度时按照队列先后顺序，先进先出地进行调度与资源分配。

FIFO是Hadoop默认的调度策略，很明显这种策略过于简单，在多用户场景下，新加入的作业很容易出现长时间等待调度的现象。

### 公平调度器

公平调度器是Facebook为Hadoop开发的多用户多作业调度器。其将用户的任务分配到多个资源池（Pool），每个资源池设定资源分配最低保障和最高上限，管理员也可以指定资源池的优先级，优先级高的资源池会被分配更多的资源，当一个资源池资源有剩余时，可以临时将剩余资源共享给其他资源池。

调度过程：
- 根据每个资源池的最小资源保障量，将系统中的部分资源分配给各个资源池。
- 根据资源池的指定优先级将剩余资源按照比例分配给各个资源池。
- 在各个资源池中，按照作业优先级或者根据公平策略将资源分配给各个作业。

公平调度器和能力调度器都是Hadoop常用的调度策略，与能力调度器相比，公平调度器有两个明显的区别：
- 公平调度器支持抢占式调度，即如果某个资源池长时间未能被分配到公平共享量的资源，则调度器可以杀死过多分配资源的资源池中的任务，以空出资源供这个资源池使用。
- 公平调度器更强调作业间的公平性。在每个资源池中，公平调度器默认使用公平策略来实现资源分配

### 能力调度器

能力调度器是Yahoo为Hadoop开发的多用户调度器，适合用户量众多的应用场景，与公平调度器相比，其更强调资源在用户之间而非作业之间的公平性。

它将用户和任务组织成多个队列，每个队列可以设定资源最低保障和使用上限，当一个队列的资源有剩余时，可以将剩余资源暂时分享给其他队列。调度器在调度时，优先将资源分配给资源使用率最低的队列（即队列已使用资源量占分配给队列的资源量比例最小的队列）；在队列内部，则按照作业优先级的先后顺序遵循FIFO策略进行调度。

### 延迟调度策略

延迟调度策略不是一个独立的调度方式，往往会作为其他调度策略的辅助措施来增加调度的数据局部性，以此来增加任务执行效率。

对于当前被调度到要被分配资源的任务i，如果当前资源不满足数据局部性，那么可以暂时放弃分配公平性，任务i不接受当前资源，而是等待后续的资源分配；当前资源可以跳过任务i分配给其他待调度任务j，如果任务i在被跳过k次后仍然等不到满足局部性的资源，则放弃数据局部性，被迫接受当前资源来启动任务执行。

### 主资源公平调度策略

主资源公平调度策略（简称DRF）是Mesos中央调度器采用的公平调度策略，也是最大最小公平算法的一个具体体现。最大最小公平算法的基本思想是：最大化目前分配到最少资源量的用户或
者任务的资源量。这个算法常常用来对单个资源进行公平分配，而DRF则将其扩展到了多个资源的公平分配场景下。

## Mesos

Mesos是美国加州大学伯克利分校AMPLab实验室推出的资源管理与调度系统，从其范型来讲是一个典型的两级调度器。

Mesos的整体架构如图所示，其采用了典型的“主-从”架构。中央调度器由多个主控服务器（Master）构成，通过ZooKeeper可以保证当正在工作的主控服务器出现故障时，备用主控服务器（Standby Master）可以快速将管理工作接替过来，以此增加整个调度系统的健壮性。

![[file-20250922151029334.jpg | 500]]

分配资源过程：
- 首先，1号从节点（Slave 1）向主控服务器汇报其有4个CUP和4 GB内存资源可用，主控服务器触发资源分配模块，由其分配策略告知主控服务器应该将所有资源分配给框架1（步骤1）。在步骤2，Mesos将这份资源封装为“资源供应”并将其传给框架1的调度器。框架1的调度器运行二级调度策略，将这份资源分配给两个任务，其中任务1分配了2个CPU和1GB内存，任务2分配了1个CUP和2GB内存（步骤3）
- Mesos将这两个任务分配给1号从节点，由从节点实际分配这些资源给对应任务并启动执行器执行任务，同时从节点还提供任务运行的资源隔离管理（步骤4）。因为1号从节点提供的资源还有剩余，Mesos中央调度器还可以继续将其分配给框架2。通过这种方式，Mesos实现了一个典型的两级调度器。

![[file-20250922151139957.jpg | 500]]

Mesos比较适合不同框架任务同质化场景，尤其是大部分都是短作业的情景（比如Hadoop等批处理任务），因为从上述描述可知，Mesos是不支持抢占式调度的，资源分配出去后只能等待任务运行结束后自行释放，如果是大量短作业，那么资源释放速度较快，这样总有新资源可分配，对于后续的任务来说可以较快获得资源，避免长时间等待。

## YARN

YARN是Hadoop 2.0的重要组成部分，也被称作MRV2，其全称是“另一个资源协调器”（Yet Another Resource Negotiator），顾名思义，其是一个独立的资源管理系统。

MRV2与MRV1相比，最大的改变就是抽象出YARN这个独立资源调度系统。在MRV1中，所有任务的资源管理以及生命期管理都由全局唯一的JobTracker来负责，造成了JobTracker功能繁复，成为整个Hadoop系统的瓶颈，严重限制了系统的可扩展性，之前报道MRV1系统最大能支持的集群规模为4 000台服务器。

YARN同Mesos一样，是个典型的两级调度器，其中RM类似于Mesos中的主控服务器，充当中央调度器功能。每个任务的AM类似于Mesos中的二级调度器。AM负责向RM申请作业所需资源，并在作业的众多任务中进行资源分配与协调。

YARN与Mesos都有很大的共性，但是两者之间也有明显的区别，比如YARN的中央调度器支持“抢占式调度”以及AM可以在向RM申请资源时提出明确的数据局部性条件等。

**整体架构**：其最主要的构件包括：唯一的资源管理器（RM）、每个作业一个的“应用服务器”（AM）以及每个机器一个的“节点管理器”（Node Manager，NM）。

![[file-20250922152130039.jpg | 500]]
- RM：负责全局的资源管理工作，其内部主要功能部件包括：调度器、AM服务器（AMService/ApplicationMasters，AMS）、Client-RM接口以及RM-NM接口。YARN的RM支持“抢占式调度”，当集群资源稀缺时，RM可以通过协议命令AM释放指定的资源。另外，AM在资源请求信息内也可以明确指明数据局部性偏好。
- AM：负责向RM申请启动任务所需的资源，同时协调作业内各个任务的运行过程。尽管其功能有特殊性，但是其运行过程也像普通的任务一样运行在某台机器的容器内。
- NM：是YARN中在每台机器上都部署的节点管理器，主要负责机器内容器资源的管理，比如容器间的依赖关系、监控容器执行以及为容器提供资源隔离等各种服务等。

执行过程：
（1）用户通过客户端向YARN提交作业。
（2）RM通过调度器申请资源，用于启动运行作业的AM；如果申请到，则AMS负责通知节点管理器在相应容器内启动执行AM。
（3）AM负责将作业划分为若干任务，并向RM请求启动任务所需的资源；RM接收到请求后，通过调度器分配资源，找到合适的容器后，将这些资源信息返回给AM。
（4）AM根据资源信息，在任务间优化资源分配策略，确定后直接与资源所在的节点管理器联系，在对应的容器中启动任务，节点管理器负责容器的资源隔离。
（5）AM在部分任务执行完成后逐步向RM释放所占资源。

![[file-20250922152759724.jpg | 400]]

YARN是一个典型的两级调度器，RM担当中央调度器功能，支持“抢占式调度”，AM担当二级调度器的功能。与Mesos比较，由于Hadoop的广泛流行，再加上YARN代表了Hadoop的未来发展趋势，所以相比而言更活跃，发展前景更乐观。

# 第5章——分布式协调系统

## Chubby锁服务

Chubby是Google公司研发的针对分布式系统协调管理的粗粒度锁服务，一个Chubby实例大约可以负责1万台4核CPU机器相互之间对资源的协同管理。这种锁服务的主要功能是让众多客户端程序进行相互之间的同步，并对系统环境或者资源达成一致认知。

锁服务：通过对数据加锁的方式来实现各种分布式环境下的资源协调问题。

Chubby是一种“粗粒度”锁，所谓“粗粒度”指的是锁的持有时间比较长，反之如果锁的持有时间较短（秒级别）则被称为细粒度锁

**设计哲学**：强调协调系统的可靠性与高可用性及语义易于理解，而不追求处理读／写请求的高吞吐量及在协调系统内存储大量数据。

**理论基础**：Paxos一致性协议，Paxos是在完全分布环境下，不同客户端能够通过交互通信并投票，对于某个决定达成一致的算法
### 系统架构

由客户端链接的库程序和多个“Chubby单元”构成，一般一个数据中心部署一套“Chubby单元”。每个“Chubby单元”通常包含5台服务器，通过Paxos协议选举的方式推举其中一台作为“主控服务器”，所有读／写操作都由主控服务器完成，其他4台作为备份服务器，在内存中维护和主控服务器完全一致的树形结构

![[file-20250923195504240.jpg | 500]]

- “主控服务器”由所有服务器选举推出，有“任期”的，此即“主控服务器租约”（Master Lease），当主控服务器“任期”期满后，系统会再次投票选举出新的“主控服务器”，如果无故障等异常情况发生，一般情况下系统还是尽量将租约交给原先的“主控服务器”
- 客户端通过嵌入的库程序，利用RPC通信来和服务器进行交互，对Chubby的读／写请求都由“主控服务器”来负责。

### 数据模型

Chubby类似于文件系统的目录和文件管理系统，并在此基础上提供针对目录和文件的锁服务。

Chubby的文件主要存储一些管理信息或者基础数据，Chubby要求对文件内容一次性地全部读完或者写入，这是为了尽可能地抑制客户端程序写入大量数据到文件中，因为Chubby的目的不是数据存储，而是对资源的同步管理，所以不推荐在文件中保存大量数据。

Chubby还提供了文件内容或者目录更改后的通知机制，客户端可以订阅某个文件或目录，当文件内容和子目录发生变化或者一些系统环境发生变化时，Chubby会主动通知这些订阅该文件或目录的客户端，以使得这种信息变化得以及时传播。

![[file-20250923195842521.jpg | 500]]
Chubby在这个树形目录结构中提供了很多管理功能，比如针对某个目录或者文件的加锁服务，针对目录或者文件的访问权限控制，文件内容存取以及事件通知机制。通过这种结构，可以有效实现分布式系统中的同步协同和资源管理功能。

### 会话与KeepAlive机制

这里的会话（Session）指的是客户端和主控服务器之间建立的联系通道，而会话的维持是由周期性进行握手的KeepAlive机制保证的，即通过两者之间不断的KeepAlive通信来延续会话。

每次会话也有相应的租约，在租约时间段内服务器保证不会单方面将会话终止。

会话机制工作过程：客户端向主控服务器发出KeepAlive消息（一个RPC调用），服务器在接收到KeepAlive消息后，阻塞这个RPC调用，直到客户端原先的租约接近过期为止。

### 客户端缓存

为了减少客户端和服务器之间的通信量，Chubby允许客户端在本地缓存部分服务器数据，而由Chubby来保证缓存数据和服务器端数据完全一致。

### ZooKeeper

ZooKeeper是Yahoo开发并开源出的一套可扩展高吞吐分布式协调系统，目前已经在各种NoSQL数据库及诸多开源软件中获得广泛使用。正确地使用ZooKeeper可以很方便地解决各种分布式系统的管理协调问题，本节主要介绍这一协调系统的设计架构及相应的使用场景。

### 体系结构

ZooKeeper是一个高吞吐的分布式协调系统，同一时刻可以同时响应上万个客户端请求。ZooKeeper服务由若干台服务器构成，每台服务器内存中维护相同的类似于文件系统的树形数据结构，其中的一台通过ZAB原子广播协议选举作为主控服务器，其他的作为从属服务器。客户端可以通过TCP协议连接任意一台服务器，如果客户端是读操作请求，则任意一个服务器都可以直接响应请求；如果是更新数据操作（写数据或者更新数据），则只能由主控服务器来协调更新操作；如果客户端连接的是从属服务器，则从属服务器会将更新数据请求转发到主控服务器，由其完成更新操作。

![[file-20250923200647825.jpg | 500]]

主控服务器将所有更新操作序列化，利用ZAB协议将数据更新请求通知所有从属服务器，ZAB保证更新操作的一致性及顺序性。

ZooKeeper的任意一台服务器都可以响应客户端的读操作，这是为何其吞吐量高的主要原因。

Chubby在这点上与ZooKeeper不同，所有读／写操作都由主控服务器完成，从属服务器只是为了提高整个协调系统的可用性，即主控服务器发生故障后能够在从属服务器中快速选举出新的主控服务器。

- **潜在问题**：客户端可能会读到过期数据
- **解决方案**：在ZooKeeper的接口API函数中提供了Sync操作，应用可以根据需要在读数据前调用该操作，其含义是：接收到Sync命令的从属服务器从主控服务器同步状态信息，保证两者完全一致。这样如果在读操作前调用Sync操作，则可以保证客户端一定可以读取到最新状态的数据。

ZooKeeper通过“重放日志（Replay log）”结合“模糊快照（Fuzzy Snapshot）”来对服务器故障进行容错。
- “重放日志”：在将更新操作体现在内存数据之前先写入外存日志中避免数据丢失；
- “模糊快照”：在周期性对内存数据做数据快照时，并不对内存数据加锁，而是用深度遍历的方式将内存中的树形结构转入外存快照数据中

### 数据模型

与Chubby一样，ZooKeeper的内存数据模型类似于传统的文件系统模式，由树形的层级目录结构构成，其中的节点被称作Znode。Znode可以是文件，也可以是目录，如果是目录的话还可以有子目录。如果是文件的话，一般需要整体完成读／写操作的小文件，这与Chubby一样是出于避免应用将协调系统当作存储系统来用。

![[file-20250923201038244.jpg | 300]]
Znode节点有两种类型：
- 持久节点不论客户端会话情况，一直存在，只有当客户端显式调用删除操作才会消失。
- 临时节点不同，会在客户端会话结束或者发生故障的时候被ZooKeeper系统自动清除。

### API

ZooKeeper提供了简单的操作原语供应用使用，这些操作原语类似于文件系统的调用接口，含义很清楚。

sync操作的语义：其含义是通知客户端连接到的那个ZooKeeper服务器，将其内存数据库内容从其他服务器同步到最新状态，这是为了避免读到过期数据。

### ZooKeeper的典型应用场景

#### 1．领导者选举（Leader Election）

分布式系统中一种经典的体系结构是主从结构（Master-Slave），主控服务器负责全局管理控制工作，而从节点负责具体的任务计算或者数据存储管理工作。

为了防止单点失效，往往会采取一主一备或者一主多备，当主控服务器发生故障后，由某台备机接管主控服务器功能成为新的主控机，而这一般被称为领导者选举，ZooKeeper是解决这类问题的常见解决方案。

ZooKeeper在实现领导者选举时，将临时节点Zl设置为领导者专用节点，节点内容中存储领导者的地址信息及其他辅助信息。每个进程（或者机器）执行以下逻辑实现领导者选举过程。

进程p读取Zl内容并设置观察标识，如果读取操作成功，说明目前已有领导者并可从读取结果中获得领导者相关信息；如果读取失败，说明目前无领导者，则进程p试图自己创建该节点（临时节点方式），并将自己的相关信息写入。如果创建并写入成功则p成为领导者，其他非领导者因为设置了观察标识，所以ZooKeeper会通知所有非领导者说领导者发生了变化，非领导者可以读取Zl内容获取最新领导者信息。

#### 2．配置管理（Configuration Management）

ZooKeeper可以用来进行配置信息的动态管理,配置文件存储在ZooKeeper的某个节点Zc中，分布式系统中的客户端进程在启动时从Zc 中读取配置信息，并设置观察标记。若配置文件内容在以后被改变，客户端进程会接收到Zc 的变化通知，可以再次读取Zc 节点内容以捕获变化点并同时再次设置观察标记，这样以后每次配置文件的变化客户端都可以及时收到通知。

![[file-20250923202014501.jpg | 400]]

#### 3．组成员管理（Group Membership）

组成员管理的任务目标是动态监控一个组内成员的变化情况，比如有成员加入群组或者离开群组。一个典型的应用场景，比如工作服务器（Worker）的动态添加与故障发现，如果整个系统工作负载太大，可以新增工作服务器来进行负载均衡，而主控服务器如何自动发现新加入的机器则是组成员管理问题；

ZooKeeper可以利用临时节点来进行组成员管理.

#### 4．任务分配

在分布式环境下，将不同的任务负载分别分配到多台可用服务器也是一个比较常见的问题。使用ZooKeeper可以比较方便地实施这一过程
![[file-20250923202134282.jpg | 400]]
对于监控进程来说，可以创建任务队列管理节点tasks，所有新进入系统的任务都可以在tasks节点下创建子节点，监控进程观察tasks节点的变化。

#### 5．锁管理（Locks）

可以利用其提供的原语构造锁服务功能。可以用ZooKeeper实现读／写锁、排他锁。

#### 6．双向路障同步（Double Barrier）

所谓路障同步，是指多个并发进程都要到达某个同步点后再继续向后推进，图计算中的BSP模型就是一个典型的路障同步算法，只有所有并发进程的上一轮计算都完全到达同步点后才能开始下一轮计算。

ZooKeeper可以用来进行并发进程的双向路障同步。

## ZooKeeper的实际应用

- ZooKeeper在HBase的使用场景包括主控服务器选举与主备切换，作为配置管理在ZooKeeper中存储系统启动信息，发现新的子表服务器及侦测子表服务器是否依然存活等。
- LinkedIn的Pub-Sub消息系统Kafka在以下场景使用ZooKeeper：自动发现新添加的消息服务器（Broker）和消息消费者（Consumer）；在消息服务器间进行自动负载均衡；在ZooKeeper里保存消费者和消息队列的映射关系及消费者当前消费信息在消息队列的位置等。
# 第6章——分布式通信


