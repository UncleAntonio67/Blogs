---
tags:
  - 技术书籍
aliases:
  - 从零构建大模型
time: 20251213——
author: Sebastian Raschka
website: https://skindhu.github.io/Build-A-Large-Language-Model-CN/
---
# 目录

第1章——理解大预言模型
第2章——处理文本数据
第3章——编码注意力机制
第4章——从头实现GPT模型进行文本生成
第5章——在无标签数据上进行预训练
第6章——针对分类的微调
第7章——通过微调遵循人类指令

# 第1章 理解大语言模型

**深度学习**是**机器学习**和**人工智能**领域的一个重要分支，主要聚焦于神经网络的研究。

基于Transformer架构并利用数据集来训练大预言模型的转变，已经从根本上变革了自然语言处理领域，为机器理解并为人类语言互动提供了更强大的工具。

## 1.1 什么是大语言模型

大语言模型：用于理解、生成和响应类似人类语言文本呢的神经网络。

使用了Transformer架构，允许模型在进行预测时有选择的关注输入文本的不同部分，使得他们特别擅长应对人类语言的细微差别和复杂性。

![[1765630747635.png | 300]]

## 1.2 大语言模型的应用

机器翻译、文本生成、情感分析、文本摘要等等

| LLM               | 多模態 | 推理  | 工具運用 |
| ----------------- | --- | --- | ---- |
| GPT-4o            | ✅   | 🟡  | ✅    |
| Claude 4 Sonnet   | ✅   | ✅   | ✅    |
| Grok 3            | ❌   | ✅   | ✅    |
| o3                | ❌   | ✅   | ✅    |
| Claude 4 Opus     | ✅   | ✅   | ✅    |
| Gemini 2.5 Pro    | ✅   | ✅   | ✅    |
| DeepSeek R1       | ❌   | ✅   | ✅    |
| Gemma 3（4B）       | ❌   | ❌   | ❌    |
| Mistral Small 3.1 | ✅   | 🟡  | 🟡   |
| Qwen 3（4B）        | ❌   | 🟡  | ✅    |
## 1.3 构建和使用大预言模型的各个阶段

大语言模型的构建通常包括预训练和微调：
- 预训练：模型的初始阶段，模型会在大规模、多样化的数据集上训练，形成全面的语言理解能力（自监督学习）
- 微调：以预训练为基础，在特定任务、领域进行针对性训练。

![[1765633485916.png | 400]]
微调大模型最流行的方式：
- 指令微调：标注数据集由“指令-答案”对组成
- 分类微调：标注数据集由文本及其类别标签组成

## 1.4 Transformer架构介绍

谷歌2017年发布论文“Attention is All your Need”提出，Transformer最早是为机器翻译任务开发。

![[file-20251213215159224.png | 400]]

两个子模块构成：
- **编码器**：负责处理输入文本，将其编码为一系列数值表示或者向量，捕捉输入的上下文信息
- **解码器**：接收这些向量，生成输出文本。

**注意力机制**：允许模型衡量序列中不同单词或词元之间的相对重要性，能使得模型捕获到输入数据中长距离的依赖和上下文关系，提升生成连贯且上下文相关的输出的能力。

为了适应不同类型的下游，Transformer由不同的变体：
- BERT（Bidirectional Encoder Representations from Transformer）：专注于掩码训练，预测给定距离被掩码的词。主要用于情感预测、文档分类
- GPT（Generative Pretrained Transformer）：主要用于处理生成文本的任务，包括机器翻译、文本摘要、小说创作、文本补全等等。
## 1.5 利用大型数据集

GPT3的预训练数据集：

|**数据集**|**预训练数据量 (Token数量)**|**占总权重 (Approx.)**|
|---|---|---|
|**CommonCrawl (过滤后)**|约 4100 亿|60%|
|**WebText2**|约 190 亿|22%|
|**Books1 & Books2**|约 670 亿|16%|
|**Wikipedia**|约 30 亿|3%|
|**总计**|约 5000 亿|100%|
**词元**（Token）：约等于单词和标点符号的数量。

*训练GPT-3的云计算费用成本高达460万美元

## 1.6 深入剖析GPT架构

GPT的通用架构更为简洁，只包含解码器，因为它是逐词预测，因为它被认为是一种**自回归模型**，将之前的输出作为未来预测的输入。

*GPT-3的规模远超Transformer架构。原始的Transformer架构将编解码器重复6次，GPT-3总共有96层Transformer和1750亿参数。

模型能够完成未经明确训练的任务的能力称为**涌现**，充分体现了这类大规模生成式语言模型的优势。

## 1.7 构建大语言模型


![[file-20251213222512518.jpg]]

1. 学习数据预处理、着手实现大语言模型的核心组件——注意力机制
2. 学习如何编写代码并预训练一个能够生成新文本的类GPT模型
3. 进行微调，使其回答查询、分类等任务


# 第2章 处理文本数据

