---
tags:
  - 技术书籍
aliases:
  - 从零构建大模型
time: 20251213——
author: Sebastian Raschka
website: https://skindhu.github.io/Build-A-Large-Language-Model-CN/
---
# 目录

第1章——理解大预言模型
第2章——处理文本数据
第3章——编码注意力机制
第4章——从头实现GPT模型进行文本生成
第5章——在无标签数据上进行预训练
第6章——针对分类的微调
第7章——通过微调遵循人类指令

# 第1章 理解大语言模型

**深度学习**是**机器学习**和**人工智能**领域的一个重要分支，主要聚焦于神经网络的研究。

基于Transformer架构并利用数据集来训练大预言模型的转变，已经从根本上变革了自然语言处理领域，为机器理解并为人类语言互动提供了更强大的工具。

## 1.1 什么是大语言模型

大语言模型：用于理解、生成和响应类似人类语言文本呢的神经网络。

使用了Transformer架构，允许模型在进行预测时有选择的关注输入文本的不同部分，使得他们特别擅长应对人类语言的细微差别和复杂性。

![[1765630747635.png | 300]]

## 1.2 大语言模型的应用

机器翻译、文本生成、情感分析、文本摘要等等

| LLM               | 多模態 | 推理  | 工具運用 |
| ----------------- | --- | --- | ---- |
| GPT-4o            | ✅   | 🟡  | ✅    |
| Claude 4 Sonnet   | ✅   | ✅   | ✅    |
| Grok 3            | ❌   | ✅   | ✅    |
| o3                | ❌   | ✅   | ✅    |
| Claude 4 Opus     | ✅   | ✅   | ✅    |
| Gemini 2.5 Pro    | ✅   | ✅   | ✅    |
| DeepSeek R1       | ❌   | ✅   | ✅    |
| Gemma 3（4B）       | ❌   | ❌   | ❌    |
| Mistral Small 3.1 | ✅   | 🟡  | 🟡   |
| Qwen 3（4B）        | ❌   | 🟡  | ✅    |
## 1.3 构建和使用大预言模型的各个阶段

大语言模型的构建通常包括预训练和微调：
- 预训练：模型的初始阶段，模型会在大规模、多样化的数据集上训练，形成全面的语言理解能力（自监督学习）
- 微调：以预训练为基础，在特定任务、领域进行针对性训练。

![[1765633485916.png | 400]]
微调大模型最流行的方式：
- 指令微调：标注数据集由“指令-答案”对组成
- 分类微调：标注数据集由文本及其类别标签组成

## 1.4 Transformer架构介绍

谷歌2017年发布论文“Attention is All your Need”提出，Transformer最早是为机器翻译任务开发。

![[file-20251213215159224.png | 400]]

两个子模块构成：
- **编码器**：负责处理输入文本，将其编码为一系列数值表示或者向量，捕捉输入的上下文信息
- **解码器**：接收这些向量，生成输出文本。

**注意力机制**：允许模型衡量序列中不同单词或词元之间的相对重要性，能使得模型捕获到输入数据中长距离的依赖和上下文关系，提升生成连贯且上下文相关的输出的能力。

为了适应不同类型的下游，Transformer由不同的变体：
- BERT（Bidirectional Encoder Representations from Transformer）：专注于掩码训练，预测给定距离被掩码的词。主要用于情感预测、文档分类
- GPT（Generative Pretrained Transformer）：主要用于处理生成文本的任务，包括机器翻译、文本摘要、小说创作、文本补全等等。
## 1.5 利用大型数据集

GPT3的预训练数据集：

|**数据集**|**预训练数据量 (Token数量)**|**占总权重 (Approx.)**|
|---|---|---|
|**CommonCrawl (过滤后)**|约 4100 亿|60%|
|**WebText2**|约 190 亿|22%|
|**Books1 & Books2**|约 670 亿|16%|
|**Wikipedia**|约 30 亿|3%|
|**总计**|约 5000 亿|100%|
**词元**（Token）：约等于单词和标点符号的数量。

*训练GPT-3的云计算费用成本高达460万美元

## 1.6 深入剖析GPT架构

GPT的通用架构更为简洁，只包含解码器，因为它是逐词预测，因为它被认为是一种**自回归模型**，将之前的输出作为未来预测的输入。

*GPT-3的规模远超Transformer架构。原始的Transformer架构将编解码器重复6次，GPT-3总共有96层Transformer和1750亿参数。

模型能够完成未经明确训练的任务的能力称为**涌现**，充分体现了这类大规模生成式语言模型的优势。

## 1.7 构建大语言模型


![[file-20251213222512518.jpg]]

1. 学习数据预处理、着手实现大语言模型的核心组件——注意力机制
2. 学习如何编写代码并预训练一个能够生成新文本的类GPT模型
3. 进行微调，使其回答查询、分类等任务


# 第2章 处理文本数据

如何为大模型准备输入文本，将文本分割为独立的单词词元和子词词元，高级的分词技术BPE，字节对编码

## 2.1 理解词嵌入

**嵌入**：把数据转换为向量格式，不同的数据需要使用不同的模型嵌入。嵌入的本质是将离散对象（如单词、图像或整个文档）映射到连续向量空间中的点。嵌入的主要目的是将非数值数据转换为神经网络能够处理的格式。

![[file-20251214220214060.png | 450]]

- 词嵌入最常见，Word2Vec是较早且最受欢迎的项目之一，核心思想：出现在相似上下文中的词通常具有相似的含义
- 句子嵌入、段落嵌入在检索增强生成（RAG）领域非常流行

*词嵌入的维度越高，越有助于捕捉到更细微的关系。

大预言模型通过会自动生成嵌入，嵌入式是输入层的一部分，这样可以针对特定的任务和数据进行优化。

GPT-2的嵌入维度：768；GPT-3的嵌入维度：12288

## 2.2 理解词嵌入

文本分为独立的词元：

![[file-20251214221312852.png | 500]]

通过正则表达式进行分词：
![[file-20251214223151151.png]]

## 2.3 将词元转换为词元ID

把这些词元从字符串转换为整数表示，生成词元ID。嵌入向量前的必经步骤。

词元-ID映射表：
![[file-20251214223459116.png]]
构建词元、ID互相构建的方法，encode：字符串——>整数以及ID，decode：整数——>字符串。

![[file-20251214223946209.png | 600]]

![[file-20251214224349338.png | 600]]

## 2.4 引入特殊上下文词元

在遇到词汇表不存在的单词时，可以使用特殊词元来代替，它也会插在不相关的文本间。

![[file-20251215225742125.png | 600]]

常见的特殊词元包括：
- `[BOS]`：marks the beginning of text
- `[EOS]`：marks where the text ends
- `[UNK]`： to represent words that are not included in the vocabulary
- `|endoftext|` tokens between two independent sources of text

- GPT模型使用的分词器不依赖这些特殊的词元，仅仅使用`|endoftext|` 简化处理流程
- GPT模型的分词器也不适用`[UNK]`来处理超出范围的单词，而是使用BPE分词器进行拆分

## 2.5 BPE

BPE分词器用于训练大语言模型，使用开源库tiktoken。

![[file-20251215231622871.png]]

两个观察结果：
- `|endoftext|`被分配了一个较大的词元，用于训练GPT中原始模型的词汇总量为50257，它时最大的词元
- BPE分词器可以正确的编码和解码未知的单词，原理是将不在预定义的单词分析为更小的单词甚至单个字符。
![[file-20251215232131093.png | 500]]


## 2.6 使用滑动窗口进行数据采样

生成用于模型训练的输入-目标对

![[file-20251216210757780.png | 500]]
![[file-20251216211206063.png | 300]]

![[file-20251216211357418.png | 600]]

这样，用于训练的输入-目标对就创建好了

在转化为嵌入向量前，还需要实现一个高效的数据加载器：遍历输入数据集，将输入和目标以张量的形式范围。实际的代码会直接操作词元ID。

![[file-20251216211705044.png]]


![[file-20251216212057234.png]]

给了一个Dataloader例子，上下文长度是4，一般实际训练过程中是不小于256.
- stride决定的是输入之间的偏移量，可以增加到4来充分利用数据集，避免过拟合。

![[file-20251216212422662.png | 600]]

## 2.7 创建词元嵌入

将词元ID转换为嵌入向量

![[file-20251216213014134.png | 500]]

类GPT大模型使用反向传播算法训练的深度神经网络，因此需要连续的向量表示或嵌入。

词元嵌入向量的工作原理：
- 使用 `vocab_size` 和 `output_dim`在 PyTorch 中实例化一个嵌入层，并将随机种子设置为 123
- 查看嵌入层的基础权重矩阵，每一行代表词汇表中的一个token（每个token都有一个唯一的向量表示），而每一列代表嵌入空间中的一个维度（在这个例子中，嵌入维度为3，即每个token被表示为一个3维向量）。
- 嵌入层本质上是一个查找功能，通过token ID 从嵌入层的权重矩阵中检索行。

![[file-20251216214250705.png]]

**应用这个方法，将词元转为嵌入向量，就是从嵌入权重矩阵中查找得来的。**

![[file-20251216214724625.png]]
## 2.8 编码单词位置信息

嵌入向量还不能直接作为大模型的输入，大模型的一个缺陷就是它们的自注意力机制无法感知词元在序列中的位置或顺序，所以需要向大模型注入额外的位置信息。

![[file-20251216215612565.png | 500]]

可以采用两种位置信息嵌入策略：
- **绝对位置嵌入**：绝对位置嵌入与序列中的特定位置直接相关。对于输入序列中的每个位置，都会将一个唯一的绝对位置嵌入向量添加到token的嵌入向量中，以传达其确切位置
- **相对位置嵌入**：强调的是token之间的相对位置或距离。这意味着模型学习的是“相隔多远”的关系，而不是“在什么确切位置”。这样的优势在于，即使模型在训练时没有接触过不同的长度，它也可以更好地适应各种长度的序列。

![[file-20251216220031924.png | 600]]

OpenAI 的 GPT 模型使用绝对位置嵌入，这些嵌入在训练过程中进行优化。

- 使用一个258维，词汇量为50257的嵌入层进行词元嵌入。
- 开始绝对位置嵌入，创建一个维度相同的嵌入层
- 然后直接添加上去，这样就生成了大语言模型核心模块可以处理的嵌入输入实例。

![[file-20251216220943280.png | 400]]

**❓反向传播算法需求连续的向量表示或嵌入？**

LLM使用的Transformer架构，本质上是设计用来处理**连续的、数值化的数据**（即浮点数的张量）
- Embedding层将每个离散的ID映射到一个**稠密的、连续的实数向量**（例如，一个 $768$ 维的向量）。这种连续性使得向量对于损失函数是**可微分的**，这是使用反向传播算法（Backpropagation）进行训练和优化的**先决条件**。
- Embedding是一种“分布式表示”（Distributed Representation）。它不是用一个单一的维度代表一个词的全部意义，而是将词的意义分散存储在向量的**所有维度**上。

**❓嵌入层的构建过程，嵌入权重的生成？**

 随机初始化是为了**打破对称性**。如果所有的词汇都从同一个向量开始，那么在反向传播中，它们将总是以相同的方式更新，模型将无法区分它们。 在这一刻，这些向量**不包含任何语义信息**。词汇 $0$ 和词汇 $5$ 的向量在空间中的距离是随机的，不能代表它们的相似性。
- **优化**：Embedding 向量是深度神经网络中**可学习的参数**
	- **训练目标：** 模型（如GPT）的目标是最大化预测下一个词的概率，或者最小化其损失函数（Loss）。
	- **梯度流：** 当模型犯错时（例如，预测“猫”是“香蕉”），损失函数会计算误差。这个误差会通过**反向传播**算法，以梯度的形式流过整个网络，最终流回 Embedding 层。
	- **权重更新：** 梯度告诉优化器（如Adam）：“为了减少这个错误，你应该往哪个方向微调‘猫’和‘香蕉’的向量？” 优化器就会按照梯度方向**更新**这些随机初始化的向量。

**❓绝对位置增加的嵌入层？**

加法确保了**词嵌入**和**对应位置编码**的精确配对。
- **词元 A**（在位置 0）：$\text{Token}_A + \text{PE}_0$
- **词元 B**（在位置 1）：$\text{Token}_B + \text{PE}_1$
- **词元 C**（在位置 2）：$\text{Token}_C + \text{PE}_2$


# 3 编码注意力机制

