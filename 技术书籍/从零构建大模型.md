---
tags:
  - 技术书籍
aliases:
  - 从零构建大模型
time: 20251213——
author: Sebastian Raschka
website: https://skindhu.github.io/Build-A-Large-Language-Model-CN/
---
# 目录

第1章——理解大预言模型
第2章——处理文本数据
第3章——编码注意力机制
第4章——从头实现GPT模型进行文本生成
第5章——在无标签数据上进行预训练
第6章——针对分类的微调
第7章——通过微调遵循人类指令

# 第1章 理解大语言模型

**深度学习**是**机器学习**和**人工智能**领域的一个重要分支，主要聚焦于神经网络的研究。

基于Transformer架构并利用数据集来训练大预言模型的转变，已经从根本上变革了自然语言处理领域，为机器理解并为人类语言互动提供了更强大的工具。

## 1.1 什么是大语言模型

大语言模型：用于理解、生成和响应类似人类语言文本呢的神经网络。

使用了Transformer架构，允许模型在进行预测时有选择的关注输入文本的不同部分，使得他们特别擅长应对人类语言的细微差别和复杂性。

![[1765630747635.png | 300]]

## 1.2 大语言模型的应用

机器翻译、文本生成、情感分析、文本摘要等等

| LLM               | 多模態 | 推理  | 工具運用 |
| ----------------- | --- | --- | ---- |
| GPT-4o            | ✅   | 🟡  | ✅    |
| Claude 4 Sonnet   | ✅   | ✅   | ✅    |
| Grok 3            | ❌   | ✅   | ✅    |
| o3                | ❌   | ✅   | ✅    |
| Claude 4 Opus     | ✅   | ✅   | ✅    |
| Gemini 2.5 Pro    | ✅   | ✅   | ✅    |
| DeepSeek R1       | ❌   | ✅   | ✅    |
| Gemma 3（4B）       | ❌   | ❌   | ❌    |
| Mistral Small 3.1 | ✅   | 🟡  | 🟡   |
| Qwen 3（4B）        | ❌   | 🟡  | ✅    |
## 1.3 构建和使用大预言模型的各个阶段

大语言模型的构建通常包括预训练和微调：
- 预训练：模型的初始阶段，模型会在大规模、多样化的数据集上训练，形成全面的语言理解能力（自监督学习）
- 微调：以预训练为基础，在特定任务、领域进行针对性训练。

![[1765633485916.png | 400]]
微调大模型最流行的方式：
- 指令微调：标注数据集由“指令-答案”对组成
- 分类微调：标注数据集由文本及其类别标签组成

## 1.4 Transformer架构介绍

谷歌2017年发布论文“Attention is All your Need”提出，Transformer最早是为机器翻译任务开发。

![[file-20251213215159224.png | 400]]

两个子模块构成：
- **编码器**：负责处理输入文本，将其编码为一系列数值表示或者向量，捕捉输入的上下文信息
- **解码器**：接收这些向量，生成输出文本。

**注意力机制**：允许模型衡量序列中不同单词或词元之间的相对重要性，能使得模型捕获到输入数据中长距离的依赖和上下文关系，提升生成连贯且上下文相关的输出的能力。

为了适应不同类型的下游，Transformer由不同的变体：
- BERT（Bidirectional Encoder Representations from Transformer）：专注于掩码训练，预测给定距离被掩码的词。主要用于情感预测、文档分类
- GPT（Generative Pretrained Transformer）：主要用于处理生成文本的任务，包括机器翻译、文本摘要、小说创作、文本补全等等。
## 1.5 利用大型数据集

GPT3的预训练数据集：

|**数据集**|**预训练数据量 (Token数量)**|**占总权重 (Approx.)**|
|---|---|---|
|**CommonCrawl (过滤后)**|约 4100 亿|60%|
|**WebText2**|约 190 亿|22%|
|**Books1 & Books2**|约 670 亿|16%|
|**Wikipedia**|约 30 亿|3%|
|**总计**|约 5000 亿|100%|
**词元**（Token）：约等于单词和标点符号的数量。

*训练GPT-3的云计算费用成本高达460万美元

## 1.6 深入剖析GPT架构

GPT的通用架构更为简洁，只包含解码器，因为它是逐词预测，因为它被认为是一种**自回归模型**，将之前的输出作为未来预测的输入。

*GPT-3的规模远超Transformer架构。原始的Transformer架构将编解码器重复6次，GPT-3总共有96层Transformer和1750亿参数。

模型能够完成未经明确训练的任务的能力称为**涌现**，充分体现了这类大规模生成式语言模型的优势。

## 1.7 构建大语言模型


![[file-20251213222512518.jpg]]

1. 学习数据预处理、着手实现大语言模型的核心组件——注意力机制
2. 学习如何编写代码并预训练一个能够生成新文本的类GPT模型
3. 进行微调，使其回答查询、分类等任务


# 第2章 处理文本数据

如何为大模型准备输入文本，将文本分割为独立的单词词元和子词词元，高级的分词技术BPE，字节对编码

## 2.1 理解词嵌入

**嵌入**：把数据转换为向量格式，不同的数据需要使用不同的模型嵌入。嵌入的本质是将离散对象（如单词、图像或整个文档）映射到连续向量空间中的点。嵌入的主要目的是将非数值数据转换为神经网络能够处理的格式。

![[file-20251214220214060.png | 450]]

- 词嵌入最常见，Word2Vec是较早且最受欢迎的项目之一，核心思想：出现在相似上下文中的词通常具有相似的含义
- 句子嵌入、段落嵌入在检索增强生成（RAG）领域非常流行

*词嵌入的维度越高，越有助于捕捉到更细微的关系。

大预言模型通过会自动生成嵌入，嵌入式是输入层的一部分，这样可以针对特定的任务和数据进行优化。

GPT-2的嵌入维度：768；GPT-3的嵌入维度：12288

## 2.2 理解词嵌入

文本分为独立的词元：

![[file-20251214221312852.png | 500]]

通过正则表达式进行分词：
![[file-20251214223151151.png]]

## 2.3 将词元转换为词元ID

把这些词元从字符串转换为整数表示，生成词元ID。嵌入向量前的必经步骤。

词元-ID映射表：
![[file-20251214223459116.png]]
构建词元、ID互相构建的方法，encode：字符串——>整数以及ID，decode：整数——>字符串。

![[file-20251214223946209.png | 600]]

![[file-20251214224349338.png | 600]]

## 2.4 引入特殊上下文词元





