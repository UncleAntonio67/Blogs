---
tags:
  - 技术书籍
aliases:
  - 从零构建大模型
time: 20251213——
author: Sebastian Raschka
website: https://skindhu.github.io/Build-A-Large-Language-Model-CN/
---
# 目录

第1章——理解大预言模型
第2章——处理文本数据
第3章——编码注意力机制
第4章——从头实现GPT模型进行文本生成
第5章——在无标签数据上进行预训练
第6章——针对分类的微调
第7章——通过微调遵循人类指令

# 第1章 理解大语言模型

**深度学习**是**机器学习**和**人工智能**领域的一个重要分支，主要聚焦于神经网络的研究。

基于Transformer架构并利用数据集来训练大预言模型的转变，已经从根本上变革了自然语言处理领域，为机器理解并为人类语言互动提供了更强大的工具。

## 1.1 什么是大语言模型

大语言模型：用于理解、生成和响应类似人类语言文本呢的神经网络。

使用了Transformer架构，允许模型在进行预测时有选择的关注输入文本的不同部分，使得他们特别擅长应对人类语言的细微差别和复杂性。

![[1765630747635.png | 300]]

## 1.2 大语言模型的应用

机器翻译、文本生成、情感分析、文本摘要等等

| LLM               | 多模態 | 推理  | 工具運用 |
| ----------------- | --- | --- | ---- |
| GPT-4o            | ✅   | 🟡  | ✅    |
| Claude 4 Sonnet   | ✅   | ✅   | ✅    |
| Grok 3            | ❌   | ✅   | ✅    |
| o3                | ❌   | ✅   | ✅    |
| Claude 4 Opus     | ✅   | ✅   | ✅    |
| Gemini 2.5 Pro    | ✅   | ✅   | ✅    |
| DeepSeek R1       | ❌   | ✅   | ✅    |
| Gemma 3（4B）       | ❌   | ❌   | ❌    |
| Mistral Small 3.1 | ✅   | 🟡  | 🟡   |
| Qwen 3（4B）        | ❌   | 🟡  | ✅    |
## 1.3 构建和使用大预言模型的各个阶段

大语言模型的构建通常包括预训练和微调：
- 预训练：模型的初始阶段，模型会在大规模、多样化的数据集上训练，形成全面的语言理解能力（自监督学习）
- 微调：以预训练为基础，在特定任务、领域进行针对性训练。

![[1765633485916.png | 400]]
微调大模型最流行的方式：
- 指令微调：标注数据集由“指令-答案”对组成
- 分类微调：标注数据集由文本及其类别标签组成

## 1.4 Transformer架构介绍

谷歌2017年发布论文“Attention is All your Need”提出，Transformer最早是为机器翻译任务开发。

![[file-20251213215159224.png | 400]]

两个子模块构成：
- **编码器**：负责处理输入文本，将其编码为一系列数值表示或者向量，捕捉输入的上下文信息
- **解码器**：接收这些向量，生成输出文本。

**注意力机制**：允许模型衡量序列中不同单词或词元之间的相对重要性，能使得模型捕获到输入数据中长距离的依赖和上下文关系，提升生成连贯且上下文相关的输出的能力。

为了适应不同类型的下游，Transformer由不同的变体：
- BERT（Bidirectional Encoder Representations from Transformer）：专注于掩码训练，预测给定距离被掩码的词。主要用于情感预测、文档分类
- GPT（Generative Pretrained Transformer）：主要用于处理生成文本的任务，包括机器翻译、文本摘要、小说创作、文本补全等等。
## 1.5 利用大型数据集

GPT3的预训练数据集：

|**数据集**|**预训练数据量 (Token数量)**|**占总权重 (Approx.)**|
|---|---|---|
|**CommonCrawl (过滤后)**|约 4100 亿|60%|
|**WebText2**|约 190 亿|22%|
|**Books1 & Books2**|约 670 亿|16%|
|**Wikipedia**|约 30 亿|3%|
|**总计**|约 5000 亿|100%|
**词元**（Token）：约等于单词和标点符号的数量。

*训练GPT-3的云计算费用成本高达460万美元

## 1.6 深入剖析GPT架构

GPT的通用架构更为简洁，只包含解码器，因为它是逐词预测，因为它被认为是一种**自回归模型**，将之前的输出作为未来预测的输入。

*GPT-3的规模远超Transformer架构。原始的Transformer架构将编解码器重复6次，GPT-3总共有96层Transformer和1750亿参数。

模型能够完成未经明确训练的任务的能力称为**涌现**，充分体现了这类大规模生成式语言模型的优势。

## 1.7 构建大语言模型


![[file-20251213222512518.jpg]]

1. 学习数据预处理、着手实现大语言模型的核心组件——注意力机制
2. 学习如何编写代码并预训练一个能够生成新文本的类GPT模型
3. 进行微调，使其回答查询、分类等任务


# 第2章 处理文本数据

如何为大模型准备输入文本，将文本分割为独立的单词词元和子词词元，高级的分词技术BPE，字节对编码

## 2.1 理解词嵌入

**嵌入**：把数据转换为向量格式，不同的数据需要使用不同的模型嵌入。嵌入的本质是将离散对象（如单词、图像或整个文档）映射到连续向量空间中的点。嵌入的主要目的是将非数值数据转换为神经网络能够处理的格式。

![[file-20251214220214060.png | 450]]

- 词嵌入最常见，Word2Vec是较早且最受欢迎的项目之一，核心思想：出现在相似上下文中的词通常具有相似的含义
- 句子嵌入、段落嵌入在检索增强生成（RAG）领域非常流行

*词嵌入的维度越高，越有助于捕捉到更细微的关系。

大预言模型通过会自动生成嵌入，嵌入式是输入层的一部分，这样可以针对特定的任务和数据进行优化。

GPT-2的嵌入维度：768；GPT-3的嵌入维度：12288

## 2.2 理解词嵌入

文本分为独立的词元：

![[file-20251214221312852.png | 500]]

通过正则表达式进行分词：
![[file-20251214223151151.png]]

## 2.3 将词元转换为词元ID

把这些词元从字符串转换为整数表示，生成词元ID。嵌入向量前的必经步骤。

词元-ID映射表：
![[file-20251214223459116.png]]
构建词元、ID互相构建的方法，encode：字符串——>整数以及ID，decode：整数——>字符串。

![[file-20251214223946209.png | 600]]

![[file-20251214224349338.png | 600]]

## 2.4 引入特殊上下文词元

在遇到词汇表不存在的单词时，可以使用特殊词元来代替，它也会插在不相关的文本间。

![[file-20251215225742125.png | 600]]

常见的特殊词元包括：
- `[BOS]`：marks the beginning of text
- `[EOS]`：marks where the text ends
- `[UNK]`： to represent words that are not included in the vocabulary
- `|endoftext|` tokens between two independent sources of text

- GPT模型使用的分词器不依赖这些特殊的词元，仅仅使用`|endoftext|` 简化处理流程
- GPT模型的分词器也不适用`[UNK]`来处理超出范围的单词，而是使用BPE分词器进行拆分

## 2.5 BPE

BPE分词器用于训练大语言模型，使用开源库tiktoken。

![[file-20251215231622871.png]]

两个观察结果：
- `|endoftext|`被分配了一个较大的词元，用于训练GPT中原始模型的词汇总量为50257，它时最大的词元
- BPE分词器可以正确的编码和解码未知的单词，原理是将不在预定义的单词分析为更小的单词甚至单个字符。
![[file-20251215232131093.png | 500]]


## 2.6 使用滑动窗口进行数据采样

生成用于模型训练的输入-目标对

![[file-20251216210757780.png | 500]]
![[file-20251216211206063.png | 300]]

![[file-20251216211357418.png | 600]]

这样，用于训练的输入-目标对就创建好了

在转化为嵌入向量前，还需要实现一个高效的数据加载器：遍历输入数据集，将输入和目标以张量的形式范围。实际的代码会直接操作词元ID。

![[file-20251216211705044.png]]


![[file-20251216212057234.png]]

给了一个Dataloader例子，上下文长度是4，一般实际训练过程中是不小于256.
- stride决定的是输入之间的偏移量，可以增加到4来充分利用数据集，避免过拟合。

![[file-20251216212422662.png | 600]]

## 2.7 创建词元嵌入

将词元ID转换为嵌入向量

![[file-20251216213014134.png | 500]]

类GPT大模型使用反向传播算法训练的深度神经网络，因此需要连续的向量表示或嵌入。

词元嵌入向量的工作原理：
- 使用 `vocab_size` 和 `output_dim`在 PyTorch 中实例化一个嵌入层，并将随机种子设置为 123
- 查看嵌入层的基础权重矩阵，每一行代表词汇表中的一个token（每个token都有一个唯一的向量表示），而每一列代表嵌入空间中的一个维度（在这个例子中，嵌入维度为3，即每个token被表示为一个3维向量）。
- 嵌入层本质上是一个查找功能，通过token ID 从嵌入层的权重矩阵中检索行。

![[file-20251216214250705.png]]

**应用这个方法，将词元转为嵌入向量，就是从嵌入权重矩阵中查找得来的。**

![[file-20251216214724625.png]]
## 2.8 编码单词位置信息

嵌入向量还不能直接作为大模型的输入，大模型的一个缺陷就是它们的自注意力机制无法感知词元在序列中的位置或顺序，所以需要向大模型注入额外的位置信息。

![[file-20251216215612565.png | 500]]

可以采用两种位置信息嵌入策略：
- **绝对位置嵌入**：绝对位置嵌入与序列中的特定位置直接相关。对于输入序列中的每个位置，都会将一个唯一的绝对位置嵌入向量添加到token的嵌入向量中，以传达其确切位置
- **相对位置嵌入**：强调的是token之间的相对位置或距离。这意味着模型学习的是“相隔多远”的关系，而不是“在什么确切位置”。这样的优势在于，即使模型在训练时没有接触过不同的长度，它也可以更好地适应各种长度的序列。

![[file-20251216220031924.png | 600]]

OpenAI 的 GPT 模型使用绝对位置嵌入，这些嵌入在训练过程中进行优化。

- 使用一个258维，词汇量为50257的嵌入层进行词元嵌入。
- 开始绝对位置嵌入，创建一个维度相同的嵌入层
- 然后直接添加上去，这样就生成了大语言模型核心模块可以处理的嵌入输入实例。

![[file-20251216220943280.png | 400]]

**❓反向传播算法需求连续的向量表示或嵌入？**

LLM使用的Transformer架构，本质上是设计用来处理**连续的、数值化的数据**（即浮点数的张量）
- Embedding层将每个离散的ID映射到一个**稠密的、连续的实数向量**（例如，一个 $768$ 维的向量）。这种连续性使得向量对于损失函数是**可微分的**，这是使用反向传播算法（Backpropagation）进行训练和优化的**先决条件**。
- Embedding是一种“分布式表示”（Distributed Representation）。它不是用一个单一的维度代表一个词的全部意义，而是将词的意义分散存储在向量的**所有维度**上。

**❓嵌入层的构建过程，嵌入权重的生成？**

 随机初始化是为了**打破对称性**。如果所有的词汇都从同一个向量开始，那么在反向传播中，它们将总是以相同的方式更新，模型将无法区分它们。 在这一刻，这些向量**不包含任何语义信息**。词汇 $0$ 和词汇 $5$ 的向量在空间中的距离是随机的，不能代表它们的相似性。
- **优化**：Embedding 向量是深度神经网络中**可学习的参数**
	- **训练目标：** 模型（如GPT）的目标是最大化预测下一个词的概率，或者最小化其损失函数（Loss）。
	- **梯度流：** 当模型犯错时（例如，预测“猫”是“香蕉”），损失函数会计算误差。这个误差会通过**反向传播**算法，以梯度的形式流过整个网络，最终流回 Embedding 层。
	- **权重更新：** 梯度告诉优化器（如Adam）：“为了减少这个错误，你应该往哪个方向微调‘猫’和‘香蕉’的向量？” 优化器就会按照梯度方向**更新**这些随机初始化的向量。

**❓绝对位置增加的嵌入层？**

加法确保了**词嵌入**和**对应位置编码**的精确配对。
- **词元 A**（在位置 0）：$\text{Token}_A + \text{PE}_0$
- **词元 B**（在位置 1）：$\text{Token}_B + \text{PE}_1$
- **词元 C**（在位置 2）：$\text{Token}_C + \text{PE}_2$


# 3 编码注意力机制

前面的章节已经告诉我们如何将文本分割成单词，将词元编码成向量表示。

本章探讨注意力机制。

![[1766064684785.png]]
上图给出了4种注意力机制的变体。

## 3.1 长序列建模中的问题

文本翻译时不能逐词翻译，源和目标语言的语法结构不同。

![[file-20251218213326053.png | 600]]

为了解决逐词翻译的局限性，通常使用包含两个子模块的深度神经网络，即所谓的编码器（encoder）和解码器（decoder）。编码器的任务是先读取并处理整个文本，然后解码器生成翻译后的文本。

Transformer出现前，最流行的用于语言翻译的编码器-解码器架构是RNN。

**RNN：** 将前一步的输出作为当前的输入，适合处理文本序列数据。输入文本被输入到编码器中，编码器按顺序处理文本内容。在每个步骤中，编码器会更新其隐状态（即隐藏层的内部值），试图在最终的隐状态中捕捉整个输入句子的含义。

![[file-20251218213716380.png | 600]]

编码器部分将整个输入文本处理为一个隐藏状态（记忆单元）。解码器随后使用该隐藏状态生成输出。您可以将这个隐藏状态视为一个嵌入向量。

**缺点**：RNN无法直接访问编码器中的早期隐藏状态，只能依赖当前的隐藏状态。在依赖关系较长的复杂句子中，会导致上下文信息的丢失。

编码器-解码器 RNN 存在一个缺点，这一缺点促使了注意力机制的设计。

## 3.2 通过注意力机制捕捉数据依赖关系

研究人员在 2014 年为 RNN 开发了所谓的 Bahdanau 注意力机制。该机制对编码器-解码器架构的 RNN 进行了改进，使得解码器在每个解码步骤可以选择性地访问输入序列的不同部分。

![[file-20251218214331599.png | 600]]

三年后，研究人员发现构建用于自然语言处理的深度神经网络并不需要 RNN 结构，随后提出了基于自注意力机制的原始 Transformer 架构，其灵感来自 Bahdanau 提出的注意力机制。

自注意力机制允许输入序列中的每个位置在计算序列表示时关注同一序列中所有位置的机制。自注意力机制是基于Transformer架构的当代大语言模型（如GPT系列模型）的关键组成部分。

本章将重点讲解并实现 GPT 类模型中使用的自注意力机制。

![[1766065589484.png | 400]]

## 3.3 通过自注意力机制关注输入的不同部分

**自注意力机制**： “self”指的是该机制通过关联同一输入序列中的不同位置来计算注意力权重的能力。它评估并学习输入内部各部分之间的关系和依赖性，例如句子中的单词或图像中的像素。
**注意力机制**：关注的是两个不同序列间的关系，例如序列到序列模型中，注意力可能存在于输入序列和输出序列之间。

### 3.3.1 一种不含可训练权重的简化自注意力机制

![[1766066370769(1).png | 500]]
- 一个输入序列，记作 x，由 T 个元素组成，表示为 x(1) 到 x(T)。该序列通常代表文本，例如一个句子，并且该文本已被转换为 token 嵌入。
- 在自注意力机制中，目标是为输入序列中的每个元素 x(i) 计算其对应的上下文向量 z(i) 。上下文向量可以被解释为一种增强的嵌入向量，它们的目的是通过整合序列中所有其他元素的信息（如同一个句子中的其他词），为输入序列中的每个元素创建丰富的表示。这对大语言模型至关重要，因为模型需要理解句子中各个词之间的关系和关联性。

实现自注意力机制的第一步是计算中间值 **ω**，即注意力得分：
计算方式：通过计算查询 x(2) 与每个其他输入 token 的点积来确定这些得分。
较高的点积值表示向量之间有更高的对齐程度或相似度。在自注意力机制的背景下，点积决定了序列中元素之间的关注程度：点积值越高，两个元素之间的相似度和注意力得分就越高。


> [!NOTE]
> 实际上，每个输入Token会先通过权重矩阵W分别计算出它的Q、K、V三个向量，针对每一个目标token，Transformer会计算它的 `Q向量` 与其它所有的token的 `K向量` 的点积，以确定每个词对当前词的重要性.


![[file-20251218220442628.png]]
对于注意力分数进行归一化处理，归一化的主要目的是使注意力权重之和为 1。这种归一化是一种有助于解释和保持LLM训练稳定性的惯例：

![[file-20251218221920895.png]]
使用 softmax 函数来进行归一化，更擅长处理极端值，并且在训练过程中提供了更有利的梯度特性。softmax 函数确保注意力权重始终为正值。这使得输出可以被解释为概率或相对重要性，其中较高的权重表示更重要。

![[file-20251218222153932.png]]

这种简单的 softmax 实现（softmax_naive）在处理较大或较小的输入值时，可能会遇到数值不稳定性问题，例如上溢或下溢。因此，实际操作中，建议使用 PyTorch 的 softmax 实现，它经过了充分的性能优化：

![[file-20251218222259277.png]]

> [!NOTE]
> `Softmax`, 它是一种常用的激活函数，尤其在神经网络的分类任务中被广泛使用。它的作用是将一个任意的实数向量转换为一个概率分布，且所有元素的概率之和为 1。
> 在神经网络中，特别是分类模型（如图像分类、文本分类）中，Softmax 层通常用作最后一层输出。


最后一步：通过将嵌入后的输入 token x(i) 与相应的注意力权重相乘，再将所得向量求和来计算上下文向量 z(2)。
在计算并归一化注意力分数以获取查询x(2)的注意力权重后，计算上下文向量。

![[file-20251218224406980.png]]

### 3.3.2 为所有输入的 token 计算注意力权重

上一节，计算了第二个输入元素的注意力权重和上下文向量，现在，我们将扩展该计算，以对所有输入计算注意力权重和上下文向量。

![[file-20251218224906490.png | 500]]
![[file-20251218224944399.png | 400]]

在使用 PyTorch 时，像 `torch.softmax` 这样的函数中的 `dim` 参数指定了将在输入张量中的哪个维度上进行归一化计算。通过设置 `dim=-1`，我们指示 `softmax` 函数沿着 `attn_scores` 张量的最后一个维度进行归一化操作。如果 `attn_scores` 是一个二维张量（例如，形状为 `[行数, 列数]`），则 `dim=-1` 将沿列方向进行归一化，使得每一行的值（沿列方向求和）之和等于 1。

![[file-20251218230114079.png ]]
用这些注意力权重通过矩阵乘法计算所有的上下文向量。

![[file-20251218230730191.png]]

## 3.4  实现带有可训练权重的自注意力机制

接下来实现一种在原始 Transformer 架构、GPT 模型以及大多数其他流行的大语言模型中使用的自注意力机制。这种自注意力机制也被称为缩放点积注意力。
![[file-20251218231312741.png]]

带有可训练权重的自注意力机制是基于之前简化自注意力机制的改进：我们希望计算某个特定输入元素的嵌入向量的加权和来作为上下文向量。

最显著的区别在于引入了在模型训练过程中不断更新的权重矩阵。这些可训练的权重矩阵至关重要，它们使模型（特别是模型内部的注意力模块）能够学习生成“优质”的上下文向量。

### 3.4.1 逐步计算注意力权重



