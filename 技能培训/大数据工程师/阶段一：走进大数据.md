# 一.学好大数据先攻克Linux
## 1.Linux虚拟机安装配置

VMware安装虚拟机（已创建好），步骤：
1. 创建单个虚拟机
2. 克隆多个虚拟机
3. 使用MobaXTerm、SecureCRT连接虚拟机
![[file-20260104231143776.png | 400]]

## 2.Linux极速上手

**vi：文件编辑**
- 编辑文件内容，按“i”进行（编辑模式）
- 查找字符串，输入“/”，然后回车，按“n”是下一个（命令模式）
- 快速修改内容，输入行号，输入“set nu”可以显示行号（命令模式）
- 复制一行，连按两次“y”，然后按“p“（命令模式）
- 快速删除几行，光标定位到这一行，连按两次”d“删一行，按3个9加”d“是后面的全部删除（命令行）
- 移动到第一行、最后一行，最后一行”G“，第一行”gg“
- 如果有误关操作，那就删除对应的临时文件

**wc：统计字数**
- wc -c/m/l 文件名

**sort：文件排序**
- sort -n（字符串数值） -r（逆序） -k 2（指定列）文件名

**uniq：检查重复的行列**
- uniq -c（次数） -u（不重复的） 文件名  `只能对连续的去重`

**head：获取前n条命令**
- head -3 文件名

**date：获取当前时间**
- date 格式化参数

**ps：显示进程信息**
- ps -ef 显示系统内的所有进程，配合grep常用

**netstat：显示端口信息**
- netstat -anp 显示全部端口

**jps：显示Java进程信息**
- 显示当前用户启动的java进程信息

**top：动态监控进程信息**
- 动态显示系统消耗资源最多的进程信息

**kill：杀掉进程**
- kill PID 自杀，kill -9 PID 它杀

**grep：查找**
- 查找文件理符合条件的字符串，支持正则

**sed：自动编辑一个或多个文件**
- ‘2a\haha’，第二行添加haha

**awk：处理文本，文本分析工具**
- awk -F: '{print $1}' 文件名

## 3.Linux配置与shell实战

**静态IP设置：**

- 编辑ens33文件
```shell
vi /etc/sysconfig/network-scripts/ifcfg-ens33
```
- 修改BOOTPROTO属性为static，增加静态IP配置信息
```config
IPADDR = 192.168.182.100
GATEWAY = 
DNS1 = 
```
- 重启网卡
```shell
service netword restart
```

**hostname设置：**

- 临时设置
```shell
hostname tmpname
```
- 永久设置
```shell
vi /etc/hostname
```

**防火墙关闭：**

- 临时关闭
```shell
systemctl  stop firewalld
```
- 永久关闭
```shell
systemctl disable firewalld
```
*不同版本关闭的命令有差别*

**shell编程**

shell脚本一般后缀为.sh
```shell
#!/bin/bash
#first command
echo hello world
```
执行方式：bash xxx.sh或者sh hello.sh

增加执行权限，即可直接执行；如果没有./，会执行失败，需要在path中增加"."路径
```shell
chmod u+x hello.sh
./hello.sh

#限制每一步执行的情况
bash -x hello.sh
```

shell变量不需要声明，赋值不需要加空格
```shell
name=zs
echo ${name}haha
```

变量可以分为四种：
- `var_name=name` 本地变量，一般是临时变量，就只在当前shell脚本中有效
- `export var_name=name` 环境变量，环境变量对子shell进程有效，如果需要永久，需要添加到/etc/profile，输入`soucre /etc/profile`
- `$0 $1` 位置变量，hello.sh abc bcd
- `$?` 特殊变量，上一条命令的返回状态码；`$#` shell脚本所有参数的个数

变量和引号的特殊使用：
- 单引号不解析变量
- 双引号解析变量
- 反引号解析变量，并且获取值

shell中的循环
```shell
#!/bin/bash
#for循环格式1
for((i=0;i<10;i++))
do
echo $i
done
#for循环格式2
for i in 1 2 3
do
echo $i
done
#while循环格式1
while test 2 -gt 1
do
echo $i
done
#while循环格式2
while [ 2 -gt 1]
do
echo $i
done
```

shell中的判断：
```shell
#!/bin/bash
if [ $# -lt 1 ]
then
	echo "not found"
	exit 100
fi
flag=$1
if [ $flag -eq 1 ]
then
	echo one
elif [ $flag -eq 2 ]
then
	echo two
else
	echo two
fi
```

*shell文件格式注意适配操作系统环境*

扩展介绍：

- 脚本前面nohup，后面加&
```shell
nohub sh while.sh &
```
- 标准输出(1)、标准错误输出(2)、重定向(>：覆盖，>>：追加)
```shell
ll 1> a.txt
```

```shell
#把标准错误输出重定向到标准输出，输出到黑洞
nohup hello.sh >/dev/null 2>&1 &
```

- 定时器，周期性执行命令`*(m) *(h) *(d) *(m) *(w) user-name command`，下一个小时重新开始
- 
```shell
#查看服务状态
systemctl status crond
#启动/停止服务
systemctl start/stop crond
#执行定时服务 在脚本里添加对应的指令，将输出重定向
#查看执行日志
tail -f /var/log/cron
```

## 4.Linux总结及走进大数据

在linux上安装JDK
```shell
#解压
#重命名
#配置环境变化
export JAVA_HOME=/data/soft/jdk1.8
export PATH=.:$JAVA_HOME/bin:$PATH
#配置生效
source /etc/profile
```

**什么是大数据**

实时路况/今日头条/买披萨的故事

**4V特征**：Volume（量大）、Variety（格式多）、Velocity（速度快）、Value（价值密度低）

# 二.大数据起源之初识Hadoop

## 1.初始Haoop

Hadoop适合海量数据**分布式存储**和**分布式计算**，起名是他孩子的毛绒象玩具名字

Hadoop已经有很多发行版：
- Apache Hadoop：官方版本
- CDH：商业版本
- HDP：开源版本

版本演变历史：
- 1.x版：HDFS、MapReduce
- 2.x版：HDFS、MapReduce、YARN、Others
- 3.x版：HDFS、MapReduce、YARN、Others（Java8以上、HDFS支持纠删码、支持多NameNode、MR任务级本地优化）
## 2.两种安装方式

### 核心知识

- 伪分布式集群
- **分布式集群安装**

![[file-20260110121346631.png | 600]]

hadoop目录（相关变量导入path路径）：
- bin目录：启动hdfs、yarn相关命令
- sbin目录：启动或停止集群

正式启动前需要格式化：
```shell
#只能执行一次
hdfs namenode -format
#如果再格式化
rm -rf 日志目录
```

启动hadoop集群
```shell
sbin/start-all.sh
#查看进程信息
jps
```


**主要步骤**：
1.配置静态IP、关防火墙、开启ssh免登录
2.下载安装hadoop、jdk、配置hadoop相关文件
3.启动Hadoop服务

**实际工作中，通过客户端节点连接集群进行操作，不直接连接节点进行操作**
![[file-20260110182455364.png | 550]]

### 实验操作

1. 节点基础信息表

| **主机名**    | **IP 地址**         | **内存** | **磁盘** | **操作系统**   | **角色描述**                                 |
| ---------- | ----------------- | ------ | ------ | ---------- | ---------------------------------------- |
| **node01** | `192.168.148.100` | 4G     | 40G    | CentOS 7.x | NameNode, Master,  Worker,ResouceManager |
| **node02** | `192.168.148.100` | 2G     | 40G    | CentOS 7.x | DataNode, Worker, NodeManager            |
| **node03** | `192.168.148.100` | 2G     | 40G    | CentOS 7.x | DataNode, Worker, NodeManager            |
2. 软件版本信息表

| **软件名称**   | **版本号**   | **安装路径**                     | **备注**          |
| ---------- | --------- | ---------------------------- | --------------- |
| **JDK**    | 1.8.0_333 | /usr/java/jdk1.8...          | 环境变量 JAVA_HOME  |
| **Hadoop** | 3.3.1     | /export/servers/hadoop-3.3.1 | 当前使用的版本         |
| **Spark**  | 3.3.1     | /export/servers/spark-3.3.1  | 对应 Hadoop 3 的版本 |
3. 核心端口号

| **服务名称**                 | **默认端口** | **浏览器访问地址             |
| ------------------------ | -------- | --------------------- |
| **HDFS Web UI**          | 9870     | `http://node01:9870`  |
| **YARN ResourceManager** | 18088    | `http://node01:8088`  |
| **Spark Master UI**      | 8080     | `http://node01:8080`  |
| **Spark History Server** | 18080    | `http://node01:18080` |
4. 配置文件

可参考官网配置：https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/ClusterSetup.html

```shell
#同步时间
ntpdate -u ntp.sjtu.edu.cn
```

```shell
#hadoop-env.sh
JAVA_HOME=/export/servers/jdk1.8.0_333
#hadoop-env.sh
<configuration>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/export/servers/hadoop-3.3.1/tmp</value>
        <description>Abase for other temporary directories.</description>
    </property>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://node01:9000</value>
    </property>
</configuration>
#hdfs-site.xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/export/servers/hadoop-3.3.1/tmp/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/export/servers/hadoop-3.3.1/tmp/dfs/data</value>
    </property>
</configuration>
#mapred-site.xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
#yarn-site.xml
<property>
    <name>yarn.resourcemanager.address</name>
    <value>node01:18040</value>
  </property>
#workers
node01
node02
node03
#改启停脚本的配置
```

- 启动HDFS
![[file-20260110181806560.png | 250]]![[file-20260110181843449.png | 250]]

# 三.Hadoop之HDFS的使用

## 1.HDFS介绍

![[file-20260110185723433.png | 600]]

![[file-20260110185807816.png | 600]]


允许文件通过网络在多台主机上分享的文件系统。不适合小文件存储

- shell操作
- ![[file-20260110190057002.png | 400]]

## 2.HDFS常见操作

### 核心知识

![[file-20260110190256266.png]]

- 查看文件：`hdfs dfs -ls hdfs://node01:9000/`
- 上传文件：`hdfs dfs -put README. hdfs://node01:9000/`
- 查看文件内容：`hdfs dfs -cat /README`
- 下载文件：`hdfs dfs -get /README .`
- 创建文件夹：`hdfs dfs -mkdir`
- 删除文件：`hdfs dfs -rm`

### 实验操作

![[file-20260110190920807.png]]


## 3.JAVA操作HDFS

### 核心知识

- 配置maven，更改setting目录，增加环境变量
- 新建maven项目，更新maven配置，添加hadoop依赖（需要调整hdfs权限）
- 开展增删改操作


### 实验操作

- 配置maven
![[file-20260110192759871.png]]
- 创建项目，更新配置
```yaml
<dependencies>  
    <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client -->  
    <dependency>  
        <groupId>org.apache.hadoop</groupId>  
        <artifactId>hadoop-client</artifactId>  
        <version>3.3.1</version>  
    </dependency></dependencies>
```

```java
package com.imooc.hdfs;  
  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.FSDataInputStream;  
import org.apache.hadoop.fs.FSDataOutputStream;  
import org.apache.hadoop.fs.FileSystem;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.io.IOUtils;  
  
import java.io.FileInputStream;  
import java.io.FileNotFoundException;  
import java.io.FileOutputStream;  
import java.io.IOException;  
  
/**  
 * Java代码操作HDFS  
 * 文件操作：上传、下载、删除  
 */  
public class HdfsOps {  
  
    public static void main(String[] args) throws IOException {  
        Configuration conf = new Configuration();  
        conf.set("fs.defaultFS","hdfs://node01:9000");  
        FileSystem fileSystem = FileSystem.get(conf);  
  
        //上传文件  
        put(fileSystem);  
        //下载文件  
        get(fileSystem);  
        //删除文件  
        delete(fileSystem);  
    }  
  
    private static void delete(FileSystem fileSystem) throws IOException {  
        boolean flag = fileSystem.delete(new Path("/算法图解.epub"),true);  
        if(flag){  
            System.out.println("删除成功");  
        }else{  
            System.out.println("删除失败");  
        }  
    }  
  
    private static void get(FileSystem fileSystem) throws IOException {  
        FSDataInputStream fis = fileSystem.open(new Path("/算法图解.epub"));  
        FileOutputStream fos = new FileOutputStream("D:\\算法图解.epub");  
        //下载文件  
        IOUtils.copyBytes(fis,fos,1024,true);  
    }  
  
    private static void put(FileSystem fileSystem) throws IOException {  
        //获取本地文件输入流  
        FileInputStream fis = new FileInputStream("G:\\Kindle\\算法图解.epub");  
        FSDataOutputStream fos = fileSystem.create(new Path("/算法图解.epub"));  
        //通过工具类把输入类拷贝到输出流  
        IOUtils.copyBytes(fis,fos,1024,true);  
    }  
  
}
```

# 四.HDFS核心进程剖析

## 1.初识NameNode

### 核心知识

- HDFS支持主从结构，主节点NameNode（支持多个），从节点DataNode（支持多个）
- 还包含SecondaryNameNode进程

HDFS体系结构：
![[file-20260111110138811.png | 500]]

**NameNode**：
- 整个文件系统的管理节点
- 维护整个文件系统的文件目录树、文件对应的数据库、负责接收用户操作请求
![[file-20260111130857550.png | 300]]
这些文件所在路径在hdfs-default.xml中配置：
![[file-20260111131459243.png | 300]]

- fsimage：定时增加，文件系统的镜像，md5是镜像信息。维护了整个文件系统、文件的元数据信息
- edits：实时增加，事务文件，记录上传到哪、上传状态，都上传成功才会记录。edits文件会定期合并到fsimage，框架中SecondaryNameNode进行转换合并
- seen_txid：代表edits文件中的尾数，格式化之后是0，NameNode重启时会顺序读取到这个尾数，保证数据的一致性
- version：版本信息

| **维度**     | **fsimage (镜像文件)**                                  | **edits (编辑日志)**                        | **seen_txid (事务ID指示器)**                     | **VERSION (版本文件)**                             |
| ---------- | --------------------------------------------------- | --------------------------------------- | ------------------------------------------- | ---------------------------------------------- |
| **核心定义**   | HDFS 元数据的**全量快照**。                                  | HDFS 操作的**增量日志**。                       | 记录当前**最新 edits** 文件的后缀 ID。                  | 记录集群的**身份标识与版本**信息。                            |
| **存储内容**   | 文件系统的目录树、文件属性（权限、副本数）、Block 与文件的映射关系。               | 记录所有写操作（创建、删除、重命名、修改属性等）的具体步骤。          | 只有一个数字，对应 `edits_0000000000000xxx` 的起始编号。   | 集群 ID (clusterID)、节点 ID (datanodeUUID)、布局版本号等。 |
| **更新时机**   | **非实时更新**。仅在 Checkpoint（合并）发生时，由 SNN/Standby NN 生成。 | **实时更新**。只要有写操作，NameNode 就会将操作顺序追加到该文件。 | **实时更新**。每当旧的 edits 滚动生成新的 edits 时，该值递增。    | **格式化时生成**。通常在初次格式化或版本升级时更新。                   |
| **读写性能**   | 随机读写慢（因文件大），仅在启动或合并时读取。                             | **顺序追加写**，性能极高，保证了元数据操作的响应速度。           | 极小文件，纳秒级读写。                                 | 极小文件，仅在启动或握手时校验。                               |
| **故障恢复作用** | 提供恢复的**基准底板**，缩短数据加载的初始时间。                          | 记录从快照点到宕机瞬间的**所有补丁**，防止数据丢失。            | 告知 NameNode 重启时需要加载哪些 edits 文件，保证**顺序一致性**。 | 防止 DataNode 误连到错误的集群，保证**集群一致性**。              |
| **管理主体**   | NameNode 维护，SNN 进行转换合并。                             | NameNode 实时写入，HA 模式下由 JournalNode 共享。   | NameNode 自动维护。                              | NameNode 与 DataNode 共同持有并校验。                   |
### 实验操作

文件路径：`/export/servers/hadoop-3.3.1/tmp/dfs/name/current/`

- 查看fsimage
```shell
hdfs oiv -p XML -i fsimage_0000000000000000075 -o fsimage56.xml
```
![[file-20260111132051287.png]]

- 查看edits
```shell
hdfs oev -i edits_0000000000000000072-0000000000000000073 -o edits73.xml
```
![[file-20260111132601485.png | 300]]

## 2.NameNode进阶

### 核心知识

**SecondaryNameNode**：
- 定期将edits文件合并到fsimage中
- 合并操作成为checkpoint操作，合并时对edits进行转换
- HA架构中没有SecondaryNameNode，有standby NameNode实现

**DataNode**：
- 提供真实文件存储服务
- HDFS按照固定大小顺序对文件进行划分编号，每一个块为block，默认是128M
- 如果一个文件小于一个数据块的大小，是多大就占用多大

NameNode维护两份关系：

| **关系类型**  | **第一份：文件与数据块的关系 (File -> Blocks)**     | **第二份：数据块与节点的关系 (Block -> DataNodes)**            |
| --------- | -------------------------------------- | ------------------------------------------------- |
| **描述内容**  | 一个完整的文件被切分成了哪些 Block（Block ID）。        | 每一个具体的 Block 分别存储在哪些 DataNode 节点上。                |
| **持久化方式** | **持久化存储**。记录在 `fsimage` 和 `edits` 文件中。 | **不持久化**。保存在 NameNode 的**内存**中。                   |
| **获取方式**  | 启动时直接从本地磁盘的 `fsimage` 加载。              | 启动时由各 DataNode 主动向 NameNode 汇报（Block Report）动态构建。 |
| **变更频率**  | 低。仅在文件创建、删除或追加时改变。                     | 高。节点宕机、网络波动或负载均衡都会导致关系变化。                         |

每一个元数据都会占150Byte空间大小，不适合存储小文件

### 实验操作

文件路径：`/export/servers/hadoop-3.3.1/tmp/dfs/data/current/BP-1959411947-192.168.148.100-1768038081848/current/finalized/subdir0/subdir0/`

![[file-20260111134129241.png | 400]]

## 3.HDFS高级

### 核心知识

1. 回收站：默认没有开启，需要在core-site.xml中添加配置 `fs.trash.interval`
2. 安全模式：刚启动会进入安装模式，可以通过`hdfs dfsadmin -safemode get`\``hdfs dfsadmin -safemode leave`
3. 实战：定时上传数据到HDFS

**HDFS高可用和高扩展**：
- 高可用：JournalNodes用于同步edits数据，静态数据是JN保证，动态数据是DN保障，这种架构不需要SN。如果要自动切换，需要ZK集群。
![[file-20260111140141702.png | 600]]
- 高扩展：解决单一命名空间的问题，HDFS集群扩展性、性能更高效、更好的隔离性。

Federation+HA的应用：
![[file-20260111140553800.png]]


## 4.源码剖析

### 核心知识

**RPC**：远程过程调用，客户机请求服务器，跨主机跨进程，Hadoop整个体系结构都是基于RPC

**接口分析：**
- ClientProtocol：HDFS客户端与NameNode的接口
- DataNodeProtocol：DataNode和NameNode的接口
- NameNodeProtocol：SN和NN的接口

整体调用过程：https://po-1g604uqfa3ecabfe-1306704837.tcloudbaseapp.com/jump-mp.html?path=share&viewLinkId=608107fef346fb66d2d6d782

具体说明：

| **步骤**       | **涉及核心类**           | **关键方法**                  | **源码内部发生了什么？**                                                    |
| ------------ | ------------------- | ------------------------- | ----------------------------------------------------------------- |
| **1. 逻辑流开启** | `DFSClient`         | `primitiveCreate()`       | 内部构造 `DFSOutputStream`。这是整个上传的“大管家”。                              |
| **2. 建立管道**  | `DataStreamer`      | `nextBlockOutputStream()` | **最重要的方法**。它会调用 RPC 向 NameNode 申请 Block，并与 DataNode 建立 Socket 连接。 |
| **3. 数据切分**  | `DFSOutputStream`   | `writeChunk()`            | 用户调用 `write` 后，数据被切成 512B 的 Chunk，并计算 Checksum，随后打包成 **Packet**。  |
| **4. 异步推送**  | `DataStreamer`      | `run()`                   | 一个独立的线程。它不停地从 `dataQueue` 队列拿 Packet，顺着 Socket 丢进管道。              |
| **5. 状态确认**  | `ResponseProcessor` | `run()`                   | 另一个独立线程。专门在管道尽头等着 DataNode 回复：“我收到了”。                             |
| **6. 物理写入**  | `BlockReceiver`     | `receivePacket()`         | **DataNode 端**的源码。它负责把收到的 Packet 刷到本地磁盘，并传给 Pipeline 的下一位。        |

### 实验操作

|**组件名称**|**对应代码**|**作用描述**|
|---|---|---|
|**通信协议 (Protocol)**|`MyProtocal.java`|定义了客户端和服务器都能理解的“共同语言”（接口）。必须继承 `VersionedProtocol`。|
|**客户端 (Client/Proxy)**|`MyClient.java`|使用 `RPC.getProxy` 获取一个代理对象。它不关心逻辑怎么实现，只负责调用接口。|
|**服务端 (Server)**|`Myserver.java`|使用 `RPC.Builder` 构建并启动监听服务，将接口与具体的实现类绑定。|
|**具体实现 (Processor)**|`Myprotocolmpl.java`|真正执行业务逻辑的地方。服务器接收到请求后，会调用这个实现类。|

- 创建MyClient.java
```java
package com.imooc.rpc;  
  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.ipc.RPC;  
  
import java.net.InetSocketAddress;  
  
public class MyClient {  
    public static void main(String[] args) throws Exception{  
        //通过socket连接server  
        InetSocketAddress addr = new InetSocketAddress("localhost",1234);  
        Configuration conf = new Configuration();  
        MyProtocal proxy = RPC.getProxy(MyProtocal.class,MyProtocal.versionID,addr,conf);  
        String result = proxy.hello("RPC");  
        System.out.println("客户机收到的结果："+result);  
    }  
}
```
- 创建MyProtocal.java
```java
package com.imooc.rpc;  
  
import org.apache.hadoop.ipc.VersionedProtocol;  
  
/**  
 * 自定义RPC接口  
 */  
public interface MyProtocal extends VersionedProtocol {  
    long versionID = 123456L;  
    String hello(String name);  
}
```

- 创建Myprotocolmpl.java
```java
package com.imooc.rpc;  
  
import org.apache.hadoop.ipc.ProtocolSignature;  
  
import java.io.IOException;  
  
public class Myprotocolmpl implements MyProtocal{  
    @Override  
    public String hello(String name) {  
        System.out.println("我被调用了");  
        return "hello" + name;  
    }  
  
    @Override  
    public long getProtocolVersion(String protocol, long clientVersion) throws IOException {  
        return versionID;  
    }  
  
    @Override  
    public ProtocolSignature getProtocolSignature(String protocol, long clientVersion, int clientMethodsHash) throws IOException {  
        return new ProtocolSignature();  
    }  
}
```

- 创建Myserver.java
```java
package com.imooc.rpc;  
  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.ipc.RPC;  
  
import java.io.IOException;  
  
public class Myserver {  
    public static void main(String[] args) throws IOException {  
        Configuration conf = new Configuration();  
        RPC.Builder builder = new RPC.Builder(conf);  
        builder.setBindAddress("localhost")  
                .setPort(1234)  
                .setProtocol(MyProtocal.class)  
                .setInstance(new Myprotocolmpl());  
  
        RPC.Server server = builder.build();  
        server.start();  
        System.out.println("RPC服务器启动了");  
    }  
}
```

运行效果：
![[file-20260111144606133.png]]

# 五.初识MR

## 1.初始MapReduce

![[file-20260111153626941.png | 300]]
![[file-20260111153640802.png | 300]]
让计算去找数据，分为两阶段：
![[file-20260111153826823.png | 400]]

**MapReduce**：
- 分布式计算模型
- 有两个阶段组成，Map和Reduce

![[file-20260111154037993.png | 500]]

**原理剖析**：
![[file-20260111154149554.png]]

| **阶段**         | **核心动作**  | **逻辑对象**                     | **详细说明**                                                                    |
| -------------- | --------- | ---------------------------- | --------------------------------------------------------------------------- |
| **1. Input**   | **切片与读取** | `Split` & `[K1, V1]`         | 将 HDFS 中的物理 Block 逻辑切分为 Split。每个 Split 启动一个 Map Task，并将数据解析为键值对输入。          |
| **2. Map**     | **并行计算**  | `[K2, V2]`                   | Map Task 执行用户自定义逻辑，对每一条输入记录进行处理，产生中间结果 $[K2, V2]$。                          |
| **3. Shuffle** | **分区与排序** | `[K2, V2]` → `[K2, {V2...}]` | **核心枢纽**：负责将 Map 输出进行分区（Partition）、排序（Sort）和分组（Group），通过网络拷贝到对应的 Reduce 节点。 |
| **4. Group**   | **聚合归类**  | `[K2, {V2...}]`              | 属于同一分区的相同 Key 会被合并在一起，形成一个 Key 对应一组 Value 列表的结构，准备进入 Reduce。                |
| **5. Reduce**  | **汇总处理**  | `[K3, V3]`                   | Reduce Task 接收特定分区的所有数据，执行聚合逻辑（如 sum, count），生成最终的业务结果。                     |
| **6. Output**  | **结果落盘**  | `part-nnnnn`                 | 将计算结果以文件形式写回 HDFS。文件数量通常等同于 Reduce Task 的数量。                                |
## 2.WordCount实战

### 核心知识

**单个文件**：

![[file-20260111155350653.png]]

**多个文件**：
![[file-20260111155513376.png]]![[file-20260111155554716.png]]

| **阶段**         | **核心步骤**              | **详细操作与关键点**                                                                                                    |
| -------------- | --------------------- | --------------------------------------------------------------------------------------------------------------- |
| **一、代码开发**     | **1. Mapper 类编写**     | 继承 `Mapper` 类。关键在于通过 `value.toString()` 获取行文本，并使用 `split(" ")` 切割单词。输出为 `<Text, LongWritable>` 键值对，即 `<单词, 1>`。 |
|                | **2. Reducer 类编写**    | 继承 `Reducer` 类。通过 `for` 循环遍历相同单词的所有计数值（values），进行累加求和。最后输出 `<Text, LongWritable>`，即 `<单词, 总次数>`。                |
|                | **3. Driver 驱动类组装**   | 在 `main` 方法中初始化 `Job`。需指定 Job 的主类、Mapper/Reducer 类、Map 输出类型、最终输出类型，以及 HDFS 的输入输出路径。                             |
| **二、Maven 构建** | **1. 依赖管理 (pom.xml)** | 引入 `hadoop-client` 和 `hadoop-hdfs` 依赖。建议设置 `<scope>provided</scope>`，因为集群环境已自带这些 Jar 包。                         |
|                | **2. 打包插件配置**         | 配置 `maven-assembly-plugin` 生成“胖包”。**必须**在 `<mainClass>` 标签中填写驱动类的全限定名（如 `com.imooc.mr.WordCountJob`）。           |
|                | **3. 执行编译打包**         | 运行命令：`mvn clean package -DskipTests`。在 `target` 目录下获取包含 `-jar-with-dependencies.jar` 后缀的文件。                     |
| **三、环境准备**     | **1. Ubuntu 变量设置**    | 在 `~/.bashrc` 中添加 `HADOOP_HOME`。确保 `PATH` 中包含 `$HADOOP_HOME/bin` 和 `$HADOOP_HOME/sbin`，以便直接使用 `hadoop` 命令。      |
|                | **2. 模拟数据生成**         | 运行 Java 或 Python 脚本生成约 60MB 的 `wordcount_input.txt` 测试数据。                                                       |
|                | **3. 数据上传 HDFS**      | 启动 HDFS 服务后，执行 `hdfs dfs -put <本地文件> /test` 将测试数据上传至集群指定目录。                                                     |
| **四、任务执行**     | **1. 提交作业**           | 使用命令：`hadoop jar <jar包路径> <驱动类全路径> /test /out`。注意输出路径 `/out` 在执行前必须不存在。                                         |
|                | **2. YARN 界面监控**      | 访问 `http://<IP>:8088` 查看 YARN 资源管理页面。监控任务的运行状态、内存消耗及执行进度。                                                       |
| **五、结果验证**     | **1. 检查输出文件**         | 执行 `hdfs dfs -ls /out` 确认是否生成了成功的标志文件 `_SUCCESS` 和结果文件 `part-r-00000`。                                          |
|                | **2. 读取统计结果**         | 执行 `hdfs dfs -cat /out/part-r-00000                                                                             |

### 实验操作

- 编写Wordcount类：
```java
package com.imooc.mr;  
  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.io.LongWritable;  
import org.apache.hadoop.io.Text;  
  
import org.apache.hadoop.mapreduce.Job;  
import org.apache.hadoop.mapreduce.Mapper;  
import org.apache.hadoop.mapreduce.Reducer;  
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  
  
import java.io.File;  
import java.io.IOException;  
import java.util.Iterator;  
  
/**  
 * 读取hdfs上的hello.txt，计算出现次数  
 */  
public class WordCountJob {  
  
    public static class MyMapper extends Mapper<LongWritable, Text, Text, LongWritable> {  
        /**  
         * 实现map函数  
         * 接收<k1,v1>输出<k2，v2>  
         * @param key  
         * @param value  
         * @throws IOException  
         */        @Override  
        protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, LongWritable>.Context context) throws IOException, InterruptedException {  
            String[] words = key.toString().split(" ");  
            for (String word : words) {  
                Text k2 = new Text(word);  
                LongWritable v2 = new LongWritable(1L);  
                context.write(k2, v2);  
            }  
        }  
    }  
  
    public static class MyReduce extends Reducer<Text, LongWritable, Text, LongWritable> {  
        /**  
         * 针对<k2，v2>累加求和转化为<k3，v3>  
         * @param key  
         * @param values  
         * @throws IOException  
         */        @Override  
        protected void reduce(Text key, Iterable<LongWritable> values, Reducer<Text, LongWritable, Text, LongWritable>.Context context) throws IOException, InterruptedException {  
            long sum = 0L;  
            for(LongWritable value :values){  
                sum += value.get();  
            }  
            Text k3 = key;  
            LongWritable v3 = new LongWritable(sum);  
            context.write(k3,v3);  
        }  
    }  
  
    /**  
     * 组装Jop  
     */    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {  
        if(args.length != 2){  
            System.exit(100);  
        }  
        Configuration conf = new Configuration();  
        Job job = Job.getInstance(conf);  
        job.setJarByClass(WordCountJob.class);  
        //指定输入路径  
        FileInputFormat.setInputPaths(job,new Path(args[0]));  
        FileOutputFormat.setOutputPath(job,new Path(args[1]));  
  
        job.setMapperClass(MyMapper.class);  
        job.setMapOutputKeyClass(Text.class);  
        job.setMapOutputValueClass(LongWritable.class);  
  
        job.setReducerClass(MyReduce.class);  
        job.setOutputKeyClass(Text.class);  
        job.setOutputValueClass(LongWritable.class);  
  
        job.waitForCompletion(true);  
    }  
}
```

- 更新maven配置
```xml
<?xml version="1.0" encoding="UTF-8"?>  
<project xmlns="http://maven.apache.org/POM/4.0.0"  
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"  
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">  
    <modelVersion>4.0.0</modelVersion>  
  
    <groupId>org.example</groupId>  
    <artifactId>db_hadoop</artifactId>  
    <version>1.0-SNAPSHOT</version>  
  
    <properties>        <maven.compiler.source>9</maven.compiler.source>  
        <maven.compiler.target>9</maven.compiler.target>  
    </properties>    <dependencies>        <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client -->  
        <dependency>  
            <groupId>org.apache.hadoop</groupId>  
            <artifactId>hadoop-client</artifactId>  
            <version>3.3.1</version>  
            <scope>provided</scope>  
        </dependency>        <!-- https://mvnrepository.com/artifact/org.slf4j/slf4j-api -->  
        <dependency>  
            <groupId>org.slf4j</groupId>  
            <artifactId>slf4j-api</artifactId>  
            <version>2.0.0</version>  
            <scope>provided</scope>  
        </dependency>        <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs -->  
        <dependency>  
            <groupId>org.apache.hadoop</groupId>  
            <artifactId>hadoop-hdfs</artifactId>  
            <version>3.3.1</version>  
            <scope>provided</scope>  
        </dependency>        <!-- https://mvnrepository.com/artifact/org.slf4j/slf4j-log4j12 -->  
    </dependencies>  
    <build>        <plugins>  
            <!-- Java 编译插件 -->  
            <plugin>  
                <groupId>org.apache.maven.plugins</groupId>  
                <artifactId>maven-compiler-plugin</artifactId>  
                <version>3.8.1</version> <configuration>  
                <source>1.8</source>  
                <target>1.8</target>  
                <encoding>UTF-8</encoding>  
                <showWarnings>true</showWarnings>  
            </configuration>            </plugin>  
            <plugin>                <groupId>org.apache.maven.plugins</groupId>  
                <artifactId>maven-assembly-plugin</artifactId>  
                <version>3.3.0</version> <configuration>  
                <archive>                    <manifest>                        <mainClass></mainClass>                    </manifest>                </archive>                <descriptorRefs>                    <descriptorRef>jar-with-dependencies</descriptorRef>  
                </descriptorRefs>            </configuration>            </plugin>  
        </plugins>    </build>  
  
</project>
```

- 命令行开始打包执行
```shell
#打包
mvn clean package -DskipTests

#上传数据和文件

#执行
hadoop jar /export/data/db_hadoop-1.0-SNAPSHOT.jar com.imooc.mr.WordCountJob /test /out

#查看结果
hdfs dfs -cat /out/part-r-00000 | head -n 10
```

运行效果：
![[file-20260112231433219.png]]

![[file-20260112231450474.png | 500]]

Yarn界面效果：
![[file-20260112230642435.png]]

## 3.深入MapReduce

